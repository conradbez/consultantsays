The North American dairy industry has faced considerable volatility in volumes and prices in recent years, and 2020 is no exception. Indeed, the trend has accelerated, as swings in volume and pricing are happening over ever-shorter periods. Over an eight-week period starting in May, US cheddar cheese block hit a record high price of $2.65 a pound and traded as low as $1.20 a pound. In the preceding twelve-month period, the range was $2.16 to $1.10 (exhibit).
        
        Exhibit We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Dairy production has increased every year since the record­-high average all­-milk price in 2014, and despite the supply increase, producers were optimistic that margins in 2020 would be meaningfully higher. Feed-­price stabilization and government­-assistance initiatives—including the US Dairy Margin Coverage Program and the second phase of the Canadian Dairy Farm Investment Program —drove expectations for improved margins. Then the COVID-19 outbreak sparked concerns about the novel coronavirus’s impact on human lives and the severe economic downturn that could result from a prolonged battle with the virus. In this article, we will examine the headwinds and tailwinds that the industry is experiencing and recommend five actions executives could take to ensure their businesses’ future.
        
        The headwinds and tailwinds appear to affect all consumer dairy segments except shelf-stable dairy milk.
        
        A survey of North American dairy industry executives highlights demand as the biggest change resulting from the COVID-19 crisis. Reduction in demand has particularly affected the food-service industry, which consumes about one-third of the US dairy industry’s milk solids. In April 2020, the US food-service industry’s consumption of milk by volume was down more than 60 percent, leading some co-ops to enforce caps on milk collection and initiate milk dumping, estimated at 1 to 2 percent of April’s production.
        
        However, the impact of this downturn varies across the value chain. Maintaining volumes required processors to switch production capability propor­tionally to their food-service loss, which could not always be done. Some achieved this pivot through maximizing retail line utilization by consolidating formulas and reducing the number of SKUs. A few processors shifted production from food service to retail. In the United States, the oversupply may have resolved itself in May, as the food-service business started to come back and fill depleted pipe­lines; additionally, the Department of Agriculture went forward with its lunch box program’s purchase of dairy products.
        
        Shifts in consumer behavior due to COVID-19, including increased adoption of digital channels and movement in offline channels, have given a boost to some industry categories.
        
        Given the stay-at-home restrictions across most states and provincial regions, online purchases have increased across many industries, including grocery. Since the onset of COVID-19 in North America, overall, grocery delivery is up 22 percent, followed by a 21 percent rise in buying online for in-store pickup. Foods from the “perimeter” are now part of the standard online shopping cart, including dairy, which has broken through the digital-buying barrier and will likely stay there. Cate­gory growth across Amazon platforms underscores this trend, with year-over-year growth in frozen dairy (up 258 percent), liquid dairy (up 174 percent), and other dairy products (up 103 percent) in the eight weeks ending May 2, 2020, compared with the same period in 2019.
        
        The use of offline channels is shifting toward value offerings such as club, dollar, and discount retailers. According to a US consumer pulse survey con­ducted in mid-May, net intent to purchase was up 13 percent for club, 5 percent for dollar, and 1 percent for discount grocers. These channel shifts may further strain margins, depending on dairy category. In a recent pulse survey of North American dairy executives, 55 percent of respondents reported a decrease in margins year to date while only 13 percent reported margin increases for 2020. In that same survey, 68 percent of respondents reported believing that the effects of COVID-19 would last more than six months (up from 39 percent in a pulse survey conducted two weeks earlier).
        
        We offer five actions dairy executives can take in response to demand shifts and changes in consumer behavior:
        
        Capitalize on the momentum of a category gaining renewed popularity with consumers, reset where-to-play choices, and ensure that products (whether packaging or formula) are fit for online channels.
        
        Accelerate plans to roll out digital and advanced analytics capabilities that support the shift to digital and capture the evolving dairy con­sumer. Consumers are not only increasing their online purchases; they are also consuming more online content.
        
        Reallocate resources to anticipate channel and format shifts and create value in the places where consumers will be shopping. This might mean actively pursuing relationships with retailers, including partnering with them on assortment changes.
        
        Design products for new occasions—for example, with consumers’ new at-home reality in mind. Understanding how the COVID-19 pandemic is affecting consumer habits and preferences for the industry as well as for a specific category can help inform new product features and innovation.
        
        Invest, don’t retreat. Agility will be essential in the next normal. Companies will certainly adjust for the health and financial impacts of COVID-19. Where possible, however, companies should continue investing in consumer-engagement strategies, including marketing and communications.
        
        The next months could reveal more information about the future of the dairy industry, especially which consumer trends will stick around after the immediate impact of COVID-19 and how companies can adapt to changing consumer demand, sentiments, and behavior. Now is the time for dairy executives to reflect on their next steps and then act with urgency.Until early 2020, consumer spending on food in the United States had been remarkably stable, growing by around 4 percent over the previous five years. Total sales were roughly split evenly between retail outlets (such as grocery stores and supermarkets) and food-service companies (such as restaurants, hospitals, and schools). And until February, revenues were continuing in the same direction.
        
        Then came March and with it, the COVID-19 pandemic. Since then, physical distancing and associated lockdowns have dramatically reversed the trend of consumer spending on food. Consumers, forgoing public venues and eating at home, stocked up on groceries and supplies, boosting sales for the month by 29 percent over the prior year. Meanwhile, sales declined at restaurants, fast-food locations, coffee venues, and casual-dining locations by 27 percent (Exhibit 1).
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        By now, ripple effects into that previously balanced system have become clear. Distribution channels have been upended, with food stranded upstream, creating food-security risks for vulnerable populations. Companies that produce, convert, and deliver food to consumers and businesses face a web of interrelated risks and uncertainties across all steps in the value chain—from farmers to end-customer channels. Food-service suppliers, for example, faced abrupt order cancellations across their entire customer bases. That left many of them with excess stock that they couldn’t easily redirect to consumers because of packaging-size mismatches. Few home chefs have the cupboard space to accommodate restaurant-size cans of fruit and vegetables, but creating consumer-friendly formats would require additional investment of capital and time. And that would put perishable materials at risk, threatening narrow margins among prices, logistics, and transaction costs.
        
        Not surprisingly, all that creates uncertainty across the global value chain, with distinct challenges for farmers, distributors, producers, consumer- and packaged-goods companies, and retailers alike. Managers with a clear understanding of the challenges across the sector will be better prepared to decide whether to wait out the crisis or to invest for a longer-term shift in consumer spending. Much also depends on whether—and how quickly—they expect a return to pre-pandemic norms.
        
        For many farm operations that require significant amounts of labor (mainly, production of specialty crops, such as strawberries and lettuce), the most pressing pandemic-related challenge faced so far was the availability of workers. Some farmers faced other distinct challenges, such as a steep drop in grain prices following a shock to oil demand. Those value chains are operated in rural areas with low population density and limited opportunities to find skilled labor.
        
        Within the United States, multiple farming and processing value chains are dependent on migrant workers, including those under sponsored visa programs. Only three in ten workers in the US agricultural workforce are born in or are citizens of the United States; the rest are born in other countries, and many are in the United States on guest agricultural visas. If concerns related to the COVID-19 pandemic persist, it may be challenging to find workers, even at a premium, as people avoid close-quarters activities and limit their own exposure risk. Since worker wages are already a significant cost factor for farms, the pandemic may further strain farm economics.
        
        Moreover, movement restrictions related to the COVID-19 pandemic could deter nonlocal workforces from moving among counties or states for work. That would further increase labor challenges for farms, leading to shortages during production peaks and putting harvests at risk. Difficulties in redeploying workers to farms connected to retail-demand-driven organizations or to processing plants with consistent or increased demand could further amplify the imbalance among channels.
        
        With such uncertain futures, the dilemma farmers face is whether they should change crops; plow ahead with planned crops, hoping for a return to normal; or exit production entirely. For many value chains, crops can be returned once rotations are complete. For value chains in areas such as dairy, it can take years to recover production after farmers decide to reduce herds. Already, farmers are taking extreme measures to deal with excess product—for example, breaking eggs, spilling milk, and plowing under crops. If farmers go a step further to reduce capacity, such as eliminating hens, culling herds, and selling farmland, they could reduce capacity for the long term. That could lead to product shortages and price increases for both food producers and consumers when downstream demand returns.
        
        With such uncertain futures, the dilemma farmers face is whether they should change crops; plow ahead with planned crops, hoping for a return to normal; or exit production entirely.
        
        Distributors run an optimized and stable supply chain, with upstream orders coming in that anticipate downstream orders going out. Margins depend on there being a steady flow in both directions and having only a subset of products in inventory awaiting orders.
        
        Immediately after coronavirus-related shutdowns, outbound orders suddenly stopped because of government-mandated closures of restaurants, even though inbound orders of food kept coming in from farmers, food-service producers, and processors. That led to logistical bottlenecks and storage-space shortages as distributors worked to cancel incoming shipments of inventory from farmers. Distributors have been significantly affected by quick-service and casual-dining restaurants in their switch to takeout only, with slow recovery given the staged return to full service. Some distributors have also adapted by at least partially initiating online-ordering and delivery services, but that has not been universal. For those unaccustomed to supplying the retail channel, redirecting their sales adds the complexity of modifying their current supply chains—and that can also add to costs.
        
        Having rebalanced supplies with outgoing orders, food-service distributors are now left with overcapacity in their storage facilities and distribution networks, including the costlier “cold chain”—the temperature-controlled storage, equipment, and logistics needed to maintain a desired low temperature. The dilemma distributors face is how to stabilize their network cost structures in the interim. They could scale down support within each facility while maintaining a footprint. Or they could consolidate their networks of state-aligned distribution centers into regional ones, in spite of increased miles and lead times in a highly competitive environment. But consolidating some distribution centers and exiting others would reduce overall capacity in the long term. It would also limit local distribution options for food-service companies when demand returns, reducing channels for food-service producers as well.
        
        Food-service producers, such as produce and meat processors, face similar volume declines as their distributors do. Although in-store sales have increased to date, that increase has not covered the scale of decrease in food service, so plant utilizations remain significantly reduced. Additionally, many producers’ brands may not be recognized by retail consumers, making it difficult to gauge demand immediately.
        
        Moreover, many food-service producers have already invested in equipment and facilities to produce and package food in large multi-serving formats for complex prepared-, processed-, frozen-, canned-, and packaged-food value chains. It would be highly inefficient to reconfigure those investments to single service sizes. In addition, producers’ plant personnel may be at risk of infection, since, in some cases, the factories require associates to work in close proximity.
        
        For food-service producers, the dilemma is around the two- to five-year payback period of new packaging lines. Reinvesting and rebalancing a food-service network for retail is not a straightforward decision. Companies making new investments would be facing a 40 percent or more decline in revenue. And any number of issues could extend the payback period or make investments unrecoverable. Forecasts are uncertain, for example, about the duration of pandemic-related demand shifts, the recovery of the food-service economy, and the timeline of returning to full employment. Competition for volume is already putting downward pressure on prices. And short-term solutions, such as manual packing, are labor intensive and face incremental challenges because of physical-distancing precautions.
        
        Retail-facing consumer- and packaged-goods companies are facing multiple challenges because of the COVID-19 crisis. As with many companies in manufacturing, they bear risks related to employees working in close quarters at plants functioning at peak capacity. They also face significant increase in demand for certain product types (especially shelf-stable products) and packaging types (such as smaller sizes for home consumption) for which they have limited capabilities and capacity to supply. And they have distribution challenges because of a heightened demand for trucking coupled with a reduction in third-party-logistics capacity. That increases both competition and prices for trucking capacity.
        
        Recent COVID-19 infections at meat-processor plants have raised the possibility of mass closures of plants, causing significant risk to a value chain with limited excess capacity. As of this writing, 18 processing plants in the United States have already been closed, affecting more than a third of the country’s beef and pork supply. The US government recently invoked the Defense Production Act of 1950 to keep plants open. To comply, companies are offering a blend of incentives and incremental safety investments to maximize worker attendance and plant production—and to keep the food supply chain running.
        
        Companies are increasing production to maintain their presence on retail shelves, offering incentives to keep employees at work, and expediting raw materials (or engaging new suppliers) to meet production demand. Some companies are also increasing their e-commerce presence by going direct to consumer, given the spike in e-commerce purchases during the COVID-19 crisis. Those actions can add costs even as on-shelf prices stay fixed—in the face of competition or to avoid raising prices under crisis conditions, some have already brought mothballed production lines back into service and are evaluating their manufacturing footprints to reduce the number of plants.
        
        But consumer- and packaged-goods companies still face the dilemmas of how to approach demand peaks and what demand scenario to prepare for. A company might continue scaling up production at the expense of margin, but when capacity is truly maximized, it will need to decide whether to activate mothballed facilities, make acquisitions, or invest in new or external capacity. It might also partner with food-service producers that likely have excess capacity or even pass up volume requests from retailers, allowing consumer trials and potential long-term share loss to competitor brands and private labels.
        
        Grocery stores are benefiting from significant demand increases from demand previously met by food-service companies. However, they face additional challenges and extraordinary activities to protect and serve their consumers. Those include constant and visible cleaning of stores, frequent loading of shelves to keep up with demand, hazard-pay bonuses and incentives to maintain employee numbers, and hiring of additional labor, with limited time for training.
        
        Challenges also include the cost of expanded hours of operation (since foot traffic is limited because of physical distancing), the cost of scaling up online-ordering and delivery systems, and the associated cost of handling consumer complaints for late and errant deliveries. E-commerce has bridged the gap of declining foot traffic in the retail world, with surging delivery volumes across multiple channels escalating the importance of last-mile delivery during the COVID-19 crisis. The e-commerce channel now represents 10 to 15 percent of total grocery spend, increasing fivefold in the past few weeks. That has created a lot of strain in the system, as there are multiple challenges associated with last-minute delivery, given the significant ramp-up in labor required with limited training time. Walmart, for example, has hired 50,000 additional people, and Instacart has hired 300,000, even as they navigate new COVID-19-related safety precautions.
        
        In the months since the pandemic began, Amazon, Walmart, and most grocers have reported impressive sales increases, but margin growth has significantly lagged. At the same time, consumers are facing increasing economic hardship, limiting their ability to pay for goods. Many retailers are caught between the demand of reassuring consumers, protecting workers, and maintaining supply at increasing costs, and the need to maintain value for consumers. They may be able to increase throughput for their supply chains, despite what will likely be a finite period of increased demand, but will need to maintain high product quality even while establishing relationships with new suppliers. And there is always the risk of potential new entrants and additional channel shifts in the next normal.
        
        In the short to medium terms, in the absence of a COVID-19 vaccine, the challenges for each value-chain participant will continue. The severity of those challenges will depend on how quickly and safely governments open up economies and how quickly channels restabilize. Even after reopening, food service will continue to face significant challenges (such as requirements for a minimum distance between patrons, causing operating constraints) that may affect demand.
        
        Given fixed prices and cost-driven margin compression in retailer value chains, the returns on investment may not exist for farmers, producers, distributors, and retailers to make medium-term investments to address channel mismatches via investments and rebalancing. Therefore, channel mismatches may continue, with significant consequences to individual participants. If inaction leads to exit by food-value-chain players, it will remove food capacity from the value chain that would, under equilibrium conditions, have been consumed. That may create inflationary pressures when demand returns, if it exceeds supply. Such exits will also remove jobs from the economy well beyond the initial recovery phase, limiting the strength of the rebound.
        
        Food-service companies will need to pursue creative solutions, such as continued delivery and pickup services, to hit break-even volumes when there is limited seating in restaurants. Retail-channel participants, from farm to shelf, will need to coordinate in unprecedented ways to ensure continuity in supply despite rolling plant closures and pockets of equilibrium rebalancing. Profit margins will likely be affected at each step during messy rebalancing. Companies will need to rewire for agility versus trying to achieve static optimization states.
        
        Over the long term, the impact of uncertainty on the food supply chain could take many shapes, depending on how business owners expect the situation to evolve and resolve. On a spectrum of nine potential economic scenarios, a plurality of executives expect two to be most likely. Those two assume that some combination of effective or relatively effective public-health and economic-policy interventions will either contain the virus or limit it to some minor recurrences, resulting in a slow recovery. Under those scenarios, the recovery for food services, for example, has its own trajectory, shaped by shifts in consumer habits, safety at restaurants, and the overall economy (Exhibit 2). Depending on how well the virus is contained and the level of any recurrences, it could take between one and four years for food service to recover. However, it is possible that demand will never return to pre-pandemic levels, creating further challenges across the value chain.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Trying times up and down the food value chain vex company managers with considerable uncertainty. Profit pools are bound to continue shifting, with M&A activity (including potential integration across the value chain) to be expected, raising the need for efficient but resilient supply chains.The global agriculture industry is facing its biggest changes in the postwar period, from shifts in consumer preferences to technology-enabled productivity improvements to turmoil in domestic and international markets. We have identified four trends that could greatly influence the direction of the agriculture sector in the coming years, for better and for worse:
        
        We will eat differently. Developing markets are catching up to the protein-consumption levels of developed ones, and both are battling obesity. Should this trend accelerate, it could lead to increased demand for protein-rich foods and alternative-meat products.
        
        We will source from different places. New regions of food production—notably sub-Saharan Africa, and East Asia —could emerge, spurred by lower energy costs and climate-related challenges in traditional agriculture geographies.
        
        and East Asia —could emerge, spurred by lower energy costs and climate-related challenges in traditional agriculture geographies. We will produce food and we will trade differently. Advances in agricultural technology will increase transparency and traceability across the value chain. This will likely result in increased efficiency, reduced waste, and shrinking profit margins.
        
        We will conduct trade with different rules. Government intervention and subsidies could reshape market dynamics and have long-term effects on global commerce.
        
        Taken together, or in any combination, these trends will have ripple effects extending to every corner of the agriculture sector (see sidebar, “Commodities and the four trends”). It’s imperative for stakeholders to factor the trends—and potential disruption—into their strategic decisions to ensure a more resilient future. In the following article, we elaborate on the four trends, including a discussion of scenarios that might affect how the trends play out.
        
        Sidebar Commodities and the four trends Continue to next section Corn, soybeans, sugar, and wheat are the most traded crops as a percentage of global consumption. As a result, they are considerably exposed to the trends we describe in this article. The demand outlook to 2027 for these commodities remains tightly linked to an increasing global population and income growth in Asian emerging markets, notably China. These four crops have flat cost curves, meaning that small changes in market dynamics, let alone the big changes that the four trends could prompt, can have an outsize effect on export competitiveness. It should also be noted that wheat is traded over shorter distances because it is widely produced across the world, so it would likely be less affected by disruptions than the other three crops.
        
        Most people around the world eat more calories than they need (Exhibit 1). This is particularly true in developed countries in Europe and North America, although even some developing economies in Asia and South America see average individual caloric consumption above recommended levels. According to the World Health Organization (WHO), the combination of increased caloric intake and decreased physical activity has caused worldwide obesity to nearly triple since 1975, and the majority of the world’s population lives in countries where noncommunicable diseases linked to overconsumption, such as cardiovascular disease, diabetes, and cancer, kill more people than hunger.
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Meanwhile, as developing markets mature and gain wealth, their populations’ diets shift toward higher meat consumption. Indeed, our analysis shows how meat consumption stabilizes at higher levels of income compared with less costly food products that are based on sugar (Exhibit 2). China’s booming middle class and rapid urbanization have thus been accompanied by a remarkable rise in meat consumption in recent years. According to McKinsey analysis, half of the increase in global meat consumption over the past decade can be attributed to China.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        We have developed two scenarios related to what we eat that could affect the industry in the coming years.
        
        According to Euromonitor, sugar has moved to front and center in the healthy-eating debate. A report from the EAT-Lancet Commission argues that to fight obesity, global consumption of fruits, vegetables, nuts, and legumes will have to double, and consumption of foods such as red meat and sugar will have to be reduced by more than 50 percent. In the United States, per-capita consumption of high-fructose corn syrup fell 40 percent from its peak, from 2000 to 2018.
        
        Both push and pull factors played a role in its decline. Among the push factors are that many subnational US governments have implemented or are discussing regulations and taxes on soft drinks, including California, Illinois, and New York. As for pull factors, the boom in new juices, smoothies, and sports drinks provides consumers with alternatives to traditional carbonated beverages, which are high in high-fructose corn syrup in the United States.
        
        Should this movement continue, and obesity be seen as the major 21st-century threat to health, the international sugar market could be affected, potentially hitting exporting countries such as Australia, Brazil, India, and Thailand. It could also have an impact on biofuel markets, given the production relation between sugar and ethanol. If less sugar is consumed, more of it could be used for ethanol, increasing supply of the fuel and depressing its prices in the short run.
        
        A report from the EAT-Lancet Commission advocates that to fight obesity, global consumption of fruits, vegetables, nuts, and legumes will have to double, and consumption of foods such as red meat and sugar will have to be reduced by more than 50 percent.
        
        The main reason for the rise of soybean production over the past two decades was an increase in meat consumption globally, particularly in China. To meet this growing demand, soybean production for use as animal feed grew, especially in North and South America. Total soybean exports nearly doubled in these geographies from 2008 to 2018, from 73 million tons to 143 million tons.
        
        This growth may level out as the rise in incomes slows in China and thus meat-consumption growth decelerates. Other effects include people turning toward healthier diets, growing intolerance to how animals are treated, and increased public awareness of the negative effect of livestock on climate change. Indeed, mainstream consumers of meat are expected to remain the most relevant part of the market, but the definition of meat is expanding to include a variety of meat-replacement and alternative products dubbed “meat 2.0.” These products include, for example, “cultured meats” —the price of which dropped 99 percent from 2013 to 2017 and which have less of an effect on the climate than standard meat production.
        
        But before cultured meats hit the market, an even more significant piece of the meat-consumption market is quickly growing: meat-replacement products made of, for example, soybean protein, potatoes, sunflower oil, and pea protein. Surveys suggest that the majority of the population would be inclined to try meat-replacement products, or vegetal meat. This fast-growing segment is attracting funding from venture capital as well as established companies, and initial public offerings of alternative-meat companies have begun. According to one of these alternative-meat start-ups, 93 percent of its customers also buy animal meat. Increased production of artificial meat could be a factor in increasing the overall size of the global meat-protein market. In response, food and agriculture companies should consider whether, how, and to what extent to enter the alternative-meat market.
        
        The global landscape of food-producing regions will change. This could be a result of the impact of climate change (for example, drought and higher temperatures in some places) or reduced costs (for example, energy). Lower energy costs (Exhibit 3) can help make arable some areas that are now unfit for agriculture by cutting the cost of sourcing or desalinizing water and pumping it to land deprived of it.
        
        Exhibit 3 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        We have developed two scenarios related to where we source food from that could affect the industry in the coming years.
        
        China has grown into the world’s largest soybean importer. The main bottleneck to expansion of its internal agricultural production is water. However, if energy costs decrease enough to make desalinization economically feasible, then unused arable land could be made available through irrigation. This could significantly affect global trade: in 2018, China imported 91 million tons of soybeans. In a business-as-usual scenario, this amount would rise to 108 million tons by 2028. However, technological developments in energy and desalinization might enable new producers in China, its neighboring countries, and Africa to supply a significant portion of China’s growing demand, therefore displacing some of the current Western exports to eastern Asia. If China’s Western imports stay on par with 2018 levels, that would result in a 10 percent reduction in global trade volume.
        
        Meanwhile, changes in other parts of the world could make places such as Africa more viable for increased agriculture production. The continent is generally regarded as a wild card in the agriculture industry. The potential is huge, but the continent faces significant challenges—including access to energy and irrigation, as well as the need for basic infrastructure—that put its ascendance in question. Asian investors’ participation in infrastructure may become a deciding factor in accelerating the continent’s rise toward self-sufficiency rather than remaining a net importer, especially for sugar and corn. This would affect exporters into Africa, such as France, Russia, and the United States.
        
        Digital technologies are increasingly prevalent across the agricultural value chain, reducing information asymmetries, driving transparency, and boosting production yields to new highs. To date, farming technologies have advanced the industry in four “eras” and continue to define the maturity of producing countries. For example, when it comes to maize productivity, Africa and India remain in the first stage, dubbed the green revolution, which is defined by the use of fertilizers and pesticides. Argentina, Brazil, and China are in the modern-agriculture phase, which involves new generations of crop-protection products and new methods of cultivation, for example, mechanization. The European Union and the United States have advanced to 21st-century agriculture, characterized by precision agriculture—for example, using advanced analytics to apply nutrients and crop-protection products at variable rates. In the coming years, only the producers that have mastered precision agriculture will be ready to take advantage of the fourth era, next-generation agriculture technologies. Though far-off, this era will feature the proliferation of biotechnologies, gene editing (such as CRISPR), and automation, including agricultural robots that will monitor fields and harvest crops.
        
        In the coming years, only the producers that have mastered precision agriculture will be ready to take advantage of the fourth era, next-generation agriculture technologies.
        
        We see a future in which digital platforms enable full transparency and traceability across the food value chain—creating an environment in which actors in the value chain can more easily buy and sell to each other, compare prices, and review and rate suppliers. One current online agribusiness marketplace brings together those looking to buy and sell farm equipment, insurance, farmland, tools, and vehicles. This environment could lead to the emergence of online-trading platforms for agricultural products—relying on virtual currency and financial-technology companies to foster the buying and selling of agricultural inputs.
        
        This transparency could also decrease margins for intermediaries such as distributors and traders. In other industries, such as transportation, the surplus created by more value-chain transparency was transferred to end consumers. Indeed, trading margins of agriculture commodities have been shrinking, dropping from 15 percent in 1998 to 9 percent in 2018 (Exhibit 4). Lower returns make the ability to be cost competitive—which today is largely achieved through technology adoption—even more relevant.
        
        Exhibit 4 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Subsidies and export tariffs can change market dynamics and shift the competitive position of countries across the globe. Indeed, any manipulation of market dynamics can stack distortions on top of one another. For example, while export tariffs may give governments in certain regions extra funds in the short term, they also alter the cost competitiveness of producers in the cost curve and impact profit pools (Exhibit 5).
        
        Exhibit 5 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        We have developed one scenario related to how we trade that could affect the industry in the coming years.
        
        The consequences of subsidies, export tariffs, and trade-growth reductions are felt hardest by exporters; as subsidies go to certain producers regardless of efficiency or productivity, already flat commodity cost curves get even flatter. Profit pools could be affected by 50 percent in some cases (Exhibit 5). The effect of these government actions could have a long-term impact on markets. Similarly, trade-restriction measures, if they continue to mount, could reduce overall economic growth, causing the rise in globally traded volumes of commodities to stall. As a result, the acceleration of protein consumption we discussed previously could slow down.
        
        Developments in climate change, trade wars, new technologies, and consumer choices could lead to a revolution in the way we eat, produce, source, and trade food. These potential disruptions to the food value chain pose risks to stakeholders in the agriculture sector, from growers, input providers, and traders to consumer-goods companies, investors, and policy makers. It’s essential that the stakeholders take account of these trends in their strategies. Those that do will increase the chances of creating value, even in the face of disruption.While US President Donald Trump seems to have temporarily soothed trade tensions with China by delaying a planned increase in tariffs on $200bn of Chinese goods, the threat of levies on US imports of foreign cars and car parts remains. And America’s trading partners look ready to retaliate.
        
        Governments that once carried the banner for free trade are now retreating into protectionism. But this is precisely the wrong moment for economies to turn inward — particularly advanced ones.
        
        Over the past decade, globalisation has undergone little-noticed but profound structural shifts that are tilting the playing field in favour of advanced economies. The US, UK and countries across Europe all stand to gain in globalisation’s next chapter — if they don’t slam the door prematurely.
        
        Output has continued to rise but the share of goods traded across borders has fallen sharply. This decline has nothing to do with the recent trade wars. Nor does it mean that export markets are drying up. In fact, it reflects healthy economic development in China and other emerging markets. More of what gets made in these countries is now consumed locally instead of being sent to advanced economies.
        
        The geography of global demand has shifted radically, according to a comprehensive report by the McKinsey Global Institute. The developing world accounted for less than 20 per cent of global consumption in 1995. Now that share is up to nearly 40 per cent and on a trajectory to top 50 per cent by 2030. These new global consumers are creating major export opportunities. Companies in advanced economies sold more than $4tn worth of goods to the developing world in 2017. Digital e-commerce marketplaces with global reach are opening the door for more small and medium-size manufacturers to capture a slice of this growth.
        
        While trade in goods has flattened, services and cross-border data flows have become the real connective tissue of the global economy. Some types of services trade — IT services, business services and intellectual property royalties — are growing two or three times faster than trade in goods. From design to marketing, services also account for 30 per cent of the value of exported goods. Collectively, advanced economies run a trade surplus in services of $480bn, twice as high as a decade ago. They are well-positioned to capture future growth in areas such as entertainment streaming, cloud computing, remote healthcare and education.
        
        All industry value chains, including those that produce manufactured goods, now rely more heavily on research and development and innovation. Spending on intangible assets such as brands, software and operational processes has more than doubled relative to revenue over the past decade. This bodes well for Europe, the US and other advanced economies with highly skilled workforces and strong intellectual property protections.
        
        Most people formed their opinions about globalisation during the wave of offshoring in the 1990s and early 2000s, when factories shuttered in advanced economies and manufacturing migrated to the developing world. Today, the labour arbitrage game appears to be coming to an end. Only 18 per cent of today’s goods trade now involves exports from low-wage countries to high-wage countries. That’s a far smaller share than most people assume — and one that’s declining in many industries.
        
        Automation and artificial intelligence technologies will continue to make labour costs a less important factor when companies decide where to invest in new plants. Factors such as infrastructure, workforce skills and, especially speed to market, are weighing more heavily in the equation.
        
        All of this could produce a movement away from offshoring, enabling advanced economies to recapture a bigger share of share of the world’s production — albeit in a more digitised form. This type of manufacturing will not put millions to work on assembly lines, but it does support better-paying and more highly skilled jobs.
        
        The shifts occurring in globalisation today reflect what companies are already doing. But policymakers have been slow to recognise these tailwinds, in part because Europe and the US are still confronting the legacy of the last era of globalisation. Many of the workers and communities that suffered when western manufacturing moved to low-wage countries years ago have soured on the idea of global trade. But the solutions they need involve bolder domestic policies and reinvestment — not barriers that threaten to seal off the most promising avenues of growth in the decade ahead.Even with trade tensions and tariffs dominating the headlines, important structural changes in the nature of globalization have gone largely unnoticed. In Globalization in transition: The future of trade and value chains (PDF–3.7MB), the McKinsey Global Institute analyzes the dynamics of global value chains and finds structural shifts that have been hiding in plain sight.
        
        Although output and trade continue to increase in absolute terms, trade intensity (that is, the share of output that is traded) is declining within almost every goods-producing value chain. Flows of services and data now play a much bigger role in tying the global economy together. Not only is trade in services growing faster than trade in goods, but services are creating value far beyond what national accounts measure. Using alternative measures, we find that services already constitute more value in global trade than goods. In addition, all global value chains are becoming more knowledge-intensive. Low-skill labor is becoming less important as factor of production. Contrary to popular perception, only about 18 percent of global goods trade is now driven by labor-cost arbitrage.
        
        Three factors explain these changes: growing demand in China and the rest of the developing world, which enables these countries to consume more of what they produce; the growth of more comprehensive domestic supply chains in those countries, which has reduced their reliance on imports of intermediate goods; and the impact of new technologies.
        
        Globalization is in the midst of a transformation. Yet the public debate about trade is often about recapturing the past rather than looking toward the future. The mix of countries, companies, and workers that stand to gain in the next era is changing. Understanding how the landscape is shifting will help policy makers and business leaders prepare for globalization’s next chapter and the opportunities and challenges it will present.
        
        We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The 1990s and 2000s saw the expansion of complex value chains spanning the globe. But production networks are not immutable; they continue to evolve. We observe five major shifts in global value chains over the past decade.
        
        Trade rose rapidly within nearly all global value chains from 1995 to 2007. More recently, trade intensity (that is, the ratio of gross exports to gross output) in almost all goods-producing value chains has fallen. Trade is still growing in absolute terms, but the share of output moving across the world’s borders has fallen from 28.1 percent in 2007 to 22.5 percent in 2017. Trade volume growth has also slowed. Between 1990 and 2007, global trade volumes grew 2.1 times faster than real GDP on average, but they have grown only 1.1 times faster than GDP since 2011.
        
        The decline in trade intensity is especially pronounced in the most complex and highly traded value chains (Exhibit 1). However, this trend does not signal that globalization is over. Rather, it reflects the development of China and other emerging economies, which are now consuming more of what they produce.
        
        In 2017, gross trade in services totaled $5.1 trillion, a figure dwarfed by the $17.3 trillion global goods trade. But trade in services has grown more than 60 percent faster than goods trade over the past decade (Exhibit 2). Some subsectors, including telecom and IT services, business services, and intellectual property charges, are growing two to three times faster.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Yet the full role of services is obscured in traditional trade statistics. First, services create roughly one-third of the value that goes into traded manufactured goods. R&D, engineering, sales and marketing, finance, and human resources all enable goods to go to market. In addition, we find that imported services are substituting for domestic services in nearly all value chains. In the future, the distinction between goods and services will continue to blur as manufacturers increasingly introduce new types of leasing, subscription, and other “as a service” business models.
        
        Second, the intangible assets that multinational companies send to their affiliates around the world—including software, branding, design, operational processes, and other intellectual property developed at headquarters—represent tremendous value, but they often go unpriced and untracked unless captured as intellectual property charges. Years of R&D go into developing pharmaceuticals and smartphones, for example, while design and branding enable companies such as Nike and Adidas to charge a premium for their products.
        
        Finally, trade statistics do not track soaring cross-border flows of free digital services, including email, real-time mapping, video conferencing, and social media. Wikipedia, for instance, encompasses 40 million free articles in roughly 300 languages. Every day, users worldwide watch more than a billion hours of YouTube’s video content for free, and billions of people use Facebook and WeChat every month. These services undoubtedly create value for users, even without a monetary price.
        
        We estimate that these three channels collectively produce up to $8.3 trillion in value annually—a figure that would increase overall trade flows by $4.0 trillion (or 20 percent) and reallocate another $4.3 trillion currently counted as part of the flow of goods to services. If viewed this way, trade in services is already more valuable than trade in goods. This perspective would substantially shift the trade balance for some countries, most notably the United States. This exercise is not meant to argue for redefining national trade statistics. It simply underscores the underappreciated role of services, which will be increasingly important for how companies and countries participate in global value chains and trade in the future.
        
        As global value chains expanded in the 1990s and early 2000s, many decisions about where to locate production were based on labor costs, particularly in industries producing labor-intensive goods and services. Yet counter to popular perceptions, today only 18 percent of goods trade is based on labor-cost arbitrage (defined as exports from countries whose GDP per capita is one-fifth or less than that of the importing country). In other words, over 80 percent of today’s global goods trade is not from a low-wage country to a high-wage country. Considerations other than low wages factor into company decisions about where to base production, such as access to skilled labor or natural resources, proximity to consumers, and the quality of infrastructure.
        
        Moreover, the share of trade based on labor-cost arbitrage has been declining in some value chains, especially labor-intensive goods manufacturing (where it dropped from 55 percent in 2005 to 43 percent in 2017). This mainly reflects rising wages in developing countries. In the future, however, automation and AI may amplify this trend, transforming labor-intensive manufacturing into capital-intensive manufacturing. This shift will have important implications for how low-income countries participate in global value chains.
        
        In all value chains, capitalized spending on R&D and intangible assets such as brands, software, and intellectual property (IP) is growing as a share of revenue. Overall, it rose from 5.4 percent of revenue in 2000 to 13.1 percent in 2016. This trend is most apparent in global innovations value chains. Companies in machinery and equipment spend 36 percent of revenue on R&D and intangibles, while those in pharmaceuticals and medical devices average 80 percent (Exhibit 3). The growing emphasis on knowledge and intangibles favors countries with highly skilled labor forces, strong innovation and R&D capabilities, and robust intellectual property protections.
        
        Exhibit 3 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        In many value chains, value creation is shifting to upstream activities, such as R&D and design, and to downstream activities, such as distribution, marketing, and after-sales services. The share of value generated by the actual production of goods is declining (in part because offshoring has lowered the price of many goods). This trend is pronounced in pharmaceuticals and consumer electronics, which have seen the rise of “virtual manufacturing” companies that focus on developing goods and outsource actual production to contract manufacturers.
        
        Until recently, long-haul trade crisscrossing oceans was becoming more prevalent as transportation and communication costs fell and as global value chains expanded into China and other developing countries. The share of trade in goods between countries within the same region (as opposed to trade between more far-flung buyers and sellers) declined from 51 percent in 2000 to 45 percent in 2012.
        
        That trend has begun to reverse in recent years. The intraregional share of global goods trade has increased by 2.7 percentage points since 2013, partially reflecting the rise of emerging-market consumption. This development is most noticeable for Asia and the EU-28 countries. Regionalization is most apparent in global innovations value chains, given their need to closely integrate many suppliers for just-in-time sequencing. This trend could accelerate in other value chains as well, as automation reduces the importance of labor costs and increases the importance of speed to market in company decisions about where to produce goods.
        
        We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The map of global demand, once heavily tilted toward advanced economies, is being redrawn—and value chains are reconfiguring as companies decide how to compete in the many major consumer markets that are now dotted worldwide. McKinsey estimates that emerging markets will consume almost two-thirds of the world’s manufactured goods by 2025, with products such as cars, building products, and machinery leading the way. By 2030, developing countries are projected to account for more than half of all global consumption. These nations continue to deepen their participation in global flows of goods, services, finance, people, and data.
        
        The biggest wave of growth has been happening in China. Previous MGI research highlighted China’s working-age population as one of the key global consumer segments; by 2030, they are projected to account for 12 cents of every $1 of worldwide urban consumption. As it reaches the tipping point of having more millionaires than any other country in the world, China now represents roughly a third of the global market for luxury goods. In 2016, 40 percent more cars were sold in China than in all of Europe, and China also accounts for 40 percent of global textiles and apparel consumption.
        
        As consumption grows, more of what gets made in China is now sold in China. This trend is contributing to the decline in trade intensity. Within the industry value chains we studied, China exported 17 percent of what it produced in 2007. By 2017, the share of exports was down to 9 percent. This is on a par with the share in the United States but is far lower than the shares in Germany (34 percent), South Korea (28 percent), and Japan (14 percent). This shift has been largely obscured because the country’s output, imports, and exports have all been rising so dramatically in absolute terms. But overall, China is gradually rebalancing toward more domestic consumption.
        
        The rising middle class in other developing countries is also flexing new spending power. By 2030, the developing world outside of China is projected to account for 35 percent of global consumption, with countries including India, Indonesia, Thailand, Malaysia, and the Philippines leading the way. In 2002, India, for example, exported 35 percent of its final output in apparel, but by 2017, that share had fallen by half, to 17 percent, as Indian consumers stepped up purchases.
        
        Growing demand in developing countries also offers an opportunity for exporters in advanced countries. Only 3 percent of exports from advanced economies went to China in 1995, but that share was up to 12 percent by 2017. The corresponding share going to other developing countries grew from 20 to 29 percent. In total, advanced economies’ exports to developing countries grew from $1 trillion in 1995 to $4.2 trillion in 2017. In the automotive industry, Japan, Germany, and the United States send 42 percent of their car exports to China and the rest of the developing world. In knowledge-intensive services, 45 percent of all exports from advanced economies go to the developing world. The Asia–Pacific region is already a top strategic priority for many Western brands.
        
        We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The rise of domestic supply chains in China and other emerging economies has also decreased global trade intensity
        
        China’s rapid growth has made it a major part of virtually every goods-producing global value chain. Overall, it now accounts for 20 percent of global gross output, up from just 4 percent in 1995. In textiles and apparel, electrical machinery, and glass, cement, and ceramics, it now produces nearly half of global output.
        
        But as its economy has matured, China has moved beyond assembling imported inputs into final products. It now produces many intermediate goods and conducts more R&D in its own domestic supply chains. This is the second factor dampening global trade intensity in goods. In computers and electronics, for instance, Chinese companies are developing the kind of sophisticated smartphone chips that China once imported from advanced economies. Building more vertically integrated domestic industries enables China to capture more value added—and simultaneously bring jobs and economic development to its poorer inland provinces.
        
        Other developing countries are beginning to exhibit the same structural shifts seen in China, although they are at earlier stages. In textiles and apparel, for instance, production networks spanning multiple stages are consolidating within individual countries such as Vietnam, Bangladesh, Malaysia, India, and Indonesia.
        
        As a group, emerging Asia has become less reliant on imported intermediate inputs for the production of goods than the rest of the developing world (8.3 percent versus 15.1 percent in 2017). By contrast, in developing Europe, where economic growth has been slower, companies have continued to integrate into the supply chains of companies in Western Europe. The decline in trade intensity reflects growing industrial maturity in emerging economies. Over time, their production capabilities and consumption are gradually converging with those of advanced economies. Declining trade intensity in goods does not mean globalization is over; rather, digital technologies and data flows are becoming the connective tissue of the global economy.
        
        We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The explosive growth of cross-border data flows, highlighted in MGI’s previous research on digital globalization, is ongoing. From 2005 to 2017, the amount of cross-border bandwidth in use grew 148 times larger. A torrent of communications and content travels along these digital pathways—and some of this traffic reflects companies interacting with foreign operations, suppliers, and customers.
        
        Instant and low-cost digital communication has had one clear effect: lowering transaction costs and enabling more trade flows. But the impact of next-generation technologies on global flows of goods and services will not be as simple. The net impact is uncertain, but in some plausible scenarios, the next wave of technology could dampen global goods trade while continuing to fuel service flows.
        
        Digital platforms, logistics technologies, and data-processing advances will continue to reduce cross-border transaction costs and enable all types of flows
        
        In goods-producing value chains, logistics costs can be substantial. Companies often lose time and money to customs processing or delays in international payments. Three sets of technologies will continue to reduce these frictions in the years ahead.
        
        Digital platforms can bring together far-flung participants, making cross-border search and coordination more efficient. E-commerce marketplaces have already enabled significant cross-border flows by aggregating huge selections and making pricing and comparisons more transparent. Alibaba’s AliResearch projects that cross-border B2C e-commerce sales will reach approximately $1 trillion by 2020. B2B e-commerce could be five or six times as large. While many of those transactions may substitute for traditional offline trade flows, e-commerce could still spur some $1.3 trillion to $2.1 trillion in incremental trade by 2030, boosting trade in manufactured goods by 6 to 10 percent. Continued rapid growth in small-parcel trade would present a challenge for customs processing, however.
        
        Logistics technologies also continue to improve. The IoT can make delivery services more efficient by tracking shipments in real time, and AI can route trucks based on current road conditions. Automated document processing can speed goods through customs. At ports, autonomous vehicles can unload, stack, and reload containers faster and with fewer errors. Blockchain shipping solutions can reduce transit times and speed payments. We calculate that new logistics technologies could reduce shipping and customs processing times by 16 to 28 percent. By removing some of the frictions that slow the movement of goods today, these technologies together could potentially boost overall trade by 6 to 11 percent by 2030.
        
        Automation and additive manufacturing change production processes and the relative importance of inputs
        
        Previous MGI research has found that roughly half of the tasks that workers are paid to do could technically be automated, suggesting a profound shift in the importance of capital versus labor across industries. The growing adoption of automation and advanced robotics in manufacturing makes proximity to consumer markets, access to resources, workforce skills, and infrastructure quality assume more importance as companies decide where to produce goods.
        
        Service processes can also be automated by artificial intelligence (AI) and virtual agents. The addition of machine learning to these virtual assistants means they can perform a growing range of tasks. Companies in advanced economies are already automating some customer support services rather than offshoring them. This could reduce the $160 billion global market for business process outsourcing (BPO), now one of the most heavily traded service sectors.
        
        Additive manufacturing (3-D printing) could also influence future trade flows. Most experts believe it will not replace mass production over the next decade; its cost, speed, and quality are still limitations. But it is gaining traction for prototypes, replacement parts, toys, shoes, and medical devices. While 3-D printing could reduce trade in some specific products substantially, the drop is unlikely to amount to more than a few percentage points across overall trade in manufactured goods by 2030. In some cases, additive manufacturing could even spur trade by enabling customization.
        
        Overall, we estimate that automation, AI, and additive manufacturing could reduce global goods trade by up to 10 percent by 2030, as compared to the baseline. However, this reflects only the direct impact of these technologies on enabling production closer to end consumers in advanced economies. It is also possible that these technologies could lead to nearshoring and regionalization of trade instead of reshoring in advanced economies. Moreover, developing countries could adopt these technologies to improve productivity and retain production, thereby sustaining trade.
        
        Technology can transform some products and services, altering the content and volume of trade flows in the process. For example, McKinsey’s automotive practice estimates that electric vehicles will make up some 17 percent of total car sales globally by 2030, up from 1 percent in 2017. This could reduce trade in vehicle parts by up to 10 percent (since EVs have many fewer moving parts than traditional models) while also dampening oil imports.
        
        The shift from physical to digital flows that started years ago with individual movies, albums, and games is now evolving once again with streaming and subscription models. Streaming now accounts for nearly 40 percent of global recorded music revenues. Cloud computing uses a similar pay-as-you-go or subscription model for storage and software, freeing users from making heavy capital investments in their own IT infrastructure.
        
        The advent of ultra-fast 5G wireless networks opens new possibilities for delivering services. Remote surgery, for example, may become more viable as networks transmit sharp images without any delays and robots respond more precisely to remote manipulation. In industrial plants, 5G can support augmented and virtual reality–based maintenance from remote locations, creating new service and data flows.
        
        We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Given the shifts in value chains, companies need to reevaluate their strategies for operating globally
        
        Both the costs and the risks of global operations are shifting. Several imperatives stand out for global companies in this landscape:
        
        Reassess where to compete along the value chain. Business leaders need to continuously monitor where value is moving in their industry and adapt accordingly. Some have narrowed their focus to R&D and distribution while outsourcing production. By contrast, many makers of consumer goods take a hyperlocal approach, with customized product portfolios for individual markets. Providers of “global-local” services, such as Airbnb and Uber, have recognized global brands but also extensive local operations that deliver in-person services. Network companies, most of which are knowledge-intensive service providers, create value through a geographically dispersed operating model and global reach. Regardless of the strategy, a key point is to maintain control, trust, and collaboration in all parts of the value chain. For some companies, this might mean bringing more operations in-house. Those that outsource need close supplier relationships and greater visibility into lower tiers of the supply chain.
        
        Consider how to capture value from services. Across multiple value chains (including manufacturing), more value is coming from services. Shifting to services can offer advantages: smoothing cyclicality in sales, providing higher-margin revenue streams, and enabling new sales or design ideas due to closer interaction with customers. At its extreme, entire business models shift from producing goods to delivering services. To make this shift successfully, companies need to gain insight into customer needs, invest in data and analytics, and develop the right subscription, per-use, or performance-based service contracts.
        
        Reconsider your operational footprint to reflect new risks. New automation technologies, changing factor costs, an expanding set of risks, and the increasing importance of speed to market in some industries are all driving localization in many goods-producing value chains. As a result, it may make sense to place production in or near key consumer markets around the world. Before investing, companies should consider the full risk-adjusted, end-to-end landed costs of location decisions—and today many do not account for all of the variables.
        
        Be flexible and resilient. Today companies face a more complex set of unknowns as the postwar world order that held for decades seems to be giving way. There is a real chance that tariffs and nontariff barriers will continue to rise, reversing decades of trade liberalization. Tax codes are being reconsidered for the digital and intangible era. Building agile operations can help firms prepare for these types of uncertainties. This can take many forms, such as using versatile common platforms to share components across product lines and multiple plants. In purchasing, companies have achieved flexibility through price hedging, long-term contracting, shaping customer demand to enable using substitutes, and building redundancies into supply chains.
        
        Prioritize speed to market and proximity to customers. Companies in all industries now have real-time, granular sales and consumer behavior data at their disposal, but it takes manufacturing and distribution excellence to capitalize on these insights. Speed to market enables faster responses to what customers want and less product waste from forecasting errors. This does not necessarily require large-scale reshoring or full vertical integration in every major market. Companies can opt for postponement—that is, creating a largely standardized product at a distance and then finishing it with custom touches at a facility near the end market.
        
        Build closer supplier relationships. Arm’s-length relationships with suppliers across the globe involve hidden risks and costs. It makes sense to identify which suppliers are core to the business, then solicit their ideas and deepen relationships with them. Firms that genuinely collaborate can secure preferred customer status and benefit from new product ideas or process efficiencies bubbling up from suppliers. Large firms can also bring about systemic changes along the value chain, improving labor and environmental standards. Logistics and production technologies can transform supply chains, but optimizing what they can do requires end-to-end integration. Larger companies may need to help their small and medium-size suppliers upgrade and add digital capabilities to realize the full value.Emerging markets will power global growth over the next 20 years. By 2025, overall global consumption is forecast to reach $62 trillion, twice its 2013 level, and fully half of this increase will come from the emerging world. In 2010, the “consuming class”—people with disposable incomes of more than $10 a day—had 2.4 billion members, just over a third of the world’s population. By 2025, that will rise to more than half. Taking population growth into account, there will be an extra 1.8 billion consumers, the vast majority living in emerging regions.
        
        For manufacturers, the story is even more compelling. We estimate that emerging markets will be the destination for 65 percent of the world’s manufactured goods by 2025. Consumption starts with the basics, and the purchase of capital-intensive goods (such as cars, building products, and machinery) is driving the shift. By 2013, emerging markets already accounted for 59 percent of total demand for building materials, 57 percent for iron and steel, and 47 percent for machinery.
        
        Accessing these huge, important new markets won’t be straightforward, however. While more than half of the world’s population will likely live in cities by 2025, the fastest growth won’t take place in today’s emerging-market megacities, like Mumbai or Shanghai. Instead, during the next two decades the source of about 35 percent of the growth will be the several hundred million people projected to be living in more than 400 midsize cities spread across the emerging world.
        
        Those cities will be as diverse in character as they are geographically. Take three examples. Surat, in western India, accounts for about two-fifths of the country’s textile production. Foshan, China’s seventh-largest city by GDP, is home to the world’s largest wholesale markets for furniture and lighting products. Porto Alegre, the capital of Rio Grande do Sul, Brazil’s fourth-largest state, is a major export center for agricultural products from soybeans to leather. While broadly similar in size and growth potential, these cities will probably differ widely in their patterns of consumption, much as their religious, cultural, and regulatory environments do.
        
        Certain cities in emerging markets will become as important economically as some entire countries are today. The GDP of the Chinese city of Tianjin is already the same as Stockholm’s. By 2025, it will be as large as Sweden’s.
        
        The challenge for manufacturing companies isn’t just to understand how demand is changing at the city rather than the country level (though research suggests that fewer than one in five executives currently makes location and resource decisions on a city basis). It is also to ensure that production capabilities are developed sufficiently close to a company’s most important new markets, since manufacturing is still predominately a local business. Two-thirds of global manufacturing value comes from industries that tend to locate close to sources of local demand, either to reduce transportation costs or to tailor products to local needs.
        
        Bigger manufacturing companies have the freedom to choose where and how they operate across the world. A key challenge for them in coming decades will be not just picking the right mix of production locations but also learning to operate as efficiently as possible in these highly diverse environments. To do so, we believe they will have to focus on three broad sets of skills. First, they must manage the complexity required to meet varied customer needs. Second, they need the organizational capabilities to accommodate that complexity without sacrificing productivity. Third, they must have the manufacturing agility to meet fast-changing customer demand more effectively than their competitors do. Let’s look at each area in turn.
        
        To meet the needs of consumers in emerging markets, manufacturers first have to understand those needs. To do so, there’s no substitute for local insight. Companies clearly need to do their research on the ground to grasp not only the tastes and purchasing behavior of customers in key emerging markets but also the offerings of regional competitors. Moreover, customers aren’t the only important stakeholders in these markets. Different regulatory regimes, political environments, input costs, and capabilities in local supply chains can all influence product designs and manufacturing decisions.
        
        Insights must be gathered on a suitably granular level. A McKinsey study, for example, found that segmenting the Chinese market on a national or even a regional basis wasn’t adequate. By analyzing consumer characteristics, demographics, government policies, and other factors, the study identified 22 distinct market clusters that can be targeted independently.
        
        In emerging markets, the right combination of attributes can make or break products. Nokia achieved a dominant position in the African mobile-phone market, for example, with a simple, robust, and splash-proof handset incorporating a flashlight and a radio. And a manufacturer of consumer products was frustrated in its attempts to enter one emerging market until it conducted detailed on-the-ground research about the product it wished to sell: only then did it learn that consumers there, unlike those in every other country where it sold the product, required packaging that could be reused for other purposes after the contents were used up.
        
        Successful products require local development as well as local research. Shifting development closer to end users simplifies user testing and feedback, and also allows companies to employ designers and engineers who live and breathe the subtleties of local requirements. For these reasons, an increasing number of companies are co-locating R&D capabilities with their emerging-market manufacturing facilities. According to a McKinsey Global Survey, a majority of executives believe their R&D organizations should decentralize, with individual R&D sites operating as nodes in a global network. Thirty-eight percent say their companies plan to increase the offshoring of global R&D.
        
        Regional manufacturing and R&D facilities need talented people. Acquiring and retaining personnel with the right technical skills is a challenge for manufacturers all over the world. But the problem is particularly acute in emerging markets, which may lack the educational infrastructure or pool of competitors to provide the right people.
        
        Overcoming personnel shortages requires a systematic, multifaceted talent-management plan. Companies may need to bring in experienced people from elsewhere in their networks to assist in training and developing new staff. They can partner with local industry associations and academic institutions to create suitable training courses ensuring a supply of new recruits with the right basic skills. “Aviation Valley,” in southeastern Poland, for example, is home to more than 100 companies that account for 90 percent of the country’s aerospace sector. The nearby Rzeszów University of Technology has become a key supplier of the sector’s engineers, designers, and technicians, especially staff qualified to run the advanced computer-numerical-control (CNC) machine tools widely used in the sector.
        
        Such clusters have the advantage of increasing scale and reducing the cost of education and training facilities. But companies must ensure that their value proposition for employees is strong enough to minimize attrition to competitors.
        
        The great re-make: Manufacturing for modern times This 21-article compendium gives practical insights for manufacturing leaders looking to keep a step ahead of today’s disruptions.
        
        Organizations in emerging markets must also be flexible. Products tuned to the diverse needs of local markets may, for example, use different materials and production methods, so manufacturing organizations in such a location may not look like their counterparts elsewhere. They need to be agile, too. Consumption patterns in emerging regions can be volatile and fast evolving, and companies must therefore respond quickly to keep up. That will require not only flexible manufacturing technologies (discussed below) but also flexible approaches to staffing (for example, the thoughtful use of contract and temporary labor to balance the ebbs and flows of demand).
        
        When companies design manufacturing systems for emerging markets, they need to balance costs, flexibility, and the ability to adopt standard methods and practices across their worldwide operations. Manufacturers in emerging markets must make the most of the additional agility inherent in their production systems: lower personnel costs will continue to let them adopt more labor-intensive methods, for example, so they can adjust the number of operators and relocate resources in response to changing demand.
        
        Advanced design and manufacturing technologies will also play a critical role. In the design of production facilities, for example, modular approaches can reduce capital expenditures and improve flexibility, so companies can establish production quickly and then scale it up cost effectively, as demand requires. In the biopharmaceutical industry, modular manufacturing plants make it possible for new production facilities to become fully operational 12 months after the start of construction (compared with three to seven years for conventional facilities). Such plants are more energy efficient and less costly to build, as well.
        
        The right product designs also help companies to balance standardization and scale with appropriate adaptations to local needs. Platform- or module-based approaches, like those common in the automotive industry, make it possible for companies to use standard-part product architectures in multiple markets and to add or remove features or to adapt customer-facing components to suit local markets. Platforms make companies more agile, too, so it is easier for them to alter the mix of final products according to demand.
        
        For manufacturers, emerging markets have become a significant source of growth. Capturing it will require companies to think and act more globally, and more locally, as well. To do so, they will need to invest in a range of areas. They must build a detailed, granular picture of varying customer requirements. They must develop truly global talent pools. And they must build agility into their technologies and processes to match rapidly evolving—and increasingly diverse—demand. The most successful organizations will manage to combine efficiencies of scale and standardization with the flexibility and insight to meet diverse customer needs.We use cookies essential for this site to function well. Please click "Accept" to help us improve its usefulness with additional cookies. Learn about our use of cookies, and collaboration with select social media and trusted analytics partners hereLearn more about cookies, Opens in new tab.China accounts for about 25 percent of the world’s manufacturing activity, more than any other country on earth. Yet the advantages gained through lower costs of labor and capital, as well as efficiency-driven innovations, are slowly eroding. China’s manufacturing productivity remains only a fifth of that of developed economies.
        
        Companies and policy makers are therefore looking to upgrade China’s digital manufacturing capabilities by embracing Industry 4.0, the shorthand widely used for automation and data exchange in manufacturing technologies (including cyberphysical systems, the Internet of Things, and cloud computing). The goal is for manufacturers to use real-time data to link product designers, “smart” factories, and distribution centers across the value chain.
        
        In June 2016, we surveyed 130 companies across sectors to gauge China’s readiness. As the exhibit shows, Chinese manufacturers, particularly private companies, are more optimistic than their counterparts in Germany, Japan, and the United States on the potential of Industry 4.0 to transform industry. However, that is tempered by the lack of a solid game plan. Chinese manufacturers say they are less prepared than their counterparts to push ahead with Industry 4.0 initiatives. Notably, only 44 percent of state-owned enterprises report they are prepared.
        
        In interviews with executives, we drilled more deeply into the challenges. While Industry 4.0 has become a management buzzword across manufacturing, organizational capabilities, talent, and mind-sets are all lagging in many companies. Only 9 percent of companies have assigned responsibilities for Industry 4.0 initiatives versus more than a third in the United States and Germany. An even smaller number of Chinese companies, 6 percent, have a clear road map of the way ahead versus a fifth or more in the developed-economy cohort. Few companies have made digitization a priority or raised the awareness and skills of frontline managers. We also found that digital manufacturing tools along the value chain remain inadequate. Chinese auto companies, for example, lack the digital grounding to analyze, manage, and use data collected from production lines. Such data are crucial to the product development and R&D efforts required to raise quality and create globally competitive cars.
        
        Our research suggests that to fully capture the benefits of Industry 4.0, Chinese players will need a tailored approach to digital transformation. They should avoid a one-size-fits-all strategy and instead focus on three fundamentals: building a foundation of lean manufacturing, developing a solid management infrastructure, and developing new mind-sets and capabilities, especially in data and advanced analytics.After three decades of sizzling growth, China is now regarded by the World Bank as an upper-middle-income nation, and it’s on its way to being one of the world’s advanced economies. The investment-led growth model that underpinned this extraordinary progress has served China well. Yet some strains associated with that approach have become evident.
        
        In 2015, the country’s GDP growth dipped to a 25-year low, corporate debt soared, foreign reserves fell by $500 billion, and the stock market dropped by nearly 50 percent. A long tail of poorly performing companies pulls down the average, although top-performing Chinese companies often have returns comparable with those of top US companies in their industries. More than 80 percent of economic profit comes from financial services—a distorted economy. Speculation that China could be on track for a financial crisis has been on the rise.
        
        The nation faces an important choice: whether to continue with its old model and raise the risk of a hard landing for the economy, or to shift gears. A new McKinsey Global Institute report, China’s choice: Capturing the $5 trillion productivity opportunity, finds that a new approach centered on productivity could generate 36 trillion renminbi ($5.6 trillion) of additional GDP by 2030, compared with continuing the investment-led path. Household income could rise by 33 trillion renminbi ($5.1 trillion), as the exhibit shows.
        
        Exhibit We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Exploring China’s productivity Examine how six industry archetypes contribute to the country’s growth by province.
        
        China has the capacity to manage the decisive shift to a productivity-led model. Its government can pull fiscal and monetary levers, such as raising sovereign debt and securing additional financing on the basis of 123 trillion renminbi in state-owned assets. China has a vibrant private sector, earning three times the returns on assets of state-owned enterprises. There are now 116 million middle-class and affluent households (with annual disposable income of at least $21,000 per year), compared with just 2 million such households in 2000. And the country is ripe for a productivity revolution. Labor productivity is 15 to 30 percent of the average in countries that are part of the Organisation for Economic Co-operation and Development (OECD).
        
        A new productivity-led model would enable China to create more sustainable jobs, reinforcing the rise of the consuming middle class and accelerating progress toward being a full-fledged advanced economy. Such a shift will require China to steer investment away from overbuilt industries to businesses that have the potential to raise productivity and create new jobs. Weak competitors would need to be allowed to fail rather than drag down profitability in major sectors. Consumers would have more access to services and opportunities to participate in the economy.
        
        Video Revolutionizing China’s economy Five ways improving productivity can help the country confront its economic challenges.
        
        Making this transition is an urgent imperative. The longer China continues to accumulate debt to support near-term goals for GDP growth, the greater the risks of a hard landing. We estimate that the nonperforming-loan ratio in 2015 was already at about 7 percent, well above the reported 1.7 percent. If no visible progress is made to curb lending to poorly performing companies, and if the performance of Chinese companies overall continues to deteriorate, we estimate that the nonperforming-loan ratio could rise to 15 percent. This would trigger a substantial impairment of banks’ capital and require replenishing equity by as much as 8.2 trillion renminbi ($1.3 trillion) in 2019. In other words, every year of delay could raise the potential cost by more than 2 trillion renminbi ($310 billion). Although such an escalation would not lead to a systemic banking crisis, a liquidity crunch among corporate borrowers and waning confidence of investors and consumers during the recovery phase would have a significant negative impact on growth.
        
        unleashing more than 39 trillion renminbi ($6 trillion) in consumption by serving middle-class consumers better
        
        moving up the value chain through innovation, especially in R&D-intensive sectors, where profits are only about one-third of those of global leaders
        
        improving business operations through lean techniques and higher energy efficiency, for instance, which could deliver a 15 to 30 percent productivity boost
        
        strengthening competitiveness by deepening global connections, potentially raising productivity by 10 to 15 percent
        
        Capturing these opportunities requires sweeping change to institutions. China needs to open up more sectors to competition, enable corporate restructuring, and further develop its capital markets. It needs to raise the skills of the labor force to fill its talent gap and to sustain labor mobility. The government will need to manage conflicts among many stakeholders, as well as shift governance and incentives that rewarded a single-minded focus on rising GDP, even as it modernizes its own processes.
        
        Exactly how can China’s economy become more productive? Go to Tableau Public to examine how six industry archetypes contribute to the country’s growth by province.Over the past 50 years, global economic growth was exceptionally rapid. The world economy expanded sixfold. Average per capita income almost tripled. Hundreds of millions of people were lifted out of poverty. Yet unless we can dramatically improve productivity, the next half century will look very different. The rapid expansion of the past five decades will be seen as an aberration of history, and the world economy will slide back toward its relatively sluggish long-term growth rate (Exhibit 1).
        
        Exhibit 1 Global economic growth is set to slow dramatically. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Video Global growth: Can productivity save the day in an aging world? MGI’s Richard Dobbs, James Manyika, and Jaana Remes explain why global economic growth is destined to slow unless governments and business find ways to increase productivity.
        
        The problem is that slower population growth and longer life expectancy are limiting growth in the working-age population. For the past half century, the twin engines of rapid population growth (expanding the number of workers) and a brisk increase in labor productivity powered the expansion of gross domestic product. Employment and productivity grew at compound annual rates of 1.7 percent and 1.8 percent, respectively, between 1964 and 2014, pushing the output of an average employee 2.4 times higher. Yet this demographic tailwind is weakening and even becoming a headwind in many countries.
        
        The net result is that employment will grow by just 0.3 percent annually during the next 50 years, forecasts a new report from the McKinsey Global Institute (MGI)—Global growth: Can productivity save the day in an aging world? Even if productivity growth matches its rapid rate during the past half century, the rate of increase in global GDP growth will therefore still fall by 40 percent, to about 2.1 percent a year (Exhibit 2). Our new normal would then be economic growth slower than it was during the past five years of recovery from the Great Recession and during the energy-crisis decade of 1974 to 1984. Per capita income and living standards, in both the developed and the emerging worlds, will rise more slowly.
        
        Exhibit 2 Labor’s contribution to GDP growth is disappearing, so productivity must pick up the slack. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Global employment growth has been slowing for more than two decades. By around 2050, our research finds, the global number of employees is likely to peak. In fact, employee headcounts are already declining in Germany, Italy, Japan, and Russia; in China and South Korea, they are likely to begin falling as early as 2024. While there is significant scope for policies that boost labor-market participation among women, young people, and those over the age of 65, that will be far from easy. Employment growth could double, to 0.6 percent, in the countries we studied: the G19 (the G20 without the European Union as a composite member) plus Nigeria—economies that account for 63 percent of the world’s population and 80 percent of global GDP. But that will happen only if each gender and age group, throughout these countries, closes the employment gap with the high-performing economies. In any case, even a doubling of employment growth won’t fully counter the erosion of the labor pool.
        
        So productivity growth must drive the expansion of GDP in the longer term. Indeed, it would have to reach 3.3 percent a year—80 percent faster than its average rate during the past half century—to compensate fully for slower employment growth. Is this possible? Actually, our case studies of five sectors (agriculture, automotive, food processing, healthcare, and retailing) found scope to boost annual productivity growth as high as 4 percent, more than enough to counter demographic trends.
        
        The world isn’t running out of technological potential for growth. But achieving the increase in productivity required to revitalize the global economy will force business owners, managers, and workers to innovate by adopting new approaches that improve the way they operate.
        
        Our study found that about three-quarters of the potential productivity growth comes from the broader adoption of existing best practices, or catch-up improvements. The remaining one-quarter—counting only what we can foresee—comes from technological, operational, or business innovations that go beyond today’s best practices and push the frontier of the world’s GDP potential. Efforts to improve the traditionally weak productivity performance of the large and growing government and healthcare sectors around the world will be particularly important.
        
        Business must play a critical role: aggressively upgrading capital and technology, taking risks by investing in R&D and unproven technologies or processes, and mitigating the labor pool’s erosion by providing a more flexible work environment for women and older workers, as well as training and mentorship for young people. In an environment of potentially weaker global economic growth, and definitely evolving growth dynamics, executives need to anticipate where the market opportunities will be and the competitors they will meet in those markets. Above all, companies need to be competitive in a world where productivity will increasingly be the arbiter of success or failure.
        
        The past half century has been a time of extraordinary economic expansion. Yet without significantly boosting the one engine the world economy still has—productivity growth—this period may prove to be a historic anomaly. Our report has identified ten enablers that could lift global GDP growth closer to its potential by increasing transparency and competition, creating incentives for innovation, mobilizing labor, and further integrating the world economy. But all this will be hard. Only sweeping change by the private and public sectors—and a smarter approach to growth—will overcome the forces that now threaten global economic prosperity.
        
        To read more on the topic of global growth, see our series of contributions from leading thinkers on how to sustain rising prosperity for the long term.The relentless parade of new technologies is unfolding on many fronts. Almost every advance is billed as a breakthrough, and the list of “next big things” grows ever longer. Not every emerging technology will alter the business or social landscape—but some truly do have the potential to disrupt the status quo, alter the way people live and work, and rearrange value pools. It is therefore critical that business and policy leaders understand which technologies will matter to them and prepare accordingly.
        
        Disruptive technologies: Advances that will transform life, business, and the global economy, a report from the McKinsey Global Institute, cuts through the noise and identifies 12 technologies that could drive truly massive economic transformations and disruptions in the coming years. The report also looks at exactly how these technologies could change our world, as well as their benefits and challenges, and offers guidelines to help leaders from businesses and other institutions respond.
        
        We estimate that, together, applications of the 12 technologies discussed in the report could have a potential economic impact between $14 trillion and $33 trillion a year in 2025. This estimate is neither predictive nor comprehensive. It is based on an in-depth analysis of key potential applications and the value they could create in a number of ways, including the consumer surplus that arises from better products, lower prices, a cleaner environment, and better health.
        
        Some technologies detailed in the report have been gestating for years and thus will be familiar. Others are more surprising. Examples of the 12 disruptive technologies include:
        
        Advanced robotics—that is, increasingly capable robots or robotic tools, with enhanced "senses," dexterity, and intelligence—can take on tasks once thought too delicate or uneconomical to automate. These technologies can also generate significant societal benefits, including robotic surgical systems that make procedures less invasive, as well as robotic prosthetics and "exoskeletons" that restore functions of amputees and the elderly.
        
        Slideshow A gallery of disruptive technologies We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com Open interactive popup
        
        Next-generation genomics marries the science used for imaging nucleotide base pairs (the units that make up DNA) with rapidly advancing computational and analytic capabilities. As our understanding of the genomic makeup of humans increases, so does the ability to manipulate genes and improve health diagnostics and treatments. Next-generation genomics will offer similar advances in our understanding of plants and animals, potentially creating opportunities to improve the performance of agriculture and to create high-value substances—for instance, ethanol and biodiesel—from ordinary organisms, such as E. coli bacteria.
        
        Energy-storage devices or physical systems store energy for later use. These technologies, such as lithium-ion batteries and fuel cells, already power electric and hybrid vehicles, along with billions of portable consumer electronics. Over the coming decade, advancing energy-storage technology could make electric vehicles cost competitive, bring electricity to remote areas of developing countries, and improve the efficiency of the utility grid.
        
        The potential benefits of the technologies discussed in the report are tremendous—but so are the challenges of preparing for their impact. If business and government leaders wait until these technologies are exerting their full influence on the economy, it will be too late to capture the benefits or react to the consequences. While the appropriate responses will vary by stakeholder and technology, we find that certain guiding principles can help businesses and governments as they plan for the effects of disruptive technologies.In this video, Google executive chairman Eric Schmidt explores the phenomenon of technological disruption and selects those technologies likely to have the greatest impact on economies, business models, and people. This interview was conducted by James Manyika, a director in McKinsey’s San Francisco office, in February 2013. What follows is an edited transcript of Eric Schmidt’s remarks.
        
        The screen that you want to apply about technology is not what technologies are interesting, because there are so many that are interesting. You want to look at which ones have a chance of having a volume impact on many, many people, or large segments of the society.
        
        We’re going, in a single lifetime, from a small elite having access to information to essentially everyone in the world having access to all of the world’s information. That has huge implications for privacy, communications, security, the way people behave, the way information is spread, censorship, how governments behave, and so forth.
        
        That’s the primary narrative, I think, today. It changes education. It changes the way intellectual property works, it changes the way businesses work, it changes the way the media works, on and on and on. We’re in the middle of that right now.
        
        The one that comes next is undoubtedly biology. The same tools and techniques for combinatorial calculations, the kind of analytical computer use that we do today, when applied to biological systems, has an even greater impact. As we begin to say, “We’re going to take the analog world of biology—how genes work, how diseases work—put them in a digital framework, calculate for a while, do some machine learning on how things happen,” we’ll be able to not only help you become a better human being, but predict what’s going to happen to you physically in terms of your health, and so forth.
        
        Everything that we can do to build a model of how biology works, and in particular how the human brain works, how DNA works, how protein folding works, these sorts of things, is a serious step change for humanity. So, all of the grand challenges, like the sequencing of the human genome, for example. There are now firms and foundations building databases of DNA to use, to move to a model of individual diagnosis of disease, where you literally just press a button, the sequences occur, and it tells you what’s wrong. So the use of analytical tools in a historically analog world is a very big change.
        
        What’s happened in technology is that a new set of ultrapowerful, ultralight, ultraconductive materials can now be manufactured at scale. And there’s a revolution, largely driven by a set of universities, around new kinds of these manufacturing services that will change everything.
        
        So that revolution, plus the arrival of three-dimensional printing, where you can essentially build your own thing, means that—during the rest of our lifetimes, anyway—it’ll be possible to build very interesting things from very interesting, new materials, which have all sorts of new properties.
        
        We already know that there’s a whole hobbyist area around buying these 3-D printers for plastic. Well, if you can get these new materials, you could put them in the printers, and then over time those printers will become capable of machining, mining, and producing these materials.
        
        It’s certainly true that much of what we call innovation today is essentially routine, or evolutionary innovation. Cloud computing has been around for a long time, right? And it’s getting better, and better, and better. After all, cloud computing is just mainframe computing in a different way, which is how I learned how to compute when I was a young boy. So the fact of the matter is these ideas have been around for a long time. Is that going to change the world? It certainly makes it better, but it’s another step in the evolution of computer architecture.
        
        There’s a new generation of user-interface theory that says there should not be a user interface; the information should just be around you. We have a product called Google Now, which is available on Android, which actually attempts (by watching what you’re doing, and with your permission, and so forth) to make some suggestions.
        
        So it’s now figured out roughly where I live, and roughly where I work. And it tells me how long it takes me to get back and forth to work. Sort of useful. I didn’t ask it to do that. It figured out that I was going back and forth every day, and it said, “Oh, there’s a traffic jam,” and so forth. Now what are the limits of that technology? That’s an artificial-intelligence question. But it’s highly useful for it to have made a suggestion that would be good.
        
        So I think we’re going to go from the sort of command-and-control interfaces where you tell the computer, like a dog, “Bark,” to a situation where the computer becomes much more of a friend. And, a friend in the sense that the computer says, “Well, we kind of know what you care about.” And again, you’ve given it permission to do this. And it says, “Well, maybe you should do this,” or, “Maybe you should do that.”
        
        And the ultimate model is that the computer does what it does well, which is these complicated, analytical needle-in-a-haystack problems, and has perfect memory. And humans do what we do well, which is judgment, and having fun, and thinking about things. The relationship is symbiotic. The computer is making suggestions that are pretty good, they’re pretty helpful, but you’re ultimately in charge.
        
        The race that’s not being followed in the media is the race between humans and automation. And this race is run every day, and it’s a very tough race. So when I go to the local convenience store, they’ve replaced a low-wage worker with a machine to do my checkout. And that machine costs a great deal of money. And I’m sure it was a good business decision for them.
        
        So what happened to that low-wage worker? Well, their low wages probably did not go up. They might have even gone down. Maybe they’re on part of government assistance. So what’s the solution for that low-wage worker? Better education. So in the race against automation, which is the race we’re winning, and which politicians never articulate, the answer is better education.
        
        Now there are some other answers as well. For example, immigration of high-skilled workers; rather, we don’t have to educate everybody in America. We can also get a few educated people from other countries, and they’ll help us out, because they’ll hire all these other people here in America. And again, people are slowly beginning to understand that, in any particular country, you want an unfair share of highly educated people—in all industries, by the way—because in the race, they’re the winners.For the past seven years, thousands of executives from around the world—across a range of industries and functional areas—have responded to a McKinsey survey on how organizations are using social (or Web 2.0) technologies. In 2009 we created an interactive tool that links the data from these survey results and charts it to the emerging trends in Web 2.0 adoption.
        
        This interactive focuses on several of the survey’s core questions—from what technologies and tools companies view as most important to what kind of investments, if any, organizations plan to make in Web 2.0 in the future. Our most recent survey examines the business use of 13 social technologies and tools: blogs, collaborative document editing, mash-ups (a Web application that combines multiple sources of data into a single tool), microblogging, online videoconferencing, podcasts, prediction markets, rating, RSS (Really Simple Syndication), social networking, tagging, video sharing, and wikis.
        
        Using the interactive, you can track the performance of each technology through the years or customize the view to compare particular technologies side by side. The interactive also contains an audio guide from Michael Chui—a McKinsey principal and one of the drivers of the Web 2.0 research initiative—who takes you further inside the results and trends.
        
        This interactive archive will evolve from year to year as the survey progresses and as businesses continue to evaluate their use of and satisfaction with Web 2.0.Technologies known collectively as Web 2.0 have spread widely among consumers over the past five years. Social-networking Web sites, such as Facebook and MySpace, now attract more than 100 million visitors a month. As the popularity of Web 2.0 has grown, companies have noted the intense consumer engagement and creativity surrounding these technologies. Many organizations, keen to harness Web 2.0 internally, are experimenting with the tools or deploying them on a trial basis.
        
        Over the past two years, McKinsey has studied more than 50 early adopters to garner insights into successful efforts to use Web 2.0 as a way of unlocking participation. We have surveyed, independently, a range of executives on Web 2.0 adoption. Our work suggests the challenges that lie ahead. To date, as many survey respondents are dissatisfied with their use of Web 2.0 technologies as are satisfied. Many of the dissenters cite impediments such as organizational structure, the inability of managers to understand the new levers of change, and a lack of understanding about how value is created using Web 2.0 tools. We have found that, unless a number of success factors are present, Web 2.0 efforts often fail to launch or to reach expected heights of usage. Executives who are suspicious or uncomfortable with perceived changes or risks often call off these efforts. Others fail because managers simply don’t know how to encourage the type of participation that will produce meaningful results.
        
        Sidebar Twitter responses from our readers After “Six ways to make Web 2.0 work” was posted, we wanted to encourage Twitter users to continue the conversation. Twitter allows individuals to broadcast 140-character posts to a loosely connected community of followers. Within a few days, over 300 posts used the #web2.0work hashtag we established to monitor conversations and respond to the stream of opinions surrounding the article. The tweets came in several varieties. Many respondents simply reported that we had posted the article and offered a shortened URL back to the piece on mckinseyquarterly.com. Others, however, went further, commenting on the findings of the article and sharing how they have been integrating some of the “six ways” precepts into their own Web 2.0 processes. @estephen: @mckquarterly #web2.0work Rec 1 is spot on, even for conservative companies. we use this technique for wiki/blog internally. very effective.
        
        @Racecarver48: #web2.0work From recent first pilots, I can specifically agree with your theses 4, 5, and 6. We applied the successful w/o knowing them;-)
        
        @Nurbani: @McKQuarterly #web2.0work #5 hit home 4 mktg initiatives. The issue that Twitter respondents seemed to agree on the most was that Web 2.0 work can’t be added to an already full load, but instead needs to be meshed with everyday workflows. @Barry_H: #web2.0work McKinsey report http://bit.ly/5Ac1y. Point 3 really important, embed new things in the day job don't add more work
        
        @sifowler: @McKQuarterly 3. The tool has to fit in the workflow. Perceived 'extra' work reduces take-up. #web2.0work
        
        @Salv_Reina: @McKQuarterly re Workflow, this is critical. If the tool sits outside day 2 day work, it won't take easily #web2.0work Some users raised questions about points in our analysis that they thought were missing or not fully developed. @tomguarriello: @McKQuarterly Yes, but your recs don't address the fear of social media that paralyzes many organizations. Loss of control/risk stops them
        
        @RichardStacy: @mckquarterly possible adjunct to point 2 - create permission to fail, encourage experimentation and provide 'ROI break' #web2.0work. Or what they thought we got wrong. @drkleiman: it feels like too much focus on the tools and tech itself, not enough on thinking thru strategy & goals that can be accomplished #web2.0work Others felt the article’s list focused too heavily on issues internal to companies rather than on the potential for participation beyond organizational borders. @timolsen: #web2.0work - Article emphasizes how to use web 2.0 internally, I think using it externally is the real kicker. One of the article’s authors, Michael Chui, @mchui, directed this respondent to other published research where we analyze how Web 2.0 can be used to enlist participation among consumers. @mchui: @timolsen Definitely agree that external uses of Web 2.0 are important - see http://bit.ly/FDPsM #web2.0work In some cases, the article prompted a fuller dialogue away from the Twitter environment. A number of tweets posted links to more substantial blog site responses. @theparallaxview: new post 'McKinsey's Six of the Best for Web 2.0 Work' http://bit.ly/xvc9v #web2.0work @McKQuarterly
        
        @SocialMedia411: Contribution and Connection are the New Currency (ConversationAgent): http://bit.ly/aSBY2 Response to McKinsey Report
        
        @SandeepVizEdu: 6 Ways To Make Web2.0 Work - Visual Adaption of #web2.0work by McKinsey http://tinyurl.com/b5f5o6 One respondent, after reading the piece, vowed to take action. @kwjenkins: @McKQuarterly The article inspired me to start a blog on our work Intranet - we believe in collab, just not a lot of grassroots efforts yet
        
        Some historical perspective is useful. Web 2.0, the latest wave in corporate technology adoptions, could have a more far-reaching organizational impact than technologies adopted in the 1990s—such as enterprise resource planning (ERP), customer relationship management (CRM), and supply chain management (Exhibit 1). The latest Web tools have a strong bottom-up element and engage a broad base of workers. They also demand a mind-set different from that of earlier IT programs, which were instituted primarily by edicts from senior managers.
        
        Exhibit 1 The new tools Web 2.0 is the next wave of corporate-technology adoption. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Web 2.0 covers a range of technologies. The most widely used are blogs, wikis, podcasts, information tagging, prediction markets, and social networks (Exhibit 2). New technologies constantly appear as the Internet continues to evolve. Of the companies we interviewed for our research, all were using at least one of these tools. What distinguishes them from previous technologies is the high degree of participation they require to be effective. Unlike ERP and CRM, where most users either simply process information in the form of reports or use the technology to execute transactions (such as issuing payments or entering customer orders), Web 2.0 technologies are interactive and require users to generate new information and content or to edit the work of other participants.
        
        Exhibit 2 A range of technologies Participatory technologies can be categorized into five groups. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Earlier technologies often required expensive and lengthy technical implementations, as well as the realignment of formal business processes. With such memories still fresh, some executives naturally remain wary of Web 2.0. But the new tools are different. While they are inherently disruptive and often challenge an organization and its culture, they are not technically complex to implement. Rather, they are a relatively lightweight overlay to the existing infrastructure and do not necessarily require complex technology integration.
        
        Clay Shirky, an adjunct professor at New York University, calls the underused human potential at companies an immense “cognitive surplus” and one that could be tapped by participatory tools. Corporate leaders are, of course, eager to find new ways to add value. Over the past 15 years, using a combination of technology investments and process reengineering, they have substantially raised the productivity of transactional processes. Web 2.0 promises further gains, although the capabilities differ from those of the past technologies (Exhibit 3).
        
        Exhibit 3 Management capabilities unlocked by participation Six new management capabilities can be unlocked by participatory technologies. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Research by our colleagues shows how differences in collaboration are correlated with large differences in corporate performance. Our most recent Web 2.0 survey demonstrates that despite early frustrations, a growing number of companies remain committed to capturing the collaborative benefits of Web 2.0. Since we first polled global executives two years ago, the adoption of these tools has continued. Spending on them is now a relatively modest $1 billion, but the level of investment is expected to grow by more than 15 percent annually over the next five years, despite the current recession.
        
        To help companies navigate the Web 2.0 landscape, we have identified six critical factors that determine the outcome of efforts to implement these technologies.
        
        1. The transformation to a bottom-up culture needs help from the top. Web 2.0 projects often are seen as grassroots experiments, and leaders sometimes believe the technologies will be adopted without management intervention—a “build it and they will come” philosophy. These business leaders are correct in thinking that participatory technologies are founded upon bottom-up involvement from frontline staffers and that this pattern is fundamentally different from the rollout of ERP systems, for example, where compliance with rules is mandatory. Successful participation, however, requires not only grassroots activity but also a different leadership approach: senior executives often become role models and lead through informal channels.
        
        At Lockheed Martin, for instance, a direct report to the CIO championed the use of blogs and wikis when they were introduced. The executive evangelized the benefits of Web 2.0 technologies to other senior leaders and acted as a role model by establishing his own blog. He set goals for adoption across the organization, as well as for the volume of contributions. The result was widespread acceptance and collaboration across the company’s divisions.
        
        2. The best uses come from users—but they require help to scale. In earlier IT campaigns, identifying and prioritizing the applications that would generate the greatest business value was relatively easy. These applications focused primarily on improving the effectiveness and efficiency of known business processes within functional silos (for example, supply-chain-management software to improve coordination across the network). By contrast, our research shows the applications that drive the most value through participatory technologies often aren’t those that management expects.
        
        Efforts go awry when organizations try to dictate their preferred uses of the technologies—a strategy that fits applications designed specifically to improve the performance of known processes—rather than observing what works and then scaling it up. When management chooses the wrong uses, organizations often don’t regroup by switching to applications that might be successful. One global technology player, for example, introduced a collection of participatory tools that management judged would help the company’s new hires quickly get up to speed in their jobs. The intended use never caught on, but people in the company’s recruiting staff began using the tools to share recruiting tips and pass along information about specific candidates and their qualifications. The company, however, has yet to scale up this successful, albeit unintended, use.
        
        At AT&T, it was frontline staffers who found the best use for a participatory technology—in this case, using Web 2.0 for collaborative project management. Rather than dictating the use, management broadened participation by supporting an awareness campaign to seed further experimentation. Over a 12-month period, the use of the technology rose to 95 percent of employees, from 65 percent.
        
        3. What’s in the workflow is what gets used. Perhaps because of the novelty of Web 2.0 initiatives, they’re often considered separate from mainstream work. Earlier generations of technologies, by contrast, often explicitly replaced the tools employees used to accomplish tasks. Thus, using Web 2.0 and participating in online work communities often becomes just another “to do” on an already crowded list of tasks.
        
        Participatory technologies have the highest chance of success when incorporated into a user’s daily workflow. The importance of this principle is sometimes masked by short-term success when technologies are unveiled with great fanfare; with the excitement of the launch, contributions seem to flourish. As normal daily workloads pile up, however, the energy and attention surrounding the rollout decline, as does participation. One professional-services firm introduced a wiki-based knowledge-management system, to which employees were expected to contribute, in addition to their daily tasks. Immediately following the launch, a group of enthusiasts used the wikis vigorously, but as time passed they gave the effort less personal time—outside their daily workflow—and participation levels fell.
        
        Google is an instructive case to the contrary. It has modified the way work is typically done and has made Web tools relevant to how employees actually do their jobs. The company’s engineers use blogs and wikis as core tools for reporting on the progress of their work. Managers stay abreast of their progress and provide direction by using tools that make it easy to mine data on workflows. Engineers are better able to coordinate work with one another and can request or provide backup help when needed. The easily accessible project data allows senior managers to allocate resources to the most important and time-sensitive projects.
        
        Pixar moved in a similar direction when it upgraded a Web 2.0 tool that didn’t quite mesh with the way animators did their jobs. The company started with basic text-based wikis to share information about films in production and to document meeting notes. That was unsatisfactory, since collaborative problem solving at the studio works best when animators, software engineers, managers, and directors analyze and discuss real clips and frames from a movie. Once Pixar built video into the wikis, their quality improved as critiques became more relevant. The efficiency of the project groups increased as well.
        
        4. Appeal to the participants’ egos and needs—not just their wallets. Traditional management incentives aren’t particularly useful for encouraging participation. Earlier technology adoptions could be guided readily with techniques such as management by objectives, as well as standardized bonus pay or individual feedback. The failure of employees to use a mandated application would affect their performance metrics and reviews. These methods tend to fall short when applied to unlocking participation. In one failed attempt, a leading Web company set performance evaluation criteria that included the frequency of postings on the company’s newly launched wiki. While individuals were posting enough entries to meet the benchmarks, the contributions were generally of low quality. Similarly, a professional-services firm tried to use steady management pressure to get individuals to post on wikis. Participation increased when managers doled out frequent feedback but never reached self-sustaining levels.
        
        A more effective approach plays to the Web’s ethos and the participants’ desire for recognition: bolstering the reputation of participants in relevant communities, rewarding enthusiasm, or acknowledging the quality and usefulness of contributions. ArcelorMittal, for instance, found that when prizes for contributions were handed out at prominent company meetings, employees submitted many more ideas for business improvements than they did when the awards were given in less-public forums.
        
        5. The right solution comes from the right participants. Targeting users who can create a critical mass for participation as well as add value is another key to success. With an ERP rollout, the process is straightforward: a company simply identifies the number of installations (or “seats”) it needs to buy for functions such as purchasing or finance and accounting. With participatory technologies, it’s far from obvious which individuals will be the best participants. Without the right base, efforts are often ineffective. A pharmaceutical company tried to generate new product ideas by tapping suggestions from visitors to its corporate Web site. It soon discovered that most of them had neither the skills nor the knowledge to make meaningful contributions, so the quality of the ideas was very low.
        
        To select users who will help drive a self-sustaining effort (often enthusiastic early technology adopters who have rich personal networks and will thus share knowledge and exchange ideas), a thoughtful approach is required. When P&G introduced wikis and blogs to foster collaboration among its workgroups, the company targeted technology-savvy and respected opinion leaders within the organization. Some of these people ranked high in the corporate hierarchy, while others were influential scientists or employees to whom other colleagues would turn for advice or other assistance.
        
        When Best Buy experimented with internal information markets, the goal was to ensure that participation helped to create value. In these markets, employees place bets on business outcomes, such as sales forecasts. To improve the chances of success, Best Buy cast its net widely, going beyond in-house forecasting experts; it also sought out participants with a more diverse base of operational knowledge who could apply independent judgment to the prediction markets. The resulting forecasts were more accurate than those produced by the company’s experts.
        
        6. Balance the top-down and self-management of risk. A common reason for failed participation is discomfort with it, or even fear. In some cases, the lack of management control over the self-organizing nature and power of dissent is the issue. In others, it’s the potential repercussions of content—through blogs, social networks, and other venues—that is detrimental to the company. Numerous executives we interviewed said that participatory initiatives had been stalled by legal and HR concerns. These risks differ markedly from those of previous technology adoptions, where the chief downside was high costs and poor execution.
        
        Companies often have difficulty maintaining the right balance of freedom and control. Some organizations, trying to accommodate new Web standards, have adopted total laissez-faire policies, eschewing even basic controls that screen out inappropriate postings. In some cases, these organizations have been burned.
        
        Prudent managers should work with the legal, HR, and IT security functions to establish reasonable policies, such as prohibiting anonymous posting. Fears are often overblown, however, and the social norms enforced by users in the participating communities can be very effective at policing user exchanges and thus mitigating risks. The sites of some companies incorporate “flag as inappropriate” buttons, which temporarily remove suspect postings until they can be reviewed, though officials report that these functions are rarely used. Participatory technologies should include auditing functions, similar to those for e-mail, that track all contributions and their authors. Ultimately, however, companies must recognize that successful participation means engaging in authentic conversations with participants.
        
        Acceptance of Web 2.0 technologies in business is growing. Encouraging participation calls for new approaches that break with the methods used to deploy IT in the past. Company leaders first need to survey their current practices. Once they feel comfortable with some level of controlled disruption, they can begin testing the new participatory tools. The management imperatives we have outlined should improve the likelihood of success.The adoption by companies of Enterprise 2.0 tools, a cluster of web-based social technologies first popularized by consumers, appears to be leveling off after a decade of rapid growth. But new research also suggests that power users—businesses that deploy the more advanced technologies extensively—achieve stronger results than companies dabbling at the edge.
        
        For nearly a decade, we’ve tracked the adoption and diffusion of social technologies—wikis, blogs, social technologies, and the like—through a unique database of 1,500 companies. Two points stand out in our latest analysis.
        
        Levels of social-technology use, by our estimates, were low in 2006. By 2008, two-thirds of the companies in our database had adopted at least one such technology, though internal diffusion was narrow: only 20 percent of all employees had used them, and no single technology had gone mainstream. Thereafter, our analysis shows, an S-curve dynamic (Exhibit 1) spurred the wider diffusion of these tools, particularly blogs and social networks.
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Strong evidence indicates that imitation and innovation have been driving the spread of Enterprise 2.0 tools. Using modeling techniques, we found that 35 percent of the companies had adopted social technologies in response to their adoption by competitors. Copycat behavior was also responsible for their diffusion within organizations, though at a slightly lower rate: 25 percent of all employee usage. (Teams, for example, typically tried to burnish their performance by imitating early users of social networks and internal blogs.) As for innovation, company policies designed to encourage it sparked the adoption of wikis. Within enterprises, social networks help to spread innovative ideas.
        
        According to our analysis, imitation and innovation spread Enterprise 2.0 social technologies more quickly than they did nonsocial web-based ones such as email, as estimated by academic researchers. But their effect seemed to be weaker than others have found it to be on the diffusion of consumer technologies such as Facebook (social networks) or Netflix (social recommendations). One reason for the difference is that the adoption of Enterprise 2.0 tools requires two things that are not always available: additional investment and management discipline to spur integration.
        
        Roughly a fifth of the companies we studied will account for an estimated 50 percent of all social-technology usage in 2015. The steepness of the power-curve distribution diminishes slightly from 2010 to 2015 as more companies adopted these tools and broadened their internal deployment (notably of wikis and social networks). Our surveys also asked specifically about the perceived impact of Enterprise 2.0 tools on revenues and operating costs. These self-reported responses were combined to calculate a measure of enterprise value added.
        
        We found that the companies we identified as power users reported an incremental 5 percent in value added in 2010 and of up to 6.5 percent in 2014. These findings were tested with a traditional measure of statistical significance which confirmed the correlation. We also used a more sophisticated technique that indicated a causal relationship between usage and performance. That seems plausible: power laws should naturally skew performance benefits toward heavier users. It’s interesting that the incremental value from social technologies appears to be as large as it was from computers in the 1990s and, more recently, from technologies linked to big data.
        
        In addition, we found significant returns from the greater diffusion of Enterprise 2.0 within companies. The data allowed us to estimate the returns for each technology at several levels of penetration, from 25 percent to 100 percent. We found that even incremental use among employees could significantly increase the value added for each technology (Exhibit 2). The highest usage level of social networks, wikis, and blogs created a self-reported added value of at least 5 percent each, but the impact of other social technologies was much smaller. We also found returns to scope: using a second social technology doubles the value added at most levels of penetration.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Social technologies are approaching the top of the S-curve. Adoption across organizations started to taper in 2012, and internal diffusion flattened out somewhat later. Yet the growing popularity of mobile and cloud technologies, as well as the Internet of Things (see “An executive’s guide to the Internet of Things”), could alter the pattern in the future. Companies placing bets should consider how these technologies will interact with Enterprise 2.0 tools and potentially multiply their impact.
        
        Meanwhile, Facebook and other digital players are developing a new generation of social tools geared to enterprise use. These providers, with their huge base of consumers, may further increase the adoption and diffusion of Enterprise 2.0 tools among and within companies. They may also open up new sources of value, both for heavy users and for companies still sitting on the sidelines.As the Internet of Things (IoT) has gained popular attention in the five years since we first published on the topic, it has also beguiled executives. When physical assets equipped with sensors give an information system the ability to capture, communicate, and process data—and even, in a sense, to collaborate—they create game-changing opportunities: production efficiency, distribution, and innovation all stand to benefit immensely. While the consumer’s adoption of fitness bands and connected household appliances might generate more media buzz, the potential for business usage is much greater. Research from the McKinsey Global Institute suggests that the operational efficiencies and greater market reach IoT affords will create substantial value in many industries. (For more, see the video “What’s the one piece of advice for a business leader interested in the Internet of Things?” And to see how experts believe the Internet of Things will evolve, see “The Internet of Things: Five critical questions.”)
        
        There are many implications for senior leaders across this horizon of change. In what follows, we identify three sets of opportunities: expanding pools of value in global B2B markets, new levers of operational excellence, and possibilities for innovative business models. In parallel, executives will need to deal with three sets of challenges: organizational misalignment, technological interoperability and analytics hurdles, and heightened cybersecurity risks.
        
        IoT’s impact is already extending beyond its early, most visible applications. A much greater potential remains to be tapped.
        
        To make the Internet of Things more understandable, media coverage has often focused on consumer applications, such as wearable health and fitness devices, as well as the automation products that create smart homes. Our research reveals considerable value in those areas. Yet the more visible manifestations of IoT’s power shouldn’t distract executives from a core fact: business-to-business applications will account for nearly 70 percent of the value that we estimate will flow from IoT in the next ten years. We believe it could create as much as $11.1 trillion a year globally in economic value in nine different types of physical settings. Nearly $5 trillion would be generated almost exclusively in B2B settings: factories in the extended sense, such as those in manufacturing, agriculture, and even healthcare environments; work sites across mining, oil and gas, and construction; and, finally, offices.
        
        There’s also a global dimension to IoT’s B2B potential. Emerging markets, whose manufacturing-intensive economies often supply goods to final manufacturers, will be prime areas for IoT adoption. But over the next ten years, the total economic impact from IoT will be greater in advanced economies, given the possibility of larger cost savings and higher adoption rates (Exhibit 1).
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        However, an estimated 38 percent of IoT’s overall worldwide value will likely be generated in developing economies, and eventually, the number of IoT deployments in such markets could surpass those in developed ones. In fact, deployments in developing economies are likely to exceed the global average in work-site settings (such as mining, oil and gas drilling, and construction) and in factories. For instance, China, with its large and growing industrial and manufacturing base, stands to reap major benefits not only on the factory floor but also in product distribution. In fact, developing economies could leapfrog the developed world in some IoT applications because there are fewer legacy technologies to displace.
        
        Investing in IoT hardware—from sensors embedded in manufacturing equipment and products to electronically tagged items along the supply chain—is only the starting point of the value equation. The biggest competitive gains come when IoT data inform decisions. Our work shows that most of the new business value will arise from optimizing operations. For example, in factories, sensors will make processes more efficient, providing a constant flow of data to optimize workflows and staffing:
        
        Sensor data that are used to predict when equipment is wearing down or needs repair can reduce maintenance costs by as much as 40 percent and cut unplanned downtime in half.
        
        Inventory management could change radically, as well. At auto-parts supplier Wurth USA, cameras measure the number of components in iBins along production lines, and an inventory-management system automatically places supply orders to refill the containers.
        
        In mining, self-driving vehicles promise to raise productivity by 25 percent and output by 5 percent or more. They could also cut health and safety costs as much as 20 percent by reducing the number of workplace accidents.
        
        IoT systems can also take the guesswork out of product development by gathering data about how products (including capital goods) function, as well as how they are actually used. Using data from equipment rather than information from customer focus groups or surveys, manufacturers will be able to modify designs so that new models perform better and to learn what features and functionality aren’t used and should therefore be eliminated or redesigned. By analyzing usage data, for example, a carmaker found that customers were not using the seat heater as frequently as would be expected from weather data. That information prompted a redesign to allow easier access: the carmaker updated the software for the dashboard touchscreen to include the seat-heater command. This illustrates another capability of connected devices: with the ability to download new features, these products can actually become more robust and valuable while in service, rather than depreciate in value.
        
        Despite this value, most data generated by existing IoT sensors are ignored. In the oil-drilling industry, an early adopter, we found that only 1 percent of the data from the 30,000 sensors on a typical oil rig are used, and even this small fraction of data is not used for optimization, prediction, and data-driven decision making, which can drive large amounts of incremental value.
        
        IoT can also spur new business models that would shift competitive dynamics within industries. One example is using IoT data and connectivity to transform the sale of industrial machinery and other goods into a service. The pioneers of this trend were jet-engine manufacturers that shifted their business model to selling thrust and ancillary services rather than physical equipment. Now these models are proliferating across industries and settings. Transportation as a service, enabled by apps and geolocation devices, is encroaching on vehicle sales and traditional distribution alike. Manufacturers of products such as laser printers with IoT capabilities are morphing into robust service businesses.
        
        IoT makes these business models possible in a number of ways. First, the ability to track when and how physical assets are actually used allows providers to price and charge for use. Second, the combined data from all these connected assets help a supplier to operate equipment much more efficiently than its customers would, since its customers would only have a limited view of their own equipment if they purchased and ran it themselves. Furthermore, analysis of IoT data can enable condition-based, predictive maintenance, which minimizes unplanned downtime.
        
        This business-model shift will require product companies to develop and flex their service muscles. Product development, for instance, becomes service development, where value is cocreated with customers. It won’t be enough to focus on the product features customers will pay the most for. Developers will need to understand the business outcomes their customers seek and learn how to shape offerings to facilitate those outcomes most effectively. Service providers will also have to take on capacity-planning functions—including planning for peak usage and utilizing IoT data to forecast demand.
        
        As with any major technological shift, realizing IoT’s potential will require significant management attention not just to new technical imperatives but also to organizational issues.
        
        IoT will challenge traditional organizational roles as information technology becomes widely embedded across assets, inventories, and operations. One focal point will be the IT function, for the Internet of Things requires it to assume a transformed role that spans beyond computers, networks, mobile devices, and data centers. Instead, IT will have to join with line managers to oversee IoT systems that are essential to improve both the top and bottom lines.
        
        In retailing, for instance, one of the largest sources of value could be the sales lift that real-time, in-store personalized offers are expected to deliver. This will require the sophisticated integration of data across many sources: real-time location data (the shopper’s whereabouts in a store), which would link to data from sensors in the building; customer-relationship-management data, including the shopper’s online-browsing history; and data from tags in the items on display, telling the customer to enter a specific aisle, where he or she could use an instant coupon sent to a phone to buy an item previously viewed online. In short, information technology and operations technology will converge, both technically and in their metrics of success. As a result, companies will have to align their IT and operational leadership tightly, though traditionally these functions tended to work separately and, more often than not, held each other at arm’s length.
        
        Beyond expanding IT’s role, IoT will challenge other notions of organizational responsibilities. Chief financial, marketing, and operating officers, as well as leaders of business units, will have to be receptive to linking up their systems. Companies may need to train employees in new skills, so the organization can become more analytically rigorous and data driven. Analytics experts and data scientists must be connected with executive decision makers and (to optimize insights from the new data) with frontline managers. In some cases, the decision makers will be algorithms. When companies need large-scale real-time action—such as optimizing the control of equipment across an entire factory—IoT systems will make decisions automatically. Managers will monitor metrics and set policy.
        
        Strategies that use IoT data in an effective way often call for interoperability. We estimate that nearly 40 percent of the potential value, on average, will require different IoT systems to communicate with one another and to integrate data (Exhibit 2). Relatively little of that is happening now. For example, on offshore oil platforms today, components such as pumps are often installed as connected devices, but in a limited fashion: devices individually connect back to their manufacturers, which monitor and control machines and can optimize their maintenance and performance individually. However, data from multiple components and systems must be combined to identify more than half of the predictable performance issues that arise in day-to-day platform operations, including those that could impact overall oil-production volumes.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Many large companies will have enough market power to specify that their IoT vendors make systems interoperable. In some cases, this will lead vendors to choose common standards that will ultimately speed up adoption. In other cases, interoperability could also be achieved with software platforms designed to combine data from multiple systems. That will create new market opportunities for companies capable of integrating data from diverse sources.
        
        However, simply bringing data together from different IoT systems won’t be enough. Indeed, IoT may exacerbate many of the challenges we have observed when companies use big data. In moving to a world where IoT is used for prediction and optimization, companies face an analytics challenge. They’ll need to develop or purchase, to customize, and then to deploy analytical software that extracts actionable insights from the torrent of data IoT will generate. And in many cases, the algorithms embedded in this software will have to analyze data streams in real time—a task many traditional analytical tools are not designed to do. This offers another potential market opportunity for innovative software developers.
        
        The prospect of implementing the Internet of Things should prompt even greater concern about cybersecurity among executives. IoT poses not only the normal risks associated with the increased use of data but also the vastly greater risks of systemic breaches as organizations connect to millions of embedded sensors and communications devices. Each is a potential entry point for malicious hackers, and the damage from a break-in can be literally life threatening—disrupting machine-control systems on an oil rig or in a hospital, for example. The same interoperability that creates operational efficiency and effectiveness also exposes more of a company’s units to cyberrisks. Growing interconnections among companies and links with consumer devices will create other challenges to the integrity of corporate networks, too.
        
        Companies will need to rely on the capabilities of vendors to mitigate some of these risks. However, preparing for a revolutionary change in distributed connectedness and computation will also require a new strategic approach, which our colleagues have described as “digital resilience.” In other words, companies need to embed methods of protecting critical information into technology architectures, business-model-innovation processes, and interactions with customers. They can start by assessing the full set of risks in an integrated way and by creating an extensive system of defenses that will be hard for hackers to penetrate. Companies also need to tailor cybersecurity protections to the processes and information assets of each of their businesses, which in an IoT world will increasingly be linked. Given the extent of the risks and the cross-functional nature (and significant cost) of the solutions, progress will require senior-level participation and input.
        
        IoT will soon become a differentiating factor in competition. Senior leaders and board members must take a systems approach to address the organizational challenges and risks this expansion of the digital domain will create. That will allow companies to capture the full range of benefits promised by the Internet of Things.The semiconductor industry has been able to weather the fallout from the global financial crisis and realize several years of healthy growth—in part because of the widespread adoption of smartphones and tablets, which created demand for mobile and wireless applications. The industry’s average annual growth rate between 2010 and 2013 was about 5 percent. Could the same sort of growth result from widespread adoption of the Internet of Things? Many semiconductor players have been asking themselves just this question.
        
        The Internet of Things refers to the networking of physical objects through the use of embedded sensors, actuators, and other devices that can collect or transmit information about the objects. The data amassed from these devices can then be analyzed to optimize products, services, and operations. Perhaps one of the earliest and best-known applications of such technology has been in the area of energy optimization: sensors deployed across the electricity grid can help utilities remotely monitor energy usage and adjust generation and distribution flows to account for peak times and downtimes. But applications are also being introduced in a number of other industries. Some insurance companies, for example, now offer plans that require drivers to install a sensor in their cars, allowing insurers to base premiums on actual driving behavior rather than projections. And physicians can use the information collected from wireless sensors in their patients’ homes to improve their management of chronic diseases. Through continuous monitoring rather than periodic testing, physicians could reduce their treatment costs by between 10 and 20 percent, according to McKinsey Global Institute research—billions of dollars could be saved in the care of congestive heart failure alone.
        
        In each of these cases, the connected devices that transmit information across the relevant networks rely on innovations from semiconductor players—highly integrated microchip designs, for instance, and very low-power functions in certain applications. The semiconductor companies that can effectively deliver these and other innovations to original-equipment manufacturers, original-device manufacturers, and others that are building Internet of Things products and applications will play an important role in the development of the market. That market, in turn, may represent a significant growth opportunity for semiconductor players.
        
        Indeed, semiconductor executives surveyed in June 2014 as part of our quarterly poll of the components-manufacturing market said the Internet of Things will be the most important source of growth for them over the next several years—more important, for example, than trends in wireless computing or big data. McKinsey Global Institute research supports that belief, estimating that the impact of the Internet of Things on the global economy might be as high as $6.2 trillion by 2025. At the same time, the corporate leaders polled admit they lack a clear perspective on the concrete business opportunities in the Internet of Things given the breadth of applications being developed, the potential markets affected—consumer, healthcare, and industrial segments, among others—and the fact that the trend is still nascent.
        
        In this article, we take the pulse of the market for Internet of Things applications and devices. Where along the development curve are the enabling technologies, and where can semiconductor players insert themselves in the evolving ecosystem? We believe components manufacturers may be able to capture significant value primarily by acting as trusted facilitators—it is their silicon, after all, that can enable not just unprecedented connectivity but also long-term innovation across the Internet of Things.
        
        Three years ago, industry pundits and analysts predicted that, by 2020, the market for connected devices would be between 50 billion and 100 billion units. Today, the forecast is for a more reasonable but still sizable 20 billion or 30 billion units. This leveling off of expectations is in line with what we have seen in past introductions of new technologies. Throughout the late 1990s and early 2000s, for instance, there was much discussion in the semiconductor industry about the potential benefits and implications of Bluetooth technology, but the inflection point for Bluetooth did not happen until 2003 or 2004, when a large enough number of industry players adopted it as a standard and pushed new Bluetooth-based devices and applications into the market. The market for Internet of Things devices, products, and services appears to be accelerating toward just such an inflection point, in view of four critical indicators.
        
        Supplier attention. Internet of Things developer tools and products are now available. Apple, for instance, has released HealthKit and HomeKit developer tools as part of its latest operating-system upgrade, and Google acquired Nest to catalyze the development of an Internet of Things platform and applications.
        
        Technological advances. Some of the semiconductor components that are central to most Internet of Things applications are showing much more functionality at lower prices. Newer processors, such as the ARM Cortex M, use only about one-tenth of the power that most energy-efficient 16-bit processors used only two years ago. This leap forward in technological capabilities is apparent in the evolving market for smart watches. The first such products released in 2012 boasted 400-megahertz single processors and simple three-axis accelerometers. Now a typical smart watch will include 1-gigahertz dual-core processors and high-end, six-axis devices that combine gyroscopes and accelerometers. Meanwhile, the prices of the chip sets used in these products have declined by about 25 percent per year over the past two years.
        
        Increasing demand. Demand for the first generation of Internet of Things products (fitness bands, smart watches, and smart thermostats, for instance) will increase as component technologies evolve and their costs decline. A similar dynamic occurred with the rise of smartphone usage. Consumer demand for smartphones jumped from about 170 million devices sold annually just four or five years ago to more than a billion devices in 2014. The increase in orders coincided with a steep decline in the price of critical smartphone components.
        
        Emerging standards. Over the past two years, semiconductor players have joined forces with hardware, networking, and software companies, and with a number of industry associations and academic consortiums, to develop formal and informal standards for Internet of Things applications. AT&T, Cisco, GE, IBM, and Intel, for instance, cofounded the Industrial Internet Consortium, whose primary goal is to establish interoperability standards across industrial environments so that data about fleets, machines, and facilities can be accessed and shared more reliably. Other groups have been focused on standardizing the application programming interfaces (APIs) that enable basic commands and data transfer among Internet of Things devices.
        
        Analysts have predicted that the installed base for Internet of Things devices will grow from around 10 billion connected devices today to as many as 30 billion devices by 2020—an uptick of about 3 billion new devices per year (exhibit). Each of these devices will require, at a minimum, a microcontroller to add intelligence to the device, one or more sensors to allow for data collection, one or more chips to allow for connectivity and data transmission, and a memory component. For semiconductor players, this represents a direct growth opportunity that goes beyond almost all other recent innovations—with the exception, perhaps, of the smartphone.
        
        Exhibit We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        A new class of components will be required to address this opportunity: system on a chip–based devices produced specifically for the Internet of Things, with optimal power and connectivity features and with sensor integration. First-generation chips are already on the way, although it will probably be a few generations before chips can deliver all the functionality required. Intel, for instance, is releasing a low-power system on a chip designed for smaller products in automotive and industrial environments. This chip also can be used in fitness bands and other wearable devices. Additionally, sensors based on microelectro-mechanical-systems (MEMS) technology will continue to play a significant role in enabling Internet of Things applications.
        
        It’s worth noting that semiconductor players may also be able to profit indirectly from the Internet of Things, since the data generated from billions of connected devices will need to be processed—all those “little” data must be turned into big data—and users will require greater storage capacity, spurring new demand for more servers and more memory. Building on an existing market, semiconductor companies can continue to provide the critical devices and components that are at the heart of these products.
        
        The question, then, is no longer if the Internet of Things can provide substantial growth for semiconductor players; the real consideration is how best to capitalize on the trend. What are the critical challenges or inhibitors? What are the possible enablers for growth and adoption? Our research and discussions with semiconductor executives have helped us identify potential challenges in two critical areas—technology and ecosystem development.
        
        Semiconductor players may need to invest heavily to adapt their chip designs and development processes to account for specific Internet of Things system requirements. For instance, because many applications would require devices that are self-sustaining and rely on energy harvesting or long-life batteries, semiconductor companies must address the need for optimal power consumption and outstanding power management in their products. Connectivity load will be another critical concern, since hundreds or even thousands of devices may need to be connected at the same time. The average smart home, for instance, may contain 50 to 100 connected appliances, lights, thermostats, and other devices, each with its own low-power requirements. Existing connectivity solutions such as standard Bluetooth or Wi-Fi will probably not be able to meet smart-home requirements given their power and network limitations.
        
        Manufacturers may also need to emphasize flexible form factors to a greater degree than they currently do. Components must be small enough to be embedded in today’s smart watches and smart glasses but also amenable to further shrinking for incorporation into still-unidentified future products. And security and privacy issues absolutely must be addressed. Internet of Things devices will not be used for critical tasks in, say, industrial or medical environments if connectivity protocols have not been established to prevent hacking, loss of intellectual property, or other potential breaches.
        
        Semiconductor players are moving full steam ahead to address some of these challenges. Their efforts in two areas in particular are highly encouraging.
        
        Increased integration. Some semiconductor players are already considering investing in new integration capabilities—specifically, expertise in packaging and in through silicon via, a connectivity technique in electronic engineering, as well as in software development. The emergence of more integrated system-in-package and system-on-a-chip devices is helping to overcome some of the challenges described earlier, in part by addressing power, cost, and size factors. The trend toward multidimensional chip stacking and packaging (2.5-D and 3-D integrated-circuit, or 2.5DIC and 3.0DIC, devices in particular) has resulted in integrated circuits that are one-third smaller than standard chips, with 50 percent lower power consumption and bandwidth that is up to eight times higher—at a cost that can be up to 50 percent lower when compared with traditional systems on a chip of the same functionality. Monolithic integration of MEMS sensor technologies with complementary metal-oxide semiconductors is considered unlikely for Internet of Things applications. In these instances, the integration of substrates with silicon requires making certain design trade-offs and optimizing both the sensor and the logic circuits. Instead, we expect to see 2.5DIC and 3.0DIC technologies being favored for Internet of Things–specific integrated circuits.
        
        Connectivity standards. The current cellular, Wi-Fi, Bluetooth, and Zigbee specifications and standards are sufficient to enable most Internet of Things applications on the market. Some applications, however, will require low-power, low-data-rate connectivity across a range of more than 20 meters—an area in which cellular technologies and Wi-Fi often fall short. New technologies that target this need are emerging from players such as those in the Bluetooth and Weightless interest groups. The latter is an industry group comprising technology companies that are exploring the use of free wireless spectrum to establish an open communications protocol. Such standardization efforts will enable Internet of Things applications that require broadly distributed sensors operating at low power over low-cost spectrum—for instance, temperature and moisture sensors used in agricultural applications.
        
        As Joep van Beurden, the chief executive at CSR, notes, only about 10 percent of the financial value to be captured from the Internet of Things trend is likely to be in the “things”; the rest is likely to be in how these things are connected to the Internet (see “Making connections: An industry perspective on the Internet of Things”). The semiconductor players that focus primarily on the things themselves should therefore find ways to support the development of a broader ecosystem (beyond silicon) and find their niche as both enablers and creators of value for their customers and their customers’ customers. This will mean developing partnerships with players further downstream, such as companies that are building and providing cloud-based products and services.
        
        It will be important for semiconductor companies to remember that different industries are at different levels of maturity and complexity with respect to the Internet of Things—so the roles that components manufacturers can play in application development in certain industries will vary, as will the timing of growth opportunities. The market for home-automation tools, for instance, has established some common APIs, but competing standards remain. A number of application developers have already started generating monitoring products for consumers, and once standardization issues can be addressed, the market may experience significant growth rather quickly. By contrast, the markets for monitoring and control systems in factories and for beacon technologies in retail are much more fragmented and will therefore take longer to develop. In retail, for instance, all the players in the value chain—the stores, the data aggregators, the Internet service providers, and other partners—must sort out their roles and standards of operation before beacon-technology providers can approach them with a clear customer value proposition and business model.
        
        In these instances, semiconductor companies may want to test the waters by forming alliances with hardware companies, systems players, and customers or by finding ways to assist in developing standards. In the factory-monitoring-systems market, for instance, players are attempting to create common standards (through the Industrial Internet Consortium initiative, for example, and the Europe-only Industry 4.0 initiative), even though most of the hardware platforms are still proprietary, as are the data, which reside in legacy systems. Semiconductor players that pursue alliances and standard-setting activities may be able to play an enabling role in defining best practices in Internet of Things privacy, security, and authentication—issues that will be critical in markets, such as healthcare and wearables, that are dealing with sensitive consumer data.
        
        Given the potential 90 percent distribution of value to players that provide all the technologies “beyond” the silicon, there may never be a compelling enough business case for components manufacturers to develop individual chips and systems for hundreds of thousands of discrete Internet of Things industry applications. We believe semiconductor players should instead design a family of devices that are sufficiently flexible to cater to the needs of multiple industries—that can be used in industrial and consumer Internet of Things applications that boast similar characteristics. Our work suggests that these devices will probably fall somewhere along a continuum of application requirements—at one extreme, high-power, high-performance, application-processing Internet of Things devices, such as those embedded in smart watches, and, at the other extreme, low-cost, ultralow-power integrated sensors that support sufficient (but not excessive) functionality and autonomous device operation. To achieve this level of design flexibility and to address the opportunity properly, semiconductor players may need to rethink their approach to product and application development.
        
        The challenges associated with the Internet of Things are many; semiconductor executives should consider ways to integrate new development models, process capabilities, and go-to-market strategies in their existing operations. Success will require bold moves, boards that are willing to bet on unfamiliar models and activities, and collaboration with those that are developing industry standards. But the semiconductor industry should embrace this era of innovation and reinvention. The opportunities for growth outweigh the challenges, as components manufacturers explore the creation of a new class of Internet of Things–enabled semiconductors that can cut across a wider swath of potential customers than existing components can. The sector may be on the cusp of unit growth similar to the surge it experienced with the smartphone—and perhaps an even greater jump.The Internet of Things (IoT) will turn the current rush of industrial data into a rogue wave of truly colossal proportions, threatening to overwhelm even the best-prepared company. As the gigabytes, terabytes, and petabytes of unstructured information pile up, most organizations lack actionable methods to tap into, monetize, and strategically exploit this potentially enormous new value. McKinsey research reveals that companies currently underutilize most of the IoT data they collect. For instance, one oil rig with 30,000 sensors examines only 1 percent of the data collected because it uses the information primarily to detect and control anomalies, ignoring its greatest value, which involves supporting optimization and prediction activities. One effective way to put IoT data to work and cash in on the growing digital bounty involves offering the information on data marketplaces to third parties.
        
        Digital marketplaces are platforms that connect providers and consumers of data sets and data streams, ensuring high quality, consistency, and security. The data suppliers authorize the marketplace to license their information on their behalf following defined terms and conditions. Consumers can play a dual role by providing data back to the marketplace (Exhibit 1).
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Third parties can offer value-added solutions on top of the data the marketplace offers. For example, real-time analytics can make consumer insights more actionable and timelier than ever before. The marketplace also has an exchange platform as a technical base for the exchange of data and services, including platform-as-a-service offers. Six key enablers of the data marketplace can help companies put their data to work more effectively:
        
        Building an ecosystem. By assembling multitudes of third-party participants, companies can increase the relevance of their own digital platforms.
        
        Opening up new monetization opportunities. Today’s interconnected and digitized world increases the value of high-quality data assets while creating innovative revenues streams. One digital marketplace, for example, adds value to Europe’s electric-automobile market by providing information and transactional gateways for businesses such as charging-infrastructure providers, mobility-service players, and vehicle manufacturers. Charging-station operators, for example, are free to determine their own pricing structures based on data available about customer habits and market trends.
        
        Enabling crowdsourcing. Data marketplaces make it possible to share and monetize different types of information to create incremental value. By combining information and analytical models and structures to generate incentives for data suppliers, more participants will deliver data to the platform.
        
        Supporting interoperability. Data marketplaces can define metaformats and abstractions that support cross-device and cross-industry use cases.
        
        Creating a central point of “discoverability.” Marketplaces offer customers a central platform and point of access to satisfy their data needs.
        
        Achieving consistent data quality. Service-level agreements can ensure that marketplaces deliver data of consistently high quality.
        
        As they consider the process of setting up a data marketplace, company leaders need to work through a number of critical questions. An enterprise might ponder the following issues as it clarifies its data-market strategy:
        
        What is the data marketplace’s scope? In most cases, a data marketplace begins when companies set up a central exchange for data within their own organizations. Later, they determine which categories of information within that internal exchange are appropriate (from a security and a profitability perspective) and then allow other players outside their organization (and perhaps outside their industry) to access that data.
        
        How is the marketplace best structured? To foster a dynamic ecosystem, the data marketplace needs to assume a neutral position regarding participants. The legal/tax entity that the marketplace becomes and the structures that govern and finance it are key to this neutrality. Among the guiding principles that players follow in setting up data marketplaces are that a) the marketplace must finance itself through transaction-related fees and commissions, and b) neutrality must extend to future participants that provide or receive data or services, offering indiscriminate access to all interested players under fair terms and conditions. And while the data marketplace will support the creation and definition of data licenses, the data suppliers must nevertheless take responsibility for enforcing and legally auditing them. With respect to the marketplace’s governance, two business models are leading the way. Data marketplaces tend to be either independent platforms or limited ownership hybrids. Under the former model, data sets are bought and sold, while fully owned data-as-a-service providers sell primary data in specific segments or with services and solution wraps. Under the latter, the marketplace collects and aggregates data from multiple publishers or data owners and then sells the data.
        
        Who are the data marketplace’s customers? Once the marketplace is commercially viable, customers will include all types of data providers, and the marketplace system should actively source new kinds of data to become more attractive. The key providers of data will be the companies that capture it, own it, and authorize its sharing. At some point, however, application developers will offer infrastructure and support services that further increase the value of the data by offering a relevant analysis of it and facilitating its delivery.
        
        What are the marketplace’s overall terms and conditions, and data categories? During the marketplace’s technical setup phase, data suppliers define their licensing conditions independently, and the platform provides benchmarks for licensing conditions. The overall terms and conditions of the marketplace apply to all traded data. In the subsequent commercialization phase, the marketplace relies on centrally defined data categories and related licensing agreements as expressed in its general terms and conditions. This strategy enables players to license crowdsourced data independently of specific suppliers.
        
        How does the marketplace relate to other licensing models? When dealing with proprietary data, suppliers usually hold certain information apart and do not share it in the marketplace. However, data suppliers that also offer services can make use of their proprietary data to create services they can trade on the marketplace. For other licensed data, information suppliers can freely create licensing agreements that extend beyond the marketplace—for example, with their strategic partners. Both data amount and type, along with the scope of licenses for using the information, can vary from that of marketplace-supplied data. Likewise, suppliers can also impose separate licensing arrangements for data already traded in the marketplace if buyers intend to use it under different conditions.
        
        What are the role and value-creation potential of the marketplace company or participating data brokers? The potential value of the data will differ depending on whether the marketplace is in the technical start-up phase or has achieved full commercialization (Exhibit 2). In the former, the marketplace acts as a data normalizer, defining standard data models, formats, and attributes for all of the traded information. It syntactically verifies all incoming data compared with the defined standard and continuously manages and extends the data inventory. Once the marketplace enters the commercial stage, it becomes a data aggregator. At this point, in addition to normalizing data and verifying incoming information, it aggregates data and organizes it into logical bundles. For instance, it will enable users to combine data for a given region and offer it to service providers.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        While traditional licensing will provide marketplace revenue streams, participants can also develop transactional models to monetize data and services, with on-demand approaches constituting the preferred approach. With traditional licensing, companies can pursue either perpetual or one-off deals and collect customer fees using several approaches. For example, they can sign contracts with fixed fees and run times, renegotiate expired contracts, or earn revenues at the time of sale (this final approach typically provides less stability in revenue forecasting). At the transactional level, the two primary alternatives are on-demand and subscription services. With on-demand services, customers either pay as they go or choose volume pricing and pay charges based on metrics such as usage volume, the number of incidents, or hardware-related fees. Subscriptions can involve flat fees—typically applied on a monthly or yearly basis—or free/premium (“freemium”) offers, which provide the basics free of charge while offering additional features for a flat fee.
        
        Another monetization option is the “give and take” model, which offers incentives to data providers to share their information. The incentive can be monetary or take the form of something like highly relevant, aggregated data as an enticement to share. The marketplace then aggregates and anonymizes the data and offers it along with associated data-focused services to customers.
        
        One give-and-take example is an Internet-based service that offers geolocated real-time aircraft flight information. The service reportedly has one of the largest online aviation databases, covering hundreds of thousands of aircraft and flights as well as large numbers of airports and airlines. Data suppliers receive free radio equipment that collects and transmits aircraft data and a free business-level membership to the service worth $500 a year for as long as they transmit data. In another case, a large European credit bureau offers credit-rating information for consumers and corporations. Data suppliers provide information that includes banking activities, credit and leasing agreements, and payment defaults. In return, they receive credit-ranking data for individuals or businesses. Yet another give-and-take marketplace focuses on data and performance analytics on mobile-operator network coverage. It trades apps and coverage information to data suppliers in exchange for crowdsourced data that can generate mobile-network coverage maps and reveal a mobile operator’s performance by region and technology (for example, 3G or 4G networks).
        
        A wide variety of traditional commercial data services currently exists, although these services are largely in silos that focus on specific topics, such as healthcare, finance, retail, or marketing. This balkanization provides an opportunity for new, more holistic data-business models. One advantage of the current ubiquity of data providers is that most companies are already familiar with dealing with them. In fact, some sources estimate that 70 percent of large organizations already purchase external data, and all of them are likely to do so by the end of the decade. The value potential inherent in data marketplaces is attracting key players from a variety of advanced industries. A number of aerospace companies, for example, offer systems that provide guidance to customers in areas such as maintenance and troubleshooting. Similar efforts are also under way in the agricultural and mining-equipment industries, among others.
        
        The IoT’s big data promises to help companies understand customer needs, market dynamics, and strategic issues with unmatched precision. But in pursuing this goal, organizations will amass previously unimaginable quantities of information. The data marketplace offers them an innovative way to turn some of that data into cash and reap the benefits that will accrue from building a self-reinforcing ecosystem, enabling crowdsourcing, supporting interoperability, satisfying customer data needs, and improving data quality.The revolution isn’t coming—it’s already under way. In the science of management, the revolution in big data analytics is starting to transform how companies organize, operate, manage talent, and create value. Changes of this magnitude require leadership from the top, and CEOs who embrace this opportunity will increase their companies’ odds of long-term success. Those who ignore or underestimate the eventual impact of this radical shift—and fail to prepare their organizations for the transition—do so at their peril.
        
        It’s easy to see how analytics could get delegated or deprioritized: CEOs are on the hook for performance, and for all of the potential associated with analytics, many leaders operating in the here and now are reporting underwhelming results. In fact, when we surveyed a group of leaders from companies that are committed to big data–analytics initiatives, three-quarters of them reported that their revenue or cost improvements were less than 1 percent. Some of the disconnect between promise and payoff may be attributed to undercounting—the sum of the parts is not always immediately apparent. Ironically, the results of “big data” analytics are often thousands—or more—of incrementally small improvements realized system-wide. Individually, any one of these gains may appear insignificant, but when considered in the aggregate they can pack a major punch.
        
        The shortfalls, however, are more than just a matter of perception, and the pitfalls are real. Critically, an analytics-enabled transformation is as much about a cultural change as it is about parsing the data and putting in place advanced tools. “This is something I got wrong,” admits Jeff Immelt, the CEO of GE. “I thought it was all about technology. I thought if we hired a couple thousand technology people, if we upgraded our software, things like that, that was it. I was wrong. Product managers have to be different; salespeople have to be different; on-site support has to be different.”
        
        CEOs who are committed to a shift of this order, yet wonder how far the organization has truly advanced in its data-analytics journey to date, should start by stimulating a frank discussion with their top team. That includes a clear-eyed assessment of the fundamentals, including your company’s key value drivers, your organization’s existing analytics capabilities, and, perhaps most important, your purpose for committing to analytics in the first place. (See “Making data analytics work for you—instead of the other way around.”) This article poses questions—but not shortcuts—to help a company’s senior leaders determine where they are and what needs to change for their organization to deliver on the promise of advanced analytics.
        
        Immelt reached his conclusions from witnessing—and, in many respects, leading—the revolution. GE’s CEO is keenly aware that so far in the 21st century, the digitization of commerce and media has allowed a handful of US Internet stalwarts to capture almost all the market value created in the consumer sector. To avoid a similar disruption as the industrial world goes online over the coming decade, Immelt is driving a radical shift in the culture and business model of his 124-year-old company. GE is spending $1 billion this year alone to analyze data from sensors on gas turbines, jet engines, oil pipelines, and other machines and aims to triple sales of software products by 2020 to roughly $15 billion. To make sense of those new streams of data, the company is also building a cloud-based platform called Predix, which combines its own information flows with customer data and submits them to analytics software that can lower costs and increase uptime through vastly improved predictive maintenance. Getting this right will require hiring several thousand new software engineers and data scientists, retraining tens of thousands of salespeople and support staff, and fundamentally shifting GE’s business model from product sales coupled with service licenses to outcomes-based subscription pricing. “We want to treat analytics like it’s as core to the company over the next 20 years as material science has been over the past 50 years,” says Immelt.
        
        To understand further the growing power of advanced analytics, consider as well how a consumer-electronics OEM is picking up more speed in an inherently slow-growth market. The company started with a Herculean effort to pull together information on more than 1,000 variables previously collected in silos across millions of devices and sources—product sales and usage data, channel data, online transactions, and service tickets, plus external consumer data from third-party suppliers such as Acxiom. Mining this integrated big data set allowed the company to home in on a dozen or so unrealized opportunities where a shift in investment patterns or processes would really pay off. Armed with a host of new, fine-grained insights on which moves offered the greatest odds to increase sales, reduce churn, and improve product features, the company went on to realize $400 million in incremental revenue increases in year one. As success builds, the leadership has begun to fundamentally rethink how it goes about new-business development and what future capabilities its top managers will require.
        
        But for all the enormous promise, most companies—outside of a few digital natives such as Amazon, Facebook, Google, Netflix, and Uber—have so far struggled to realize anything more than modest gains from their investments in big data, advanced analytics, and machine learning. Many organizations remain preoccupied with classic large-scale IT-infrastructure programs and have not yet mastered the foundational task of creating clean, powerful, linked data assets; building the capabilities they need to extract insight from them; and creating the management capacity required to lead and direct all this toward purposeful action (exhibit).
        
        Exhibit We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Still, similar birthing pains have marked every previous major technology transition as well. And we’re still in the early days of this one: though about 90 percent of the digital data ever created in the world has been generated in just the past two years, only 1 percent of that data has been analyzed. Often, those analyses are conducted as discrete one-offs—nifty experiments, but not much more. Indeed, in many companies, analytics initiatives still seem more like sideline science-fair projects than the core of a cutting-edge business model.
        
        But the potential for significant breakthroughs demands an overhaul of that model, and the speed at which these breakthroughs advance will only accelerate. As computer-processing power and cloud-storage capacity swell, the world’s current data flood becomes a tidal wave. By 2020, some 50 billion smart devices will be connected, along with additional billions of smart sensors, ensuring that the global supply of data will continue to more than double every two years.
        
        All of these developments ensure that there will be a lot of data to analyze. Almost by definition, big data analytics means going deep into the information weeds and crunching the numbers with a technical sophistication that can appear so esoteric that senior leadership may be tempted simply to “leave it to the experts” and disengage from the conversation. But the conversation is worth having. The real power of analytics-enabled insights comes when they become so fully embedded in the culture that their predictions and prescriptions drive a company’s strategy and operations and reshape how the organization delivers on them. Extending analytics from the realm of tactical insights into the heart of the business requires hard work, but the benefits can be profound. Consider, for example:
        
        A global airline stitched together data from multiple operational systems (including those related to aircraft location and aerobridge position) to identify more precisely when and why flights were delayed as they pushed back or arrived at a gate. Its advanced prediction algorithms were able to quantify the knock-on impact of events such as mishandled luggage and helped build a system to alert supervisors in real time so that they could react before potential problems developed. Impact: a reduction in delayed arrivals of about 25 percent over the past 12 months.
        
        A global consumer-packaged-goods company seeking to drive growth across categories integrated a wide range of information (including financial, promotional, and even weather-related factors) into a single data source and then developed sophisticated algorithms to understand the incremental effects that changes based on this source could have at even granular levels. Sifting through disparate data and building up from the ground level enabled the company to identify valuable insights about its competitive landscape as a whole, such as optimal price points and opportunities for new products. Impact: a gross-profit increase in the tens of millions of dollars within one year.
        
        A pharmaceuticals company is using analytics to stem the rising cost of clinical trials. After spending billions of dollars conducting hundreds of trials over the past five years, the company began integrating information on more than 100,000 patient participants with operational data from finance and HR. Out of those tens of millions of data points, it has started to pinpoint which locations are most efficient, which patient-screening techniques increase “pass rates,” and how best to configure its own teams. Analysis of email and calendar data, for example, underscored that improving collaboration between a team leader and two specific roles within clinical operations was among the most significant predictors of delays. The anticipated result: cost savings of more than 10 percent and better-quality outcomes.
        
        And the list goes on: case after case of reduced churn, less fraud, improved collections, better return on investment from marketing and customer acquisition, and enhanced predictive maintenance. Right now, only a few leaders outside the tech sector are truly transforming their organizations with data. But more could be. To that end, we suggest five questions that company leaders should be prepared to explore in depth.
        
        Businesses can waste a lot of energy collecting data and mining them for insights if their efforts aren’t focused on the areas that matter most for the company’s chosen direction. Successful big data and advanced-analytics transformations begin with assessing your own value drivers and capabilities versus those of the competition and developing a picture of the ideal future state, one aligned with the broad business strategy and key use cases. Asking the right questions is the critical first step. These should start big: “What is the size of this opportunity? If I had the additional insights possible through advanced analytics, how much could I save? How much additional revenue could I achieve?” And they should quickly get granular. To frame and develop the right hypotheses, frontline managers must engage alongside the analytics experts throughout the process.
        
        That consumer-electronics OEM’s planning exercise, for example, led the team to ask several questions: “Who are our highest-value customers, how do we reach them, and what do we talk about? How can we drive more cross-sell of our broader portfolio of products and services? Which product features drive the highest usage or engagement, and how do we promote higher adoption of them?” At a leading private bank, the questions from a similar exercise included these: “How can we set optimal price points, day in and day out, and by the thousands each day? Which customers are most at risk of leaving, which are most likely to respond favorably to retention efforts, and what types of retention efforts work best?”
        
        In answering such questions, companies typically identify 10 to 20 key use cases in areas such as revenue growth, customer experience, risk management, and operations where advanced analytics could produce clear-cut improvements. On the basis of that self-assessment and the anticipated impact on earnings, the use cases are ranked and pilot projects are sequenced. Measuring the impact of each use case, with specific indicators and benchmarks, highlights what data are needed and keeps things on track.
        
        A critical foundational step is to overcome obstacles to using existing data. This work could include cleaning up historical data, integrating data from multiple sources, breaking down silos between business units and functions, setting data-governance standards, and deciding where the most important opportunities may lie to generate new internal data—for example, by adding sensors, or, in the case of, say, casinos, by installing webcams to assess high-roller betting behavior.
        
        Most companies, even those with rich internal data, will also conclude they need to mine the far-larger universe of structured and unstructured external sources. When one emerging-markets insurer decided to launch a new peer-to-peer-lending start-up, it realized it could make even better credit decisions by analyzing potential customers’ data and movements on its various platforms, including social networks.
        
        All these data eventually can be pooled into more shareable, accessible assets, such as new “data lakes.” Getting the foundation sorted is not the work of weeks or even months, as anyone who has wrestled with the shortcomings of legacy IT systems knows. And the cost can eventually run into hundreds of millions of dollars, while the full impact of those investments will not always be obvious in one quarter, or two, or three. But that doesn’t mean you should wait for years to capture value. Which makes it all the more important to ask the next question.
        
        Like any transition, the data-and-analytics journey takes place in stages. It’s crucial both to start laying the foundation and to start building analytics capabilities even before the foundation is set. Or, as one of our clients recently recalled as he thought about his company’s successful analytics transformation: “We needed to walk before we could run. And then we ran like hell.”
        
        To step smartly in fast-forward mode, the consumer-electronics OEM created an interim data architecture focused on building and staffing three “insights factories” that could generate actionable recommendations for its highest-priority use cases. While further foundational investments continued in parallel, those factories enabled the early pilots to deliver quick results that made them largely self-funding. The key is to move quickly from data collection to “doing the math,” with an iterative, hypothesis-driven modeling cycle. Such rapid successes help break down silos and build enthusiasm and buy-in among often skeptical frontline managers. Even if it works, a “black box” developed by data scientists working in isolation will usually prove a recipe for rejection. End users need to understand the basic assumptions and how to apply the model’s output: Are its recommendations binding, or is there flexibility to deviate? Will it be integrated directly into core tools such as customer relationship management, or will it be an additional overlay? What visual display will be most useful for the front line—in general, simpler is better—once the data are produced? Pilots should be designed to answer these questions even before the data are collected and the model is built.
        
        Once proof of concept is established and points start going on the board, it’s critical to go big as quickly as possible, which can require an infusion of talent. Best-practice companies rarely cherry-pick one or two specialist profiles to recruit to address isolated challenges. In our recent survey of more than 700 companies, we found that 15 percent of operating-profit increases were linked to the hiring of data-and-analytics experts at scale.
        
        In a recent survey of more than 500 executives, we turned up a distressing finding: while 38 percent of CEOs self-reported that they were leading their companies’ analytics agendas, only 9 percent of the other C-suite executives agreed. They instead identified the chief information officer or some other executive as the true point person. What we’ve got here, to paraphrase the warden in Cool Hand Luke, is more than a failure to communicate; it’s about not walking the talk.
        
        While CEOs and other members of the executive team don’t need to be the experts on data science, they must at least become conversant with a jungle of new jargon and buzzwords (Hadoop, genetic algorithms, in-memory analytics, deep learning, and the like) and understand at a high level the limits of the various kinds of algorithmic models. In addition to constant communication from the top that analytics is a priority and public celebration of successes, small signals such as recommending and showing up for learning opportunities also resonate.
        
        The most important role modeling a CEO can deliver, of course, is to ensure that the right kind of conversations are taking place among the company’s top management. That starts with ensuring that the right people are both in the room and empowered, and then continues with direct intervention and questioning to ensure the transition from experience-based decision making to data-based decision making: Was a conclusion A/B tested? What have we done to build up our capability to conduct rapid prototyping, to test and learn and experiment, to constantly engage in what Google chief economist Hal Varian calls “product kaizen”?
        
        The most important shift, which only the CEO can lead, is to reorganize to put advanced analytics at the center of every core process. The aspiration, in fact, should be to eventually eliminate the distinct term “analytics” from the company lexicon. Data flow through the whole organization, and the analytics should organically follow. “I just think it’s infecting everything we do, in a positive way,” says GE’s Immelt.
        
        Still, even a central nervous system requires a brain—a central analytics hub, or center of excellence. Without a dedicated team and leader, whether a chief analytics officer or a chief data officer or a senior C-level executive clearly tasked with the role, companies struggle to create a distinctive culture that can attract and nurture the best talent. But at the same time, as with a function like finance, individuals connected with the central team should also be embedded in the separate business units. We’ve found that executives from companies with a hybrid model reported a greater impact from analytics on revenue and costs than other respondents did.
        
        What additional roles, skills, and structures are necessary? Clearly, scaling up analytics requires recruiting and retaining a sufficient number of world-class data scientists and model builders. Buying such talent on an outsourced model is only an option for those still in the exploratory phase of their journey. But, to take one example, most banks in the post-stress-test world have created separate, in-house units with their own reporting lines, charged with constantly testing and validating those models to minimize the risk of spurious correlations. We believe this approach makes sense for nonbanks as well.
        
        To turn modeling outputs, however robust, into tangible business actions, companies also need a sufficient supply of “translators,” people able to connect the needs of the business units with the technical skills of the modelers. Don’t assume such “two sport” leaders are easy to find. In our experience, executives often report that attracting and retaining business users with analytics skill sets is actually slightly harder than recruiting those in-demand data scientists themselves. Alongside aggressive recruiting, winning this war for talent requires doubling down on training and improved HR analytics.
        
        In general, most organizations are underinvesting in creating intuitive tools with easy-to-use interfaces that can help frontline managers integrate data into day-to-day processes. Our rule of thumb: for the highest payoff, split your analytics investments roughly 50–50 between spending on building better models and spending on tools and training to ensure that the front line uses the new insights being generated. In many companies, that ratio is still closer to 80–20, or worse.
        
        Beyond big data and analytics, an even broader shift is under way, as robots, machine-learning algorithms, and “soft AI” systems, such as IBM’s Watson, take on more and more of the tasks that human labor used to conduct. Early in 2016, AlphaGo, a system developed by DeepMind, a British company owned by Google, unexpectedly rolled over a celebrated human champion in the ancient game of Go. To prepare for a contest in which, unlike chess, there are more possible positions than grains of sand in the universe, AlphaGo trained itself by playing endless rounds of games, which enabled the path-optimization strategies.
        
        As the use of data and analytics incorporates machine learning, and artificial intelligence continues to blur, humans can take comfort from one near certainty: as proved true in chess after 1997, when IBM’s Deep Blue defeated Garry Kasparov, the new “best players” of Go will turn out to be neither humans nor machines alone, but rather humans working in tandem with machines. Mastering how to leverage that combination may be the ultimate CEO management challenge.
        
        Science-fiction writer Arthur C. Clarke once said that “any sufficiently advanced technology is indistinguishable from magic.” We haven’t advanced to that level—yet. But as the age of big data gives way to the age of advanced analytics and machine learning, we are entering an era where the ability to analyze data will deliver a predictive capability that feels almost like magic.
        
        As in other historic shifts, such as when modern firearms “disrupted” the crossbow, the competition between those who master the new technology and those who don’t will be fierce. But the upside of adaptation is as inspiring as the downside is stark. In the years ahead, the companies and institutions that address these challenges frankly, transform their organizations accordingly, and apply these near-magical abilities seamlessly to the world’s most complex and critical issues will deliver a level of value creation that today we can barely imagine.Few dispute that organizations have more data than ever at their disposal. But actually deriving meaningful insights from that data—and converting knowledge into action—is easier said than done. We spoke with six senior leaders from major organizations and asked them about the challenges and opportunities involved in adopting advanced analytics: Murli Buluswar, chief science officer at AIG; Vince Campisi, chief information officer at GE Software; Ash Gupta, chief risk officer at American Express; Zoher Karu, vice president of global customer optimization and data at eBay; Victor Nilson, senior vice president of big data at AT&T; and Ruben Sigala, chief analytics officer at Caesars Entertainment. An edited transcript of their comments follows.
        
        Murli Buluswar, chief science officer, AIG: The biggest challenge of making the evolution from a knowing culture to a learning culture—from a culture that largely depends on heuristics in decision making to a culture that is much more objective and data driven and embraces the power of data and technology—is really not the cost. Initially, it largely ends up being imagination and inertia.
        
        What I have learned in my last few years is that the power of fear is quite tremendous in evolving oneself to think and act differently today, and to ask questions today that we weren’t asking about our roles before. And it’s that mind-set change—from an expert-based mind-set to one that is much more dynamic and much more learning oriented, as opposed to a fixed mind-set—that I think is fundamental to the sustainable health of any company, large, small, or medium.
        
        Ruben Sigala, chief analytics officer, Caesars Entertainment: What we found challenging, and what I find in my discussions with a lot of my counterparts that is still a challenge, is finding the set of tools that enable organizations to efficiently generate value through the process. I hear about individual wins in certain applications, but having a more sort of cohesive ecosystem in which this is fully integrated is something that I think we are all struggling with, in part because it’s still very early days. Although we’ve been talking about it seemingly quite a bit over the past few years, the technology is still changing; the sources are still evolving.
        
        Zoher Karu, vice president, global customer optimization and data, eBay: One of the biggest challenges is around data privacy and what is shared versus what is not shared. And my perspective on that is consumers are willing to share if there’s value returned. One-way sharing is not going to fly anymore. So how do we protect and how do we harness that information and become a partner with our consumers rather than kind of just a vendor for them?
        
        Ruben Sigala: You have to start with the charter of the organization. You have to be very specific about the aim of the function within the organization and how it’s intended to interact with the broader business. There are some organizations that start with a fairly focused view around support on traditional functions like marketing, pricing, and other specific areas. And then there are other organizations that take a much broader view of the business. I think you have to define that element first.
        
        That helps best inform the appropriate structure, the forums, and then ultimately it sets the more granular levels of operation such as training, recruitment, and so forth. But alignment around how you’re going to drive the business and the way you’re going to interact with the broader organization is absolutely critical. From there, everything else should fall in line. That’s how we started with our path.
        
        Vince Campisi, chief information officer, GE Software: One of the things we’ve learned is when we start and focus on an outcome, it’s a great way to deliver value quickly and get people excited about the opportunity. And it’s taken us to places we haven’t expected to go before. So we may go after a particular outcome and try and organize a data set to accomplish that outcome. Once you do that, people start to bring other sources of data and other things that they want to connect. And it really takes you in a place where you go after a next outcome that you didn’t anticipate going after before. You have to be willing to be a little agile and fluid in how you think about things. But if you start with one outcome and deliver it, you’ll be surprised as to where it takes you next.
        
        Ash Gupta, chief risk officer, American Express: The first change we had to make was just to make our data of higher quality. We have a lot of data, and sometimes we just weren’t using that data and we weren’t paying as much attention to its quality as we now need to. That was, one, to make sure that the data has the right lineage, that the data has the right permissible purpose to serve the customers. This, in my mind, is a journey. We made good progress and we expect to continue to make this progress across our system.
        
        The second area is working with our people and making certain that we are centralizing some aspects of our business. We are centralizing our capabilities and we are democratizing its use. I think the other aspect is that we recognize as a team and as a company that we ourselves do not have sufficient skills, and we require collaboration across all sorts of entities outside of American Express. This collaboration comes from technology innovators, it comes from data providers, it comes from analytical companies. We need to put a full package together for our business colleagues and partners so that it’s a convincing argument that we are developing things together, that we are colearning, and that we are building on top of each other.
        
        Victor Nilson, senior vice president, big data, AT&T: We always start with the customer experience. That’s what matters most. In our customer care centers now, we have a large number of very complex products. Even the simple products sometimes have very complex potential problems or solutions, so the workflow is very complex. So how do we simplify the process for both the customer-care agent and the customer at the same time, whenever there’s an interaction?
        
        We’ve used big data techniques to analyze all the different permutations to augment that experience to more quickly resolve or enhance a particular situation. We take the complexity out and turn it into something simple and actionable. Simultaneously, we can then analyze that data and then go back and say, “Are we optimizing the network proactively in this particular case?” So, we take the optimization not only for the customer care but also for the network, and then tie that together as well.
        
        Vince Campisi: I’ll give you one internal perspective and one external perspective. One is we are doing a lot in what we call enabling a digital thread—how you can connect innovation through engineering, manufacturing, and all the way out to servicing a product. [For more on the company’s “digital thread” approach, see “GE’s Jeff Immelt on digitizing in the industrial space.”] And, within that, we’ve got a focus around brilliant factory. So, take driving supply-chain optimization as an example. We’ve been able to take over 60 different silos of information related to direct-material purchasing, leverage analytics to look at new relationships, and use machine learning to identify tremendous amounts of efficiency in how we procure direct materials that go into our product.
        
        An external example is how we leverage analytics to really make assets perform better. We call it asset performance management. And we’re starting to enable digital industries, like a digital wind farm, where you can leverage analytics to help the machines optimize themselves. So you can help a power-generating provider who uses the same wind that’s come through and, by having the turbines pitch themselves properly and understand how they can optimize that level of wind, we’ve demonstrated the ability to produce up to 10 percent more production of energy off the same amount of wind. It’s an example of using analytics to help a customer generate more yield and more productivity out of their existing capital investment.
        
        Ruben Sigala: Competition for analytical talent is extreme. And preserving and maintaining a base of talent within an organization is difficult, particularly if you view this as a core competency. What we’ve focused on mostly is developing a platform that speaks to what we think is a value proposition that is important to the individuals who are looking to begin a career or to sustain a career within this field.
        
        When we talk about the value proposition, we use terms like having an opportunity to truly affect the outcomes of the business, to have a wide range of analytical exercises that you’ll be challenged with on a regular basis. But, by and large, to be part of an organization that views this as a critical part of how it competes in the marketplace—and then to execute against that regularly. In part, and to do that well, you have to have good training programs, you have to have very specific forms of interaction with the senior team. And you also have to be a part of the organization that actually drives the strategy for the company.
        
        Murli Buluswar: I have found that focusing on the fundamentals of why science was created, what our aspirations are, and how being part of this team will shape the professional evolution of the team members has been pretty profound in attracting the caliber of talent that we care about. And then, of course, comes the even harder part of living that promise on a day-in, day-out basis.
        
        Yes, money is important. My philosophy on money is I want to be in the 75th percentile range; I don’t want to be in the 99th percentile. Because no matter where you are, most people—especially people in the data-science function—have the ability to get a 20 to 30 percent increase in their compensation, should they choose to make a move. My intent is not to try and reduce that gap. My intent is to create an environment and a culture where they see that they’re learning; they see that they’re working on problems that have a broader impact on the company, on the industry, and, through that, on society; and they’re part of a vibrant team that is inspired by why it exists and how it defines success. Focusing on that, to me, is an absolutely critical enabler to attracting the caliber of talent that I need and, for that matter, anyone else would need.
        
        Victor Nilson: Talent is everything, right? You have to have the data, and, clearly, AT&T has a rich wealth of data. But without talent, it’s meaningless. Talent is the differentiator. The right talent will go find the right technologies; the right talent will go solve the problems out there.
        
        We’ve helped contribute in part to the development of many of the new technologies that are emerging in the open-source community. We have the legacy advanced techniques from the labs, we have the emerging Silicon Valley. But we also have mainstream talent across the country, where we have very advanced engineers, we have managers of all levels, and we want to develop their talent even further.
        
        So we’ve delivered over 50,000 big data related training courses just this year alone. And we’re continuing to move forward on that. It’s a whole continuum. It might be just a one-week boot camp, or it might be advanced, PhD-level data science. But we want to continue to develop that talent for those who have the aptitude and interest in it. We want to make sure that they can develop their skills and then tie that together with the tools to maximize their productivity.
        
        Zoher Karu: Talent is critical along any data and analytics journey. And analytics talent by itself is no longer sufficient, in my opinion. We cannot have people with singular skills. And the way I build out my organization is I look for people with a major and a minor. You can major in analytics, but you can minor in marketing strategy. Because if you don’t have a minor, how are you going to communicate with other parts of the organization? Otherwise, the pure data scientist will not be able to talk to the database administrator, who will not be able to talk to the market-research person, who which will not be able to talk to the email-channel owner, for example. You need to make sound business decisions, based on analytics, that can scale.Over the past several years, many companies have avidly pursued the promised benefits of big data and advanced analytics. In a recent McKinsey survey of executives in this field, nearly all of them said that their organizations had made significant investments, from data warehouses to analytics programs. But practitioners have raised questions about the magnitude and timing of the returns on such investments. In 2014, for example, we conducted a poll of senior executives and found that they had seen only modest revenue and cost improvements from them in the previous year.
        
        Our latest research investigated the returns on big data investments for a random sample of 714 companies around the world, encompassing a mix of industries and company sizes typical of most advanced economies. Our findings paint a more nuanced picture of data analytics. When we evaluated its profitability and value-added productivity benefits, we found that they appear to be substantial—similar, in fact, to those experienced during earlier periods of intense IT investment. Our results indicated that to produce these significant returns, companies need to invest substantially in data-analytics talent and in big data IT capabilities.
        
        Yet we also found that while data-analytics investments significantly increased value-added or operating profits, the simple revenue impact for consumer companies was considerably lower. This finding, mirrored among B2B companies on the cost side, appears to confirm the intuition of executives struggling to uncover simple performance correlations. The time frame of the analysis also seems to be important, since broader performance improvements from large-scale investments in data-analytics talent often don’t appear right away.
        
        The research avoided overweighting technology companies, since many denizens of the C-suite say that “we know that digital natives capture big returns, but does their experience apply to those of us who live in a hard-wired universe of factories and distribution channels?” Operating profit was used to measure returns, since it captures the impact of big data both through value-added productivity and pricing power (often resulting from better customer targeting). The data also allowed us to understand other aspects of the returns on these investments—for example, the advantages of being the first data-analytics mover in a given market.
        
        We took care to measure the returns from technologies specifically linked to big data and therefore considered only analytics investments tied to data architecture (such as servers and data-management tools) that can handle really big data. Looking beyond capital spending, we assessed complementary investments in big data talent across eight key roles, such as data scientists, analysts, and architects. Finally, we examined whether improvements were radiating throughout organizations or captured only in narrower functions or individual businesses.
        
        Our research looked at the results of big data spending across three major business domains—operations, customer-facing functions, and strategic and business intelligence. These were our key findings:
        
        History tells us that it takes time for new technologies to gather force and diffuse throughout an economy, ultimately producing tangible benefits for companies. Big data analytics—the most recent major technology wave—appears to be following that pattern. The average initial increase in profits from big data investments was 6 percent for the companies we studied. That increased to 9 percent for investments spanning five years, since the companies that made them presumably benefited from the greater diffusion of data analytics over that period. Looked at from another vantage point, big data investments amounted to 0.6 percent of corporate revenues and returned a multiple of 1.4 times that level of investment, increasing to 2.0 times over five years. That’s not only in the range of the 1.1 to 1.9 multiples observed in the computer-investment cycle of the ’80s but also exceeds the multiples others have identified for R&D and marketing expenditures.
        
        Companies, we found, benefit broadly from big data investments. With minor variations, spending on analytics to gain competitive intelligence on future market conditions, to target customers more successfully, and to optimize operations and supply chains generated operating-profit increases in the 6 percent range. Although companies struggle to roll out big data initiatives across the whole organization, these results suggest that efforts to democratize usage—getting analytics tools in the hands of as many different kinds of frontline employees as possible—will yield broad performance improvements.
        
        Our research helped us identify how significantly early investments in big data analytics can raise the pace at which operating profits improve: first movers accounted for about 25 percent of the increase in our sample. One possible explanation is that early adoption allows companies to learn by trial and error how best to design data-analytics technology and integrate it into their workflows. This, in turn, could create valuable capabilities that help companies differentiate themselves from competitors. If the cycle continues as increasingly powerful data-analytics applications come on stream, the importance of rapid experimentation and learning—and of leaders who feel comfortable with this approach—could rise.
        
        Many companies still compartmentalize their data-analytics initiatives—for example, by making IT-architecture investments in isolation. That’s a mistake: about 40 percent of the profit improvements we measured resulted from complementary and coordinated investments both in IT and in big data talent. Organizational constraints can make such gains difficult to achieve, of course, since companies often silo their investments. For instance, the IT or technology department is commonly tasked with determining the level of big data investments needed, while business units and HR departments draft their own spending plans for employee resources.
        
        We find that when companies fully coordinate their investments in IT capital with those in skilled roles, performance improves substantially. Here’s an example of what happens when they don’t coordinate them: one company’s large investment in database-management software foundered when HR neglected to hire the analysts needed to support the new data-driven business priorities. Experience also tells us that in the most capable organizations, a chief data or analytics officer often coordinates IT spending with efforts to acquire analytical talent across business units.
        
        Skilled employees across the spectrum of data-analytics roles are in short supply, so aggressive actions to address this problem are critical. Our study found that 15 percent of operating-profit increases from big data analytics were linked to the hiring of data and analytics experts. Best-practice companies rarely cherry-pick one or two specialist profiles to address isolated challenges. Instead, they build departments at scale from the start. With a broad range of talent, these companies can use data analytics to address the current challenges of their functional areas while developing forward-facing applications to stay ahead of competitors.
        
        Combined, these three investment characteristics account for about 80 percent of the operating-profit increases in our study. Staying on top of new developments, carefully balancing investments in skills and technologies, and becoming a magnet for cutting-edge talent will be the paramount considerations for leaders keen to turn their modest data-analytics gains into broader and more substantial ones.Although insurance carriers and actuaries have been using analytics for decades, “advanced analytics” has emerged as a hot topic in the media and at industry conferences in recent years. Executives at large and small carriers alike have been building centers of excellence (COEs), with dedicated staff focused on advanced analytics, also known as data science.
        
        These investments have delivered successes in some areas, including the use of claims modeling in workers’ compensation, catastrophe modeling in property insurance, sophisticated rating algorithms in personal auto, and fraud identification in both property-and-casualty (P&C) and life-insurance claims. Progress has been slower in other lines of business, such as general liability, most specialty lines, and other elements of life insurance. Overall, carriers have seen mixed results from newly established COEs; there have been clear wins in some cases, while in others, the jury is still out. However, industry executives broadly agree that advanced analytics can be used to drive value in insurance. Even many seasoned underwriters have conceded—perhaps grudgingly—that rigorous and widespread use of data can yield significant benefits.
        
        There are some best practices for starting the journey toward adoption of advanced analytics in insurance. This paper moves beyond mastering use cases and establishing a COE, to describing how advanced analytics can transform the business and change a carrier’s operating model. This journey includes four phases (Exhibit 1):
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Building insights. Initially, companies develop models that demonstrate how analytics can add new insights and deliver clear added value. However, these models are often developed in isolation from the business, and the company struggles with frontline adoption. Capturing value. As the analytics function matures, model builders work closely with frontline staff, who become involved in the nuts and bolts of building the model. The focus shifts from developing models to their adoption, and the models begin to come to life. Even if their insights are not fully applied, the models are seen as tools that enhance, rather than hamper, decision making. Achieving scale. The company has put in place a COE and established mature and transparent processes related to the COE’s work and the value it delivers. The COE also has a clearly defined process for bringing analytics solutions to market rapidly, working collaboratively with IT. An established set of centralized capabilities is emerging, including third-party-data procurement, model libraries and code sharing, and analytics-talent attraction and retention. Clear analytics leadership has been established within each major business unit and function. Becoming an analytics-driven organization. Analytics becomes the backbone for conducting business, shifting from an enabling role to one that is central to the business, and the impact of analytics is measured as part of core business results. Analytics drives underwriting, product development, claims, and distribution, and barriers between siloed functions dissolve. The system becomes more complex, with greater involvement of third parties. The talent strategy for these organizations focuses on analytic skills.
        
        In the initial phase, carriers develop models that demonstrate early evidence of success. This is a critical proof of concept for analytics that justifies further investments. Key success factors include providing insights that enhance decision making, focusing on data that matter, and embracing advanced analytics and science.
        
        Analytics cannot perfectly predict outcomes, particularly in low-frequency, high-severity, or shock-prone lines of business. For instance, during the past decade, the market for directors-and-officers-liability insurance endured waves of litigation—and subsequent spikes in claims—resulting from shock events like the financial crisis, the Madoff scandal, and new regulations governing options backdating. Analytics would have had difficulty predicting any of these events or their impact on any single risk or company. But even when analytics lack predictive value, they can enhance specific types of decisions with novel, fact-based insights. Examples include modeling the ideal attachment point (based on price versus anticipated loss at a particular layer), modeling the right level of deductible or retention, and modeling to inform debits, credits, or exclusions.
        
        In life insurance, similarly, data and analytics have not yet been able to replicate the rigor of biometric underwriting, though external data (for example, credit scores and motor-vehicle records) are widely used to supplement underwriting.
        
        Carriers have gained new insights from external sources of data. For instance, in the past five to ten years, granular geocoding has promoted a more precise understanding of geographic proximity to potential hazards. Pharmacy records have proved to be a good source of supplemental data for many life-insurance carriers. And as with personal auto, credit scores for small-business owners have proved to be a source of insight about management attitudes, which indirectly indicate a company’s riskiness.
        
        While there is untapped potential for obtaining new insights from external data, many carriers still struggle to master their internal data, which often remain disaggregated, unstructured, and generally underutilized. Typically, internal data are incomplete or miscoded, and substantial effort is required to bring the data into working condition. Consequently, improved capabilities for mining existing data would generate significant value. By demonstrating their ability to make the most of the data they already collect, carriers would “earn the right” to mine external data.
        
        The analytics landscape is rapidly evolving. Carriers can choose from many new analytics platforms. For instance, SPSS and even SAS are quickly being displaced by more modern, versatile open-source languages like Julia, Python, and R. There are also new platforms—such as Hadoop, Spark, and Storm—with higher processing capabilities that can handle real-time, unstructured data. The industry is also experimenting with cognitive computing and artificial intelligence. However, the latest technology is not always needed. For instance, real-time, unstructured processing of large volumes of data is not required when underwriting a large life policy or when developing annual rates on a portfolio comprising a few hundred or few thousand accounts, many of which have just dozens of variables.
        
        That said, modern science has the potential to greatly enhance analytics techniques, many of which are decades old and predate the advent of the computer. Machine learning in particular—which relies on automated, computer-program-driven pattern recognition—has been proved to produce a stronger signal and better fit than general linear models (GLM). The traditional process to develop insurance ratings has relied on human observations to find the variables that will predict whether a policy will be profitable or a claim will be especially severe. In contrast, machine learning automates the process and, to some extent, removes subjectivity.
        
        Actuaries at carriers have not yet embraced machine learning. To be fair, the clusters of variables that machine learning produces are more complex than traditional GLM output and could add significant complexity to building and executing rating plans (which also need to be explained to each state’s regulators). And because pattern recognition is conducted by the machine instead of humans, the results may not be intuitive or easy to explain. Even so, machine learning has demonstrated a superior ability to work with data-sparse contexts and produce superior lift compared with current GLM methods. Successful carriers will more systematically adopt machine learning across most lines of business to supplement and eventually replace GLM.
        
        As the analytics function matures, the focus shifts from the models to adoption, and the models come to life. Models are seen as critical enablers of better decision making. Critical success factors include frontline involvement, seamless work-flow integration, and performance management that specifically tracks the use of analytics.
        
        Analytics is more than modeling; it requires a deep grounding in the business and should be seen as an iterative, end-to-end process involving the front line at each step (Exhibit 2). However, carriers often fail to involve business leaders and frontline users adequately throughout this process. The traditional way of working still prevails: build the model or new tool, then roll it out to the front line. New models are often based on unclear assumptions, and the front line does not understand precisely how to apply the output. For example, is the model’s recommendation binding, or is there flexibility to deviate from it?
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Best practice entails relentlessly involving the front line in the nuts and bolts of building the model, from the earliest stages to completion.
        
        Among the most difficult issues in building any model is determining how exactly it will be used. Will it be integrated directly into core tools for customer relationship management and pricing, or will it be an additional tool or overlay to current processes? Which types of employees will use it, and how often? Pilots should be designed to answer these questions even before the data are collected and the model itself is built.
        
        Data visualization is another critical but difficult challenge: what will the front line see once the output is produced? As new models are developed, emphasis should be placed on how the output will be explained and understood.
        
        Diligently tracking the impact of use cases—particularly their adoption and usage—is a key attribute of maturing analytics organizations. There are various levels of tracking, including economic impact within business metrics, the impact compared with those who don’t adopt the model, and user satisfaction. Measuring both adoption and user satisfaction is valuable for gauging the quality of the models. Users may not initially be satisfied with models as they are rolled out, particularly given the new way of doing business; however, tracking user adoption and satisfaction over time provides an important fact base that can be used to calibrate the success of models across businesses, as well as to indicate when version 2.0 or 3.0 is needed.
        
        As carriers master the execution of use cases, the next step is to build a permanent, scalable COE to support the businesses. When successful, the COE will support many, if not most, of the businesses and will also cover several functions—pricing and underwriting, claims, distribution, and operations. We see three critical success factors for COEs: balancing business engagement with a strong central function; having an integrated analytics strategy, including an iterative, evolving road map of use cases; and directly involving top management.
        
        Carriers struggle with how to position an analytics COE relative to the businesses. Should the COE be autonomous and have its own reporting and profit-and-loss statements? Or should it be less prominent, functioning as a resource that businesses access on demand, similar to offshore captives for operations or the IT function? Each option offers clear pros and cons (Exhibit 3). Having a more autonomous function creates more transparency and more accountability for specific use cases, and it generally allows a stronger, more proactive function to guide and challenge the business. The downsides are less business ownership and greater distance from the business. An on-demand model has the benefit of more closely aligning the COE with the business agenda, but the COE is less proactive and prominent than it would be if it drove analytics independently. There is also a risk that a weaker COE will operate at a slower pace.
        
        Exhibit 3 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The best approach lies somewhere in the middle. The COE needs “teeth” to come up with ideas and recommendations proactively, but the business needs to shape and approve the COE’s agenda and be convinced of the COE’s value, including that the costs allocated to it are worthwhile.
        
        Success requires a combination of strong business leadership and analytics leadership. The strong business leader supports the analytics COE as it experiences the inevitable fits and starts in scaling up, and the strong analytics leader can promote the COE’s services to each major business unit and function.
        
        The application of advanced analytics may start in the pricing-and-underwriting function, which already applies modeling and data-driven analyses to some extent. Here, advanced analytics enhance or improve upon previous practices—for instance, finding new variables, exploring new modeling techniques, and making processes more automated. Typically, claims also is a rich area for analytics—for instance, anticipating claims severity, identifying opportunities for subrogation, or better managing loss-adjustment expenses. Recently, analytics have been successfully applied in other domains, particularly servicing and distribution. Precise management of the in-force book, coupled with better new-business targeting and funnel management, does more than yield growth in the top line. It also leads to profitable growth by identifying, attracting, and retaining better-quality risks and customers with higher lifetime value.
        
        Leading organizations have developed heat maps of opportunities that span all businesses and functions. These opportunities are prioritized based on impact, feasibility, and business priorities, then translated into a road map of use cases. This road map should cover all functions and businesses, so there is a test-and-learn cycle across the organization that shows where analytics are most effective. The road map is dynamic and updated annually, if not more frequently. There are also longer-term speculative initiatives that could have significant payoffs. For example, many carriers are exploring the potential for mining unstructured text and voice data—such as call-center data, engineering reports, and claims files.
        
        As the COE scales up, senior management makes it a critical corporate priority, paying close attention to the portfolio of initiatives and gaining a basic understanding of how the initiatives have achieved tangible impact. As part of the annual planning cycle, executives personally encourage line leadership to contribute proactively to the pipeline of analytics ideas, and the success of analytics initiatives becomes measured as a part of performance management. To fulfill this role effectively, top managers need to build a basic understanding of the techniques, tools, and technologies that drive the use of analytics.
        
        Although most carriers have not yet reached the highest level of maturity, for many the journey is well under way. As carriers become analytics driven, they adopt new ways of doing business centered on analytics. Analytics shift from being an enabler to being the core way of doing business. The COE structure becomes embedded more directly in individual businesses, having realized its goals of temporarily building new muscle for analytics and initiating progress.
        
        In the mature organization, all decisions, whether related to the business or to core support functions, are enabled by analytics, at least where modeling and data-based analyses are possible. Analytics can take many different forms, such as describing and visualizing key trends, predicting future outcomes, and prescribing actions. Across the board, senior executives demand facts and data to inform decision making, and no longer rely on static reports.
        
        As an organization reaches analytic maturity, the COE structure—in which analytics are separate from the business—becomes redundant, because analytics becomes ingrained in the company’s way of doing business. Analytics also becomes the critical function around which decision making is organized. For instance, today there are practical reasons to separate distribution, claims, and underwriting and pricing; these are different processes and require different skills. In the future, these various functions may still exist because the practical activities differ, but the core decision making, as well as the analytics engine supporting the decisions, will converge at a single point. Key decision makers will have integrated dashboards that provide a full view of the business, cutting across functions, including the distribution funnel, underwriting and pricing decisions, portfolio and risk performance, product performance, and details on the drivers of losses and claims. Very early prototypes of such 360-degree analytics exist, but most carriers are far from having this integrated view. As analytics becomes more integrated, so will decision making.
        
        In the mature organization, the business system is also more expansive, with greater involvement of third parties—whether as partners to conduct analytics or as providers of unique data and assets.
        
        Successful organizations will have clear, quantitative measurements to track the performance of all key business metrics. While they track the success of individual use cases, they do not track the financial impact of analytics separately from key business metrics. The business metrics themselves become the markers of success. In P&C, for instance, the metrics would be price adequacy and other technical price metrics, in addition to loss, expense, and combined ratios. In life insurance, measures could be the quality of new-business growth and in-force lapse, among others.
        
        Earlier in the maturity curve, organizations make significant investments in analytics and therefore need to see the return on investment and be confident of the impact. Executives seek to isolate the financial impact from particular analytics initiatives. While having this disciplined measurement can be critical, it can also be a distraction, particularly since trying to isolate the economic impact of analytics, as distinct from other business initiatives, can become an exercise in false precision.
        
        In the analytics-driven organization, analytics are used to track and manage HR decisions—for instance, assessing profiles and traits of successful staff to inform recruiting, identifying and incubating future leadership, and proactively managing retention. Redefinition of individual roles increases, so that using analytics is a core part of the work description and expectations. In addition to using analytics, staff members are required to contribute to innovation and the development of new use cases. This contribution becomes a core part of performance reviews. HR strategy explicitly promotes a culture where analytics becomes an integral part of each role.
        
        In their efforts to capture the full potential of analytics, most carriers are somewhere between phase two and phase three. Many have achieved pockets of success and validated the potential of analytics. They have also attempted to build central, scalable resources to roll out analytics more broadly. CEOs can push progress beyond this middle ground in four ways:
        
        Make analytics a senior-leadership priority. Analytics should be a relentless priority of the CEO. It is not just one of several themes, but the target end state for the organization. Make a multiyear commitment. Investments over several years are required, and they should account for false starts and trial and error. Advancing on the maturity curve does not happen in a year, and the impact may not be obvious within the first few quarters. Demand fast progress. While the full impact takes several years, quick wins and success stories will help prove the concept and maintain momentum. CEOs should personally look for several use cases each year that demonstrate new and incremental impact from analytics, and they should promote these successes as examples for the entire organization. Find the right analytics leadership. Many business leaders are reluctant to shift away from the business into a supporting function, and these leaders also do not have the practical skills or experience to drive analytics. At the same time, statisticians and data scientists often lack the skill to navigate the businesses and lead critical change management. Finding the “two-sport manager” who can bridge the gap between the COE and the business is a prerequisite for success.
        
        Even though many carriers have made progress in building a dedicated, central capability, they have only scratched the surface in realizing the impact of analytics. The first carriers to make this leap and successfully bring science to insurance are likely to capture an unrivaled competitive advantage.Our global network of deeply experienced insurance partners work with property and casualty (P&C), life insurance, and reinsurance carriers, industry associations, brokers, and other institutions to address issues on topics including strategy, organization, operations, technology, marketing, sales, and risk. We focus on core operating capabilities and help clients take a long-term, through-cycle view of the evolving competitive and regulatory landscape.
        
        Our expertise in P&C insurance encompasses claims management, where we work with clients to reduce costs through high-quality processes; underwriting, where we help commercial lines carriers improve their technical results; and operations and technology, supporting insurers with operating model review and redesign, outsourcing and offshoring, and infrastructure consolidation.
        
        We help life insurers optimize capital and use value creation as a key metric; we provide insights on branding strategy, sales force performance improvement, and distribution; and we help clients assess the impact of regulatory change.
        
        Across North America, Europe, and Asia, we serve the majority of leading carriers. Recent engagements include:
        
        conducting an underwriting file review to identify and address key drivers of pricing leakage in life insurance
        
        Global Insurance Pools. Comprehensive database captures global numbers on premiums, assets, profits, and growth
        
        Comprehensive database captures global numbers on premiums, assets, profits, and growth Insurance 360°. Insurance cost and performance benchmarking, covering the complete value chain from product development to operations and support.
        
        Insurance cost and performance benchmarking, covering the complete value chain from product development to operations and support. Ingenuity. Advanced analytics for the insurance industry.In 2014, the insurance industry staged an impressive recovery, with anticipated global growth of 6.3 percent—far exceeding the 2.8 percent reported in 2013—and total premiums reaching EUR 3.8 trillion. Growth in 2014 was also notable because it was higher than nominal GDP growth for the first time in five years.
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        What factors help explain the industry’s strong performance? Preliminary reports suggest that Health showed the highest growth, while Life demonstrated a strong recovery in growth of gross written premiums (GWP) from 2013 to 2014. Growth in P&C insurance remained relatively stable, at a decent 5 percent. Early reports also show that emerging markets grew significantly more than mature markets (12 percent vs. 5 percent), mainly because of their lower penetration levels and higher nominal GDP growth. The only two regions experiencing double-digit growth were Emerging Asia and Latin America.
        
        As in previous years, the penetration rate for mature markets (8 percent) far exceeds that of emerging markets (3 percent). This pattern will continue, since the growth seen in emerging markets is not yet strong enough to largely surpass nominal GDP growth and thus to increase penetration.
        
        Life. Most regions saw positive Life growth in 2014, but the amount of the increase, as well as the factors responsible, varied by region. In a marked departure from 2013, the US, Japan and Mature Asia demonstrated the strongest gains. Of all Life products, endowments experienced the most growth, primarily because of a resurgence in sales at Italian banks, but unit-linked products also performed well. Life Return on Equity (RoE) rose from 11.5 percent in 2012 to 12.7 percent in 2013 as equity markets were strong.
        
        Most regions saw positive Life growth in 2014, but the amount of the increase, as well as the factors responsible, varied by region. In a marked departure from 2013, the US, Japan and Mature Asia demonstrated the strongest gains. Of all Life products, endowments experienced the most growth, primarily because of a resurgence in sales at Italian banks, but unit-linked products also performed well. Life Return on Equity (RoE) rose from 11.5 percent in 2012 to 12.7 percent in 2013 as equity markets were strong. P&C. Growth in the P&C market held steady in 2014, coming in at 5 percent. The main growth drivers were Motor in emerging markets and Fire and Property in mature markets. At the regional level, growth was stable, except for a significant slowdown in Eastern Europe. As in 2013, Emerging Asia and Latin America are driving most of the P&C growth. For profitability, P&C RoE increased in almost all regions in 2013, reaching 11.5 percent globally.
        
        Growth in the P&C market held steady in 2014, coming in at 5 percent. The main growth drivers were Motor in emerging markets and Fire and Property in mature markets. At the regional level, growth was stable, except for a significant slowdown in Eastern Europe. As in 2013, Emerging Asia and Latin America are driving most of the P&C growth. For profitability, P&C RoE increased in almost all regions in 2013, reaching 11.5 percent globally. Health. The Health segment grew 8 percent in 2014, up from 5 percent annually in the two previous years. Emerging markets are rapidly gaining share and should account for about 8 percent of global premiums in 2014 (up from 5 percent in 2008). Emerging Asia and Latin America achieved the most growth, at 31 percent and 23 percent, respectively. The US is also expected to contribute strongly to growth as the Affordable Care Act is raising the number of insured. Globally, the net combined ratio for Health has been under 100 percent, on average, for the last decade, with emerging markets generally reporting higher numbers than mature markets.
        
        US and European insurers, which once ruled the global ranks, have been steadily losing ground to Asian companies as emerging markets grow and mature markets slow down. Although both US and European companies have expanded into emerging markets, they have faced many challenges and still depend on the slow-growing mature markets for most of their business. Their struggle will likely continue, since local insurers are becoming more competitive.
        
        Most insurers have managed to improve their RoE in recent years, but the largest insurers are not necessarily capturing a disproportionate share of profits in Life and Non-Life in all countries, as analysis in our paper will show. Mergers and acquisitions (M&A) have been declining across all regions, mostly because companies lack available free capital. A reversal of the downward trend is not observed yet, as several challenges to consolidation still need to be addressed, however, there are some fundamental factors in place that could help increase M&A activity over the long term.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        GIP features a proprietary Markets Database containing over 150,000 data points covering the largest 64 countries worldwide and 99 percent of global insurance premiums. It includes key financial indicators for every market, from 2000 to the present, and projections to 2020.
        
        The forecasts in our paper are based on a consensus macroeconomic scenario provided by Oxford Economics, and the informed judgment of McKinsey’s experts. The Oxford forecast assumes average global nominal GDP growth of 6.3 percent for 2014 through 2020 (compared to 5.4 percent for the previous decade) and a gradual increase in interest rates (which some would consider an optimistic view). The scenario does not include potential macroeconomic and regulatory threats.
        
        GIP recently expanded to include information on individual insurers. It now provides integrated data on selected global and local insurers and information on performance benchmarking.
        
        Local Insurers Database, including key financial indicators for the top 15 largest local insurers in 11 individual insurance markets, as well as premium data for the 10 largest insurers in more than 50 countries globally.
        
        Global Insurers Database, including key financial statement information for over 100 major global insurers, including their split for Life and Non-Life.
        
        Performance benchmarking. GIP’s tailored performance benchmarking allow insurance companies to compare themselves to their peers. This in-depth analysis covers capital markets performance; financial performance for total business, Life, and Non-Life; and country-level performance.
        
        McKinsey’s Global Insurance Pools can help insurers along several dimensions. A “Granularity of Growth” analysis can identify a company’s specific drivers of growth; our databases can also help to benchmark the company’s growth and profitability against market performance and competitors and identify the impact of different macro-economic scenarios on growth and future market shares.
        
        For more, download the full report on which this article is based, Global Insurance Insights: A detailed analysis of trends that shape the industry (PDF–2.35MB).We use cookies essential for this site to function well. Please click "Accept" to help us improve its usefulness with additional cookies. Learn about our use of cookies, and collaboration with select social media and trusted analytics partners hereLearn more about cookies, Opens in new tab.Today’s unforgiving economic climate confronts insurers with a multitude of challenges, from the low-interest-rate environment to greater price transparency, customer cost consciousness, and sweeping regulatory changes. As a result, the profitability of life and property-and-casualty (P&C) players is barely above the cost of equity, and costs are emerging as the critical element in achieving competitive advantage. Yet when it comes to explaining persistent cost gaps compared with their peers, insurers often cite factors they regard as fixed—business-model decisions such as size, sales channels, product mix, and geography. Our latest research undermines that excuse: we found that management often matters more, and that’s how a handful of players have been able to demonstrate successful long-term cost management.
        
        To uncover how top performers keep a lid on costs, we turned to our Insurance 360º benchmarking survey and its database of 38 life, 33 P&C, and 9 health insurers. The results revealed significant cost differences within the sector. In fact, the differences in operational costs between top- and bottom-quartile players were consistently more than 60 percent across every business function and, in some cases, bottom-quartile players had unit costs more than twice those of top-tier players (Exhibit 1).
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        However, this only tells part of the story. We also found that the cost drivers that are often considered carved in stone for insurers—size, sales channel, product mix, and geography—accounted for just 19 percent of the differences in unit costs among P&C insurers and 46 percent among life insurers. In short, differences in these four cost drivers simply do not explain the bulk of the disparity among industry players. So what does?
        
        Our analyses of the benchmarking data, combined with insights from our discussions with industry executives, suggest that four distinct root causes explain both the large remaining cost-level disparities and why insurers fail to optimize immutable cost drivers. All come back to what we regard as issues around management: business complexity, operating model, IT landscape, and performance management (Exhibit 2).
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Business complexity. Business complexity related to brands, sales channels, product mix, or customer-facing processes is an important driver of operating costs and limits insurers’ ability to leverage economies of scale. As a result, insurers with very large product portfolios and multiple brands and channels are also those with the highest costs on average. One insurer, for example, had introduced numerous individually negotiated discount schemes with a large number of brokers, leading to high complexity and sometimes inadequately priced contracts.
        
        Operating model. High-cost players tend not to have consolidated or optimized the setup of their operating units. They may have back-office functions distributed across numerous locations with different processes and governance structures. Workload backlogs may occur in conjunction with underutilization, leading to a drop in customer satisfaction and deteriorating financial performance. Addressing these issues requires a comprehensive examination of the operating model, including processes, location footprint, supporting technology, employee skills, sourcing, and organization and governance structures. There is no one-size-fits-all solution—what optimization means will inevitably depend on the context. One insurer, for example, consolidated its existing operating units in one central location to gain economies of scale, while another created six global centralized operating units to have a location-optimized footprint and be close to local business units. By digitizing its insurance processes, another managed to reduce claims-regulation costs by 20 to 30 percent, processing costs by 50 to 65 percent, and processing time by 50 to 90 percent—and simultaneously improve customer service.
        
        IT systems. A fragmented legacy IT landscape is often a root cause for failing to leverage economies of scale, driving high IT costs as well as mushrooming operational costs. When comparing the number of policies per full-time employee in operations for both life and P&C with IT spending per full-time employee, we found that insurers with complex legacy systems tended to have both high IT spending and low productivity, while those with streamlined IT managed to achieve high productivity with limited IT expenditure. One company tackled this issue by completely overhauling its P&C IT landscape, replacing legacy core systems with state-of-the-art standard software. As a result, it dramatically reduced costs per policy and cut time to market for new products to just a few weeks. In its life business—which still used the legacy IT system—costs and time to market remained in the bottom quartile. We also found that lean IT efforts can reduce costs by 20 to 40 percent, as well as cut errors.
        
        Performance management. Performance management drives cost outcomes across all areas. We identified a frequent lack of rigorous performance management, resulting in costs rising again just a few years after the implementation of cost-reduction measures. Reducing costs sustainably requires embedding cost consciousness and continuous improvement into a company’s culture: a one-off program will not suffice. A sustainable performance-management approach means simultaneously changing mind-sets and behaviors, defining new performance metrics and targets, designing new processes, and establishing performance dialogues—all of which need to cascade through the organization. At one P&C insurer, for example, improving performance management in this way led to a 20 percent reduction in back-office costs.
        
        Insurers still have the ability to improve costs in the four areas viewed as immutable: size, sales channel, product mix, and geography. Yet our research found that most of the cost differences among companies actually relate to management. Factoring in these imperatives allows insurers to do much more than just improve their expense ratios: it will give them the freedom to make the investments they need to continue to compete on the global stage.
        
        Download the full report on which this article is based, Successfully reducing insurance operating costs: Insights from McKinsey’s Insurance 360° benchmarking (PDF–585KB).The inflection point for market takeoff in life insurance is relatively high: sales rarely surge until GDP per capita reaches about $30,000. Higher income levels, particularly when reinforced by cultural preferences for locking in financial security, correlate strongly with insurance-market penetration. As emerging markets get richer over the next decade, they will fuel more than 80 percent of life-insurance companies’ global growth.
        
        Exhibit While far from deterministic, GDP per capita above $30,000 seems to correlate with higher life-insurance penetration. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.comThe US life-insurance industry, as a group, has returned less than its cost of equity since 1985. That year, life insurers represented about 40 percent of the financial-services industry in market capitalization; today, that’s down to 25 percent. Life insurers have lost ground to banks, asset managers, and brokerage firms, and we believe the principal cause was the decision by many life insurers to move beyond products where they enjoyed a distinct competitive advantage—such as products where they manage poolable risk—to businesses where they do not. Ironically, this search for higher returns resulted in just the opposite: while riskier investment strategies boosted profits when markets cooperated, economic downturns more than erased those gains.
        
        Yet not all life insurers have struggled. Despite the overall deterioration in the industry’s performance during the past decade, our research revealed a stunning spread in value creation among the 30 largest life insurers in the US market. Those in the top quintile increased in value by about 10 percent annually, while those in the bottom quintile declined in value by about 3 percent—creating a 400 percent difference in adjusted book-value growth over the past decade. So what do the winners do differently? Do they have a more attractive product mix? Execute better within product lines? Achieve higher investment returns?
        
        Several years ago, McKinsey launched “Life Journey,” a long-term effort to develop fact-based answers to these questions. We analyzed publicly available data for the life-insurance industry since 1985 and for the top 30 life insurers since 2000, and we interviewed dozens of industry analysts and executives. What we found is that the performance gap has been driven primarily by superior execution skills in managing liability risk, rather than from managing asset-based investment performance or product mix (Exhibit 1).
        
        Exhibit 1 Carriers have made most of their profits by managing poolable risks. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        While many factors have contributed to the performance differential in managing liability risk among the top 30 insurers, none has been more important than skill in managing poolable risk—a risk type that has delivered close to three-quarters of the industry’s profits using less than half its capital. This is striking given how little mindshare and resources life insurers are investing in risk-skill innovation. Another interesting insight from this inquiry is the limited role portfolio mix has played in explaining the spread in performance (only 25 percent difference between best and worst performance). Execution within a line of business was the leading driver of the spread in performance (45 percent), followed by investment performance (30 percent). Group lines and accident- and health-insurance products have significantly outperformed individual lines on value creation. Individual annuities have soaked up more than a third of industry capital and have created less than 20 percent of value. Career distribution has created significantly more value than third-party channels. Scale has not mattered.
        
        Our research suggests that during the next decade, outperformers will focus on four areas: building core risk and capital-management capabilities, including recognizing differences in cost of capital by line; using analytics to build competitive advantages in distribution; unlocking value in the in-force book; and leveraging customer insights to find growth in high-opportunity segments, such as managing retirement risk for baby boomers, serving the risk needs of the middle market, and capturing high-growth opportunities in emerging markets.
        
        The life-insurance industry, still recovering from the meltdown of 2008, will have to cope with a challenging macroeconomic and regulatory environment marked by high volatility, low interest rates, and slow economic growth. Yet there is a silver lining: new opportunities. The shift of financial responsibility from governments and employers to individuals, strong growth in developing markets, and an aging population not prepared for retirement will all offer profitable growth potential for life companies that can navigate the risks.
        
        As economic pressures force employers and state and federal governments to cut back on benefits and eligibility, millions of Americans are unprepared for medical crises or retirement. Agent-based distribution has moved toward the affluent and mass affluent, leaving middle-market consumers underinsured (Exhibit 2). Simply returning penetration of this market—which comprises 63 million households with an annual household income of between $25,000 and $100,000—to 2004 levels could raise annual revenue by $20 billion. Based on typical margins of 5 percent, this could lead to an increase in the industry profit pool of a billion dollars annually. Life insurers can play a major role in helping people meet their retirement needs, and the flow of funds will be unprecedented as the population ages. But capitalizing on this opportunity will challenge life insurers in several ways.
        
        Exhibit 2 Less than half of US middle-market households have life insurance. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        First, many consumers in the middle market simply do not have the resources to fund their retirement needs; life insurers will need to focus on those in a position to invest in retirement products. Meanwhile, many consumers are looking for comprehensive retirement plans that reflect their family, employment, and real-estate status and aspirations—issues that require much more than insurance products. The third challenge is that products that deal with critical-illness exposures, end-of-life care, and longevity risks are difficult to understand, suggesting that the sales process needs to begin with consumer education.
        
        In their search for growth, many carriers are also looking overseas to developing markets, where economies are expanding and life-insurance penetration is low. Research suggests that developing countries will account for more than 80 percent of global growth in the life market in the next decade. Yet there are hurdles to penetrating this market, including regulation, a scarcity of management talent, and different consumer mind-sets and behaviors. Still, some companies are building strong positions that drive growth, profit, and valuations through acquisitions, joint ventures, and long-term development from the ground up.
        
        In short, while the new growth opportunities are substantial, companies must tailor products and distribution to meet the unique needs of consumers and deal with challenging macroeconomic and regulatory environments. We recommend that life-insurance companies focus on four areas:
        
        Improving risk and capital-management skills. Companies with the discipline to focus on value rather than volume growth will make better decisions. They will need to set clear parameters for risk appetite and establish robust metrics to manage capital and govern risk. Winners will also build more flexibility into product design and pricing to make fewer long-term guarantees and share risks with their customers.
        
        Companies with the discipline to focus on value rather than volume growth will make better decisions. They will need to set clear parameters for risk appetite and establish robust metrics to manage capital and govern risk. Winners will also build more flexibility into product design and pricing to make fewer long-term guarantees and share risks with their customers. Using data analytics to expand distribution capabilities and lower distribution costs. Insurers looking for stronger margins will need to control costs, of course, which will lead to discussions about distribution, the single biggest source of cost today—and the biggest constraint on growth. There are many ways to improve distribution performance, from using deep analytics to identify leads and marketing opportunities to tailoring service offerings to agents based on their needs, as well as increasing the adoption of financial planning and building product-expert, wholesaler, and sales teams to drive agent performance.
        
        Insurers looking for stronger margins will need to control costs, of course, which will lead to discussions about distribution, the single biggest source of cost today—and the biggest constraint on growth. There are many ways to improve distribution performance, from using deep analytics to identify leads and marketing opportunities to tailoring service offerings to agents based on their needs, as well as increasing the adoption of financial planning and building product-expert, wholesaler, and sales teams to drive agent performance. Leveraging the in-force book and existing customer relationships. The in-force book—policies life insurers have already sold to their customers and are now collecting revenue from and paying claims on—accounts for the lion’s share of profits, revenue, and operating costs, yet senior management focuses most of its attention on new business. To unlock the hidden value of the in-force book, carriers must capitalize on pricing, fee, and asset-allocation flexibility; pursue cross-selling and customer-behavior-management techniques; and improve operational efficiency.
        
        The in-force book—policies life insurers have already sold to their customers and are now collecting revenue from and paying claims on—accounts for the lion’s share of profits, revenue, and operating costs, yet senior management focuses most of its attention on new business. To unlock the hidden value of the in-force book, carriers must capitalize on pricing, fee, and asset-allocation flexibility; pursue cross-selling and customer-behavior-management techniques; and improve operational efficiency. Pursuing the middle market and developing markets. To achieve profitable growth, carriers need to continue serving their core affluent markets while looking to less conventional sources of growth. While the life-insurance industry has been a major player in the retirement space, both as a manufacturer of individual retirement products and as an administrator and asset manager of defined-contribution and defined-benefit plans, asset managers and securities firms have been the big winners. To strengthen its position in this enormous and growing market, the life-insurance industry needs to help retirees understand the products that will cover the most important risks—ones that life insurers are uniquely positioned to address, such as death during peak earning years, end-of-life illness, and outliving assets. This will entail revamping advisory capabilities to a fee-based model that offers an all-encompassing approach to financial planning.
        
        Differences in the performance of life insurers are driven primarily by risk skills and the ability to manage liabilities, none among them more important than poolable risk. Charting a course and setting the sails will be comparatively easy. As with all strategies, the challenge is in the execution: winners will be those companies able to best weather the inevitable storms while continuing toward their ultimate goals.Life insurance is a long-tail business: decades can elapse between the time when a policy is sold and the claim is made. Managing a portfolio of these policies, each with its own approximately 40-year time horizon, can present an operational and IT headache. As more and more policies expire, the overhead and servicing costs for the systems that manage legacy products are spread across a dwindling number of active accounts, driving per-policy administration costs higher. Since insurers are required to book capital reserves against future expenses, those costs and their anticipated increase over time can weigh heavily on the balance sheet.
        
        The outsourcing of “legacy books” (or “closed books,” as they are also called) can provide a big lift to the industry, freeing insurers from managing the processes and IT that support these mature product lines. In addition, engaging providers that specialize in these areas typically yields substantial cost savings and can help insurers lower their capital requirements.
        
        Despite the promise of outsourcing legacy products, the practice has yet to spread beyond the United Kingdom. There, regulatory requirements on the life insurance industry, enacted at the beginning of the decade, put severe pressure on margins and costs. While some insurers sold their legacy books outright, others turned to outsourcing. What began as an effort to relieve a growing administrative burden gained traction as insurers recognized that providers could drive down servicing costs through more efficient processes and better IT integration.
        
        Outside the United Kingdom, however, insurers have remained skittish about handing over a large part of their operations to inexperienced vendors. Likewise, vendors have stayed on the sidelines, wary of investing in an unproven market. That chicken-and-egg dilemma may be about to crack as players in North America and continental Europe, prompted by rising administrative costs and persistently soft insurance markets, consider outsourcing their legacy book management.
        
        Sidebar Checklist: Getting started Outsourcing legacy books can help relieve insurers of a major operating burden. To get started, CIOs and COOs should consider these four steps: Analyze legacy portfolios and related systems and determine the financial case for outsourcing by comparing the internal cost to the average annual cost per policy that a provider charges. Assess whether current IT and business-process-outsourcing partners have the necessary skills. Compile an inventory of all affected systems, processes, and databases to determine what platform, software, and application environment the vendor will need to support. Map the transition and conduct pilots to evaluate system performance, identify compatibility issues, and manage risk.
        
        Servicing legacy books requires significant processing costs and heavy IT support on everything from policy and eligibility reviews to client statements and payouts. The core IT running most insurance processes has grown organically and through acquisitions, leaving insurers to grapple with a tangle of fragmented and outdated applications. The resulting complexity can account for up to 75 percent of the operating and underlying IT costs associated with servicing policies. Best-practice improvements such as process automation, platform consolidation, and data standardization can be difficult and costly to implement across large-scale enterprises. Moreover, such initiatives lie outside an insurer’s core business (see sidebar, “Checklist: Getting started”).
        
        In the cases we’ve seen, companies have achieved significant benefits by offloading this burden to a third party. With legacy book outsourcing, a provider manages the portfolio and all its support systems, including the IT assets, infrastructure, call centers, and support staff. While the insurer owns the customer relationship, the outsourcer assumes the day-to-day administration, such as taking customer calls and issuing statements and payouts. This arrangement enables providers to drive economies of scale in two ways. While an insurer may manage only one or two legacy book portfolios, providers can service several clients at once, spreading fixed costs over a larger volume of policies. Their experience in IT integration, process design, and work flow management means they can better target bottlenecks and inefficiencies. In addition, the greater visibility from streamlined systems helps managers track costs and performance measures more effectively. Together, these improvements typically reduce the total IT and operations cost per policy charged by the provider as much as 50 percent.
        
        Under this arrangement, an insurer pays the outsourcer a fixed price per policy, turning an uncertain operational expense into a guaranteed cost. When talking to regulators, insurers can point to this fixed, lower-cost outsourcing fee to calculate their overall cost position more accurately, potentially reducing the capital reserves associated with legacy books.
        
        In one case, executives at a life insurer asked its chosen outsourcing provider to manage its legacy portfolio. Their goal was to reduce business complexity, improve customer satisfaction, and trim costs. A joint vendor–insurer project team (composed of IT, operations, and business staff) implemented the effort in phases, first defining its scope and then identifying the systems, databases, and operational activities to transfer. The vendor’s mandate also was to improve service levels. It created a performance-management system to allocate resources to areas that had the greatest customer impact. As a result, the vendor reduced the number of customer complaints by 30 percent in four weeks, surpassing the initial target of 20 percent in six weeks. Next, it applied lean-management techniques to smooth process flows and improve transparency—for instance, by using simple imaging solutions to scan documents, thereby cutting down on paperwork and making information more accessible. With those milestones met, the vendor began simplifying the insurer’s IT architecture, trimming the number of platforms to seven (from ten) by merging systems and applications. In all, these actions cut servicing costs per policy by 40 percent.
        
        Unlike other niche outsourcing markets, where small operators can step in as specialists, legacy book outsourcing is a volume business. Therefore, providers must secure a steady stream of new deals to maintain the scale needed to generate savings. They must also be willing to build teams with the ability to run and consolidate legacy insurance IT systems. This dynamic means that players must reach scale fast and establish themselves as leaders to thrive. Since the market leaves no room for second-tier players, even the more successful UK vendors have been reluctant to invest in other regions without some assurance of volume.
        
        A few trends could open up the market significantly. Some vendors, hungry for growth, may pursue legacy outsourcing and its potential global market size of $3 billion to $7 billion as a strategic business opportunity. Alternatively, the magnitude of cost savings that some UK insurers have achieved may spur them to seek the same service for their non-UK operations. Outside investors, such as private-equity firms, might also step in as aggregators to buy legacy portfolios and outsource their management.
        
        Legacy book outsourcing can help insurers respond to the challenges of today’s economic environment. By getting out of the time-consuming and expensive responsibility of managing these slow-growth businesses, insurers can concentrate on pursuing new opportunities with higher returns. This approach can also be applied to other financial-services products, such as mortgages, that require long-term servicing after the primary revenue-generating period has ended.Is a hidden gem eluding your portfolio evaluation process? Most companies periodically scan their operations to ensure that they are the best owners and in the process identify businesses that can be divested to raise capital for other opportunities. But these companies typically overlook support services, viewing them instead as cost centers—which focus on cost reductions or outsourcing—rather than as business units ripe for divesting.
        
        The distinction is an important one. In many cases, it makes sense to outsource individual service activities, including commoditized corporate functions (such as finance and accounting, HR, and purchasing), IT functions (the help desk, infrastructure operations, applications management), and industry-specific functions (booking and fare management for airlines, payments processing for banks). Yet when a company aggregates support services into a single unit, it may constitute an attractive business that can be sold outright, with a value greater than that of a five- to eight-year contract for continued support services. The selling company reduces its operating costs, raises capital, and removes assets from its balance sheet. The purchasing company acquires assets, know-how, and perhaps an attractive geographic footprint, as well as a new support services client. Nonetheless, even executives who understand the idea in theory worry that the practical obstacles to divesting—tight credit and a weak market for assets—outweigh the benefits or that the seller will have to pay more for these services after the divestiture.
        
        For companies that meet certain prerequisites, however, the opportunity can be significant, and there are plenty of eager buyers for shared-services units that offer real value. In our research into more than 30 recent transactions, the divesting companies generated, on average, an immediate cash injection of 250 percent of book value. There were also immediate cost savings of up to 40 percent, followed by annual additional reductions of over 2 percent, and even, in most cases, improvements in quality. These numbers match up well with the latest transaction multiples for similar types of assets—and divestments of shared-services units also embody hidden opportunities for value, so the benefits probably exceed those of open-market transactions.
        
        Not all companies should consider divesting a captive support services center. Before a sale attracted buyers, many companies would first need to develop their own capabilities internally or to improve the organization of their shared-services units. That is especially true for companies whose support services are still dispersed among various business units or only loosely controlled by a central unit, as well as those whose business processes aren’t standardized or whose fragmented IT systems are based largely on legacy applications and must therefore be cleaned up. Furthermore, companies facing a large, imminent restructuring (such as the divestiture or acquisition of a major business) may also find it better to keep their services internal so that they retain control over quality, avoid adding complexity to the difficulties of the transition phase, and reduce the risk of losing key people.
        
        Obviously, if a unit has unique capabilities or a company has unusual service requirements, selling the unit outright would put the company at a disadvantage when it negotiated subsequent service agreements with the new owner, which would have the leverage to demand whatever terms it wanted. In our experience, however, sellers usually have enough qualified alternate providers to make the subsequent negotiations competitive.
        
        We have also found that the companies best positioned to divest have service centers mature enough to permit a change of control: such a unit is a separate entity, with an existing sales and service culture; has a product catalog with clear service-level agreements (SLAs) regulating the type of service the buyer receives, as well as its quantity and quality; and provides at least 30 percent of the selling company’s needs. Finally, the unit’s growth shouldn’t be strategically important to the success of that company, which must also be willing and able to manage the resulting service contracts.
        
        Even among companies that meet these prerequisites, divesting a support services unit is attractive only if the benefits exceed the value that could be created through a simple outsourcing contract. For sellers, this means finding a buyer that can provide quality services at a cost lower than the current one, offer a service contract sufficiently flexible to adjust for changes in technology and usage patterns, and pay a premium high enough to justify the permanent transfer of control and ownership of all assets. Buyers actually have shown a willingness to pay such a premium for support services units that offer value creation opportunities similar to those of any other acquisition (exhibit). Attractive units must have the ability to function as businesses on their own, a desirable geographic footprint (from an operational or a customer-facing perspective), industry-specific capabilities that would strengthen a service provider’s offering, significant growth potential, or unique intellectual property.
        
        Exhibit What buyers want Buyers have been willing to pay a premium for support-services units that offer value creation opportunities similar to those of any other acquisition. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Examples of successful sales abound. Take, for example, WNS, the support services unit of British Airways. WNS was a wholly owned subsidiary of BA until April 2002, when the airline sold 70 percent of its shares to the private-equity firm Warburg Pincus. Under Warburg Pincus, WNS was able to expand its offering of finance and accounting, HR, and benefits-management services to numbers of new clients. Another example of such a sale involves the private-equity firms Cinven and BC Partners, which acquired Amadeus Global Travel Distribution, the ticketing arm of Scandinavian Airlines System (SAS), Lufthansa, Iberia, and Air France, in 2005. Since then, Cinven has successfully worked with the company’s management to enlarge the business and reduce operational costs. In 2007, Cinven recapitalized Amadeus, earning 1.6 times its original investment. And when the European service provider Capgemini acquired Unilever’s Indian support center, Indigo, it quickly became a platform for establishing the company’s offshore business process outsourcing services.
        
        Not all companies meet the prerequisites, and those that don’t should wait. One Fortune 200 basic-materials company first evaluated the idea of selling its support services center four years ago but decided that the unit didn’t serve enough of the company—or offer enough services to the internal clients it did serve—to attract the right potential buyers. Further, the company was divesting a major division and decided it couldn’t risk losing direct control over its support services until the restructuring was complete. Recently, after addressing those issues, the company divested its support services center.
        
        In our experience, many companies will be interested in acquiring a mature support services center. The trick is to negotiate only with potential buyers for which it would have real value. The seller must understand that value for a wide range of companies—including service providers and financial buyers, such as private-equity firms—and develop a short list of no more than five candidates that would be invited to negotiate. A team that includes the CIO or the head of support services, the CFO—and, for larger deals, the CEO—usually conducts this kind of effort.
        
        At the outset, the team should determine whether its own company is the unit’s best owner by developing a realistic three- to five-year business plan based on the assumption that the unit would be free to serve any customer and that resources would be available to support its growth. The plan should account for the unit’s growth opportunities and for cost and quality improvements that would take its performance to best-practice levels. If the plan would help the center generate more value than it is currently expected to create, the team must decide whether the company has sufficient resources to make the necessary investments and add the needed capabilities, taking into account its hopes for other business units. If the plan would require a disproportionate focus on the support services center or would be challenging to execute—given, for example, the natural constraints to serving competitors—then there is probably a better owner, and the company should consider divesting the unit.
        
        When the time comes for the company to divest, its executives must manage two competing challenges: getting the best possible cash payment for the sale of the business and the best possible terms for a five- to eight-year service contract. The key to success is negotiating the service contract and the sales price at the same time—typically, by inviting a short list of credible buyers (those with the size, reputation, and ability to provide contractual quality and service guarantees) to an auction that sets the price both of the unit and of the service contract’s most important products and SLAs. The top one to three bidders should subsequently be invited to participate in detailed open-book negotiations.
        
        An excessive number of candidates can be a problem. A large multinational that began the process with more than 25 bidders found it impossible to evaluate them, because it couldn’t properly negotiate both the sale and the service contract for so many bidders at the same time. After a six-month auction failed to produce appropriate results, the multinational decided to enter into detailed negotiations with only two parties. It reached an attractive agreement within two months.
        
        Negotiation mechanics for the sale—due diligence, valuations, and so forth—are the same as those for any other divestiture, with a notable exception: the seller must be confident that a buyer will stand by its long-term contractual obligations and its guarantees if issues arise with the quality of service. This type of transaction also differs from a standard one in that the value transferred depends on more than the amount of the up-front payment for the sale; other important considerations include the size of the initial and ongoing cost reductions, the length of the service contract, and the investment needed to transfer hardware, software, and employees.
        
        The challenges of negotiating the service contract resemble those of any straightforward outsourcing contract. Sellers often include specific requirements, such as limiting the use of offshore employees and mandating a presence at certain locations. (One US financial institution, for example, required the buyer to keep nearly 100 employees in the seller’s US offices and to allocate a certain number of employees in the buyer’s offshore offices to work solely on the seller’s account.) Once the transaction closes, the seller must keep key people from its former captive to ensure that it has the contract-management skills it will need and understands the systems and processes it has sold.Despite the global downturn, the IT offshoring and outsourcing industry has continued to grow, though at a slower pace. The recession’s main effect has been heightened competition among the hundreds of IT service providers that handle a variety of tasks for global corporations. Now, a small group of winners is emerging from the fray, threatening to erode the offshore franchise of many Tier-1 and Tier-2 suppliers in countries such as India, the Philippines, and Russia.
        
        Our 2008–09 survey of the global IT offshoring and outsourcing industry—covering 200 relationships among companies in Asia, Europe, and North America, including 65 of the Fortune 200—shows that these rising suppliers have had a broad impact. In fact, they are redefining many traditional management practices; changing the long-standing model for contracting offshore services, by focusing on the quality of services delivered rather than the usual benchmarks of costs per offshore hire; collaborating with clients in new ways; and gaining more control over outsourcing strategies.
        
        What’s more, our results show that this new group of IT service providers is developing the broader and deeper pools of talent that global clients increasingly demand and using progressive techniques to manage and retain these workers. Perhaps that’s why such companies had the highest rankings for overall client satisfaction and employee retention in our survey, logging high scores across their entire client base and showing a consistent year-on-year improvement. By contrast, clients thought that most of the other established Tier-1 and Tier-2 companies were just doing an “average job,” and their performance isn’t improving. In another major shift, they can no longer win bids solely by differentiating on price, since almost all suppliers are now cost competitive.
        
        Meanwhile, the leading providers’ characteristic practices are becoming more important. The four most important practices our survey identified are a new delivery model for services, a greater ability to supply business expertise rather than just IT know-how, more successful talent management, and clearer metrics for judging results. We expect that these practices will become more important and widespread as clients push the offshore industry to achieve higher performance levels and provide more sophisticated offerings.
        
        The most widely adopted model for delivering offshore services is called staff augmentation, but it is ceding ground to the more robust managed-services model. Under the traditional system, clients pay for each staff member a supplier adds to complete an IT contract—from the help desk operators who handle service problems to Java or mainframe software developers. Clients seek the lowest cost per head, which encourages stiff price competition among suppliers, but gives the vendor limited incentive or accountability for the outcomes and quality, as no specific requirements or deliverables are specified in the contract.
        
        Under the managed-services model, suppliers agree to deliver a specified capability or functionality with a desired level of service for a given price: for example, they contract to provide data center support for a year within certain volume and downtime parameters or to support production operations with clear, mutually agreed upon service levels. This model requires a higher level of trust, as clients cede more control to suppliers. Clients benefit by locking in the services they need without having to manage variable resource requirements at the offshore venue tightly.
        
        One pharma company moving to the new model invested considerable time upfront with its offshore supplier to document the underlying business processes and build internal capabilities (such as management tools, standardized work statements, and templates for service-level agreements) where it wanted a high level of support. Then the company created and managed a knowledge transfer process to ensure a successful and timely transition to the new delivery model. Although several months passed before the benefits started to accrue, the quality of the supplier’s work improved and the company shifted additional operations to it. Overall productivity rose.
        
        Our survey shows that client organizations relying primarily on the managed-service rather than staff augmentation model reap great advantages: the best and most efficient work, the highest satisfaction levels, and the lowest attrition rates among their suppliers’ employees (Exhibit 1). Managed services may also make it possible for clients and suppliers to improve offshore results more than the traditional approach does. According to an executive at the pharma company, “While you do have to invest time up front, managing it on an ongoing basis is much less of a hassle. And you actually get more control when you focus on deliverables and things that matter rather than micromanaging the team remotely, which doesn’t work and results in a lot of frustration on both sides.”
        
        Exhibit 1 The more satisfying model The managed-services model makes customers more satisfied. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The work suppliers undertake is shifting significantly as well. Historically, clients have sought an offshore supplier with experience in their technology platforms—for example, skills in a particular programming language or in managing server installations. These contractors supported commodity onshore business processes for their clients at a significantly lower cost than the clients’ could achieve themselves. The new paradigm moves the suppliers’ work up the value chain. More and more, contracting revolves around expertise in specific business processes, or domains, such as loan origination skills, credit card processing, or account opening.
        
        When client and supplier work regularly within a domain, the supplier can deepen its expertise and leverage it in subsequent assignments. The benefits can be substantial: some survey respondents report efficiencies of 20 to 30 percent for at-scale domains. So it usually makes sense for a client to limit its offshoring in any domain to a few suppliers, which, over time, gain a better understanding of its objectives and requirements. As these relationships become more mature, as much as 60 to 70 percent of IT support for that business domain can be offshored effectively.
        
        According to a banking company’s CIO, this kind of focus on business domain and mutual investment in domain expertise forces clients to develop a clearer view of the way their demand for offshoring services will evolve over the medium term. They should then communicate that understanding to help their suppliers develop business depth in those domains. From the perspective of a supplier, investments to build its expertise can help it to win repeat business and, ultimately, to become the client’s strategic partner.
        
        Of course, the economic slowdown has led to a degree of slackness in some of the normally tight markets for offshore labor, though the suppliers in our survey report that they still face strong competition in hiring and retaining highly skilled talent. But the survey found that in some client–supplier relationships, attrition rates are low and satisfaction is high. In these relationships, talent is managed in a significantly different way.
        
        The key to minimizing attrition is for clients to give suppliers wide-ranging authority to manage their teams locally. In part, that means working cooperatively with suppliers, developing their local team leaders, and letting them manage projects themselves. These best practices can make attrition rates fall dramatically (Exhibit 2). In one instance, a client closely integrated its domestic and offshore teams and sent its home-based employees to the offshore site, where they spent a substantial amount of time during the project’s early phases. As part of the effort, the client brought onshore and offshore managers together to determine how to build the supplier’s skills for a complex business process. Attrition rates dropped to 10 percent, from the 30 percent levels common in the offshoring industry. As one executive put it, “our philosophy shifted away from ‘supplier talent management and supplier attrition are supplier problems’ to develop a partnership.”
        
        Exhibit 2 Talent-management support Best practices can make employee attrition rates fall dramatically. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Clients can also help their suppliers to reduce attrition by mixing more challenging work (such as high-end development projects) with repetitive tasks (say, system maintenance, production support, or simple enhancements of previous work). A financial-services firm, for example, gave a supplier both the routine chore of maintaining and providing production support for finance P&L systems and the more demanding job of creating a next-generation derivatives platform. When more of this kind of challenging work comprises 30 to 40 percent of the workflow, our survey shows, attrition levels can fall to as little as half of those common in relationships where work is uniformly tedious.
        
        In the survey, the highest levels of satisfaction and performance were reported by client companies that focus their offshoring performance metrics on a limited number of goals relevant at the CIO level. That’s not the traditional approach; clients have relied on an assortment of detailed, mostly cost-focused metrics that failed to frame their strategic objectives and achieve sustained performance improvement. Successful client–supplier partnerships are moving away from such legacy reporting systems, which reinforce the micromanagement aspects of the staff augmentation model.
        
        More modern measurements focus on three to five goals, such as maturing the offshore delivery model, minimizing time-consuming handoffs between onshore and offshore units, improving quality, or improving time to market for new products and services. The corresponding high-level metrics might include the percentage of work covered by managed-services versus staff augmentation contracts or the onsite-to-offshore ratio for processes. By concentrating on fewer metrics and identifying issues that affect goals directly, clients communicated more effectively with their suppliers and speeded up whatever course corrections were necessary.
        
        The most effective way of influencing and improving a supplier’s performance is to provide for greater transparency and then focus on outliers, where performance is either lagging or above average. According to one IT manager, “Creating transparency, alone, got half of the job done. Once we started showing suppliers their scores compared to other suppliers in the portfolio, it brought out the best in them.”
        
        The rules of the game in IT offshoring and outsourcing are in motion. Many executives think that in the postrecession environment, a “new normal” marked by constant pressure to lower costs and improve services will take hold. The trends we have identified in our survey suggest that a structural change is occurring in the offshore sector. Companies showing early success in this transformation are moving beyond the traditional focus on lower-cost and routine work. The new offshore model will involve highly skilled workers performing a range of strategic tasks and new organizational forms that place greater value on partnerships and managing talent.Companies have long embraced a range of IT application-development offshoring programs while keeping work on the IT infrastructure—data center and network management, end-user desktop services, security, and other core IT functions—firmly planted onshore. Then, over the past few years, increasing confidence in remote management, as well as the spread of low-cost bandwidth and the wider availability of high-speed networks, spurred the expansion of offshoring in India (and other parts of Asia) and in Europe. Buoyed by these advances, the offshoring of IT infrastructure work has grown at a compound annual rate of 80 percent since 2005. There have been some notable successes. One global financial-services institution achieved labor cost savings of more than 20 percent just halfway through a 36-month program. Organizations in industries such as pharmaceuticals and investment banking have moved 40 percent of their infrastructure labor to low-cost locations, reducing overall infrastructure costs by about 10 percent.
        
        Yet as the shift intensified, problems associated with the transition to offshoring began to appear. Our most recent experiences helped us identify the common problems and ascertain the steps companies can take to deal with them and to raise the overall value of offshoring programs. The more difficult issues include a tendency to ignore the specific needs of offshoring infrastructure work, inadequate rigor in handling process flows and service hand-offs with partners, and a lack of clarity about the end-state operating model—what the operation will look like in 36 months. When plans stumble along these lines, implementation is delayed, service problems proliferate, and savings are deferred or minimal. One large media company learned all this the hard way when a piecemeal, ad hoc approach to an infrastructure-offshoring program forced its reimplementation from the ground up, with significant cost and time overruns. This company is not alone.
        
        Our experience working with clients across a broad range of offshoring programs suggests that to reduce infrastructure labor costs significantly, companies must take an integrated approach to the global infrastructure delivery model—an approach that fuses long-term operating strategy with the practical mechanics of moving critical infrastructure components offshore. In a recessionary environment in which offshore programs could be politically sensitive, it’s more important than ever to proceed deliberately. Through our work and our interviews with CIOs, vice presidents of infrastructure, senior finance executives, and others, we have formulated six principles for implementing successful infrastructure-offshoring programs.
        
        In a difficult economic climate where cash is king and cost cutting reigns, short-term fixes may supplant longer-term considerations. A patchwork approach to offshoring can result from the pressure to resolve resource constraints and simultaneously address a range of end-user support, network-management, and other infrastructure challenges. In the absence of a clear perspective on the end-state infrastructure model, the employees responsible for executing the offshoring program may lack the compass needed to guide which functions and roles to include in it. That can complicate the deployment process significantly and limit the savings potential.
        
        The first step in an integrated approach is to assess which functions must remain physically close to assets (the physical provisioning of servers, deskside end-user support), to the applications-development and maintenance staff (business analysis, high-end systems administration), or to vendors (vendor management). Also, some functions, such as those with access to client data, have to remain within the home country for regulatory reasons. All others can be performed remotely (exhibit).
        
        Exhibit Defining the opportunity Companies can offshore substantial portions of IT infrastructure. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Selecting an appropriate sourcing model can be daunting. To simplify the decision making, best-practice companies examine both the types of offshoring arrangements available and the staffing and service provisions that make the most sense for them.
        
        For starters, a company must determine whether its business objectives call for a captive model, in which it owns the offshore entity, or a third-party model, in which it partners with one or more entities. Our experience has shown that all but the largest companies should call on the services of an external vendor or a group of vendors. Unless a company is very big and has significant reasons for wanting a presence under its own name in the chosen destination, recent experience demonstrates that it is very difficult for the economics of captive operations to work, given India’s rising salaries and tight labor market for certain high-demand skills. In fact, many companies that had successful captive operations for noninfrastructure functions have sought to get cash from these investments by selling them to outsourcers building scale.
        
        For each of the sourcing choices, a company has two options. It can either augment its own staff by subcontracting the work to an offshore partner while retaining overall project management—without any formally guaranteed service levels—or it can negotiate a service-level agreement with its vendors. Under the latter arrangement, which has the advantage of shifting some or all of the delivery risk, the vendor must meet agreed-upon performance milestones or face penalties.
        
        Adding offshore capacity through the staff augmentation model is the easiest way to move infrastructure offshore. But our experience indicates that guaranteed service levels make for greater satisfaction and savings in the medium and long term. Additionally, they drive the incentive to continue to improve efficiency, something of a benefit in the short term (in a captive situation) and at the point of contract renewal (in an outsource situation).
        
        Despite the increase in the level of offshoring, many companies still go to the standard locations, particularly India. This narrow footprint can be risky; as the volatile economic climate of the past couple of years has shown, sharp swings in currency and wage rates can wreak havoc on business planning. One organization, intent on using India as its offshore base, had an unexpectedly hard time drafting the business case—in the span of eight months, wage and currency rates each moved up by double digits. The company maintained its plans for India but had to reopen negotiations with its vendor and revise its rate-of-return assumptions.
        
        Meanwhile, promising new locations for offshoring have become available. Unlike India, whose talent pool for mainframes is limited (though growing rapidly), Brazil has strong capabilities in this area, and a number of global vendors run mainframe centers of excellence there. Pan-European companies that require French- or German-speaking support staff should consider their sourcing options in Africa and Eastern Europe, which may have deeper pools of talent to meet these specialized needs.
        
        When a company chooses its vendor, it’s doubly important to match needs with capabilities; the critical nature of many IT infrastructure functions raises both the performance bar and the risk. As some companies have learned to their cost, a vendor that is suitable for applications development may not be for infrastructure support. Special care must be taken to assess not only a vendor’s technical competencies but also its overall business model, since it would be running highly visible assets that might well have an impact on end customers.
        
        Vendors must show that they have the recruiting, training, and retention capabilities to ensure appropriate staffing levels. To minimize attrition, leading companies actively support their vendors’ talent-management efforts, aiding in retention and, where possible, ensuring a portfolio of challenging work for top performers.
        
        Not all functions or roles are suitable for offshoring. Some activities, such as trade floor support, require physical proximity to end users. In other cases, specialized forms of knowledge, such as architectural skills for platforms outside the mainstream, may not be sufficiently available in the desired offshoring location.
        
        Despite such problems, our research shows that some large organizations now locate 50 percent or more of their IT infrastructure workforce offshore. They achieve these rates by separating roles that are sufficiently stable and mature to be offshored without modification from those whose regulatory, security, or technical constraints make them hard or impossible to offshore. Then they isolate and unbundle the nonoffshorable functions.
        
        For instance, the highest level of technical support, a Level-3 team, might be viewed as impossible to offshore because engineers occasionally need access to confidential information and may therefore be subject to regulatory constraints. To get around these limitations, a financial-services company split its L3 team into two groups: one focused on customer support, which stayed onshore; the other focused entirely on engineering, which could then be offshored.
        
        For truly global enterprises with operations running 24 hours a day, seven days a week, the “follow the sun” model (with regional handoffs at shift changes) offers the attractions of real-time, anytime availability, as well as lower costs. Few companies, though, have the scale or budget for such a solution. Most find that the center-of-excellence model (with staffing pools concentrated in centralized hubs) offers the advantages of global reach and concentrated expertise, making it possible to leverage staffing, standardize processes, and develop deeper competencies in key skills. One European bank with a number of infrastructure sites, for example, decided to centralize these operations at one location in India. This consolidated structure, which helped the company serve the same customer base with fewer resources, yielded significant cost savings even before the benefits of labor arbitrage were factored into the business case.
        
        The decision about how fast to offshore infrastructure work depends on a company’s need for cost savings and its appetite for risk. Our experience suggests that it generally makes sense to move quickly and thus avoid the loss of momentum that can result from the quest for process perfection. Two issues should be considered nonnegotiable. First, there must be a clear delineation of each party’s responsibilities. If a company uses a number of vendors (say, for a mix of application-development and infrastructure activities), it must have a single point of contact. This would, in the event of an outage, ensure responsibility for coordination and escalation of issues that may fall in a gray area of infrastructure or application management. Second, in either an outside-vendor or a captive environment, there must be accord on the level of performance expected, the way it is measured, and what happens when it isn’t met.
        
        Once these two issues have been resolved, arguments about whether to ship operations abroad first and then fix them (or vice versa) should be replaced by discussions about what must be changed in order to move. With few exceptions, the business case overwhelmingly favors gaining the benefits of less expensive offshore labor rapidly, before addressing other process changes.
        
        When leading companies address the offshoring of IT infrastructure, they craft a comprehensive enterprise-wide strategy for it by blending top-down decision making with bottom-up guidance and analysis. Clarifying the key cost, performance, and location drivers at the outset helps these companies reduce the risk of offshoring and improve their sourcing choices. The old adage “spend time to save time” still holds true. Companies that plan carefully put themselves in the best position to maximize the return on their offshoring investment.The global economic downturn has slowed the growth of India’s technology and business services industry, but beyond the current crisis the industry faces a changing global environment that will probably cut into the country’s worldwide market share.
        
        McKinsey analysis suggests that there is little immediate risk to India’s dominance of the market for offshore technology and business services. But the country’s share could sink to 40 percent by 2020, from just over 50 percent at the end of 2008, primarily as a result of increased competition from other countries, talent and infrastructure constraints, and an unhelpful regulatory environment. But changes in the global market could also give India opportunities, especially if its companies become more innovative and rely less on low labor costs.
        
        The revenues of India’s business and technology services companies have grown to about $58 billion at the end of 2008 (including about $46 billion in exports), from $4 billion in 1998. In 2005, India’s National Association of Software and Services Companies (Nasscom) suggested that export revenues could reach $60 billion a year by 2010, but the global downturn will probably delay the achievement of this goal by three or four quarters.
        
        Looking past the immediate situation, McKinsey expects the global market for offshore business and technology services to grow to about $500 billion by 2020, from the current $80 billion a year. Even with this more than sixfold growth, the industry will serve less than a third of the potential market, which McKinsey estimates at $1.65 trillion to $1.80 trillion in 2020.
        
        Several factors will contribute to this global growth. Core markets—for instance, large financial-services and telecommunications companies in developed economies—should continue to expand along with the global economy, once growth returns. The pace of growth could slow down, however, if processes are automated and standardized more quickly than seems likely now. Corporate budget cuts during the downturn and protectionist regulation could also dampen demand from core and other markets.
        
        Much of the industry’s expansion will come from nontraditional customers. Increased demand from emerging markets (primarily China and India, but also Brazil and Russia) could add $450 billion to $500 billion to the global market by 2020. Individuals and small and midsize enterprises will find it easier to use offshore services as communications technologies advance, costs go down, and business models evolve, adding $240 billion to $260 billion to the market. Finally, new kinds of customers—particularly in the public sector (including state-owned enterprises), as well as health care providers—will likely turn to outsourced business and technology services, expanding the global market by an additional $210 billion to $260 billion.
        
        But as the pie expands, India will be hard pressed to maintain its 51 percent market share, which we expect will drop to around 40 percent by 2020 unless Indian providers become more innovative and global.
        
        First, the industry faces domestic constraints. Talent will be a severe problem: India produces no more than 3 million university graduates a year—too few to maintain its market share. An inadequate physical infrastructure will also hinder the industry’s expansion. Transportation systems and power and water supplies are already strained in the country’s leading cities, including offshoring hubs such as Hyderabad and Chennai. At the same time, infrastructure deficiencies and talent shortages have prevented the industry from moving aggressively into smaller cities.
        
        Another problem is that everyone wants to join the party. China, Egypt, many Eastern European countries, and dozens of others are fighting aggressively to build their domestic business and technology services industries, offering tax benefits and improved infrastructure as incentives. Egypt, for example, is developing university programs intended to provide the industry with 32,000 employable graduates by 2010.
        
        Finally, government policies—which have over the years supported the industry—have become less favorable in recent times. For example, at present there is a lack of clarity surrounding the continuation of fiscal incentives the government has used to spur industry development. Meanwhile, the industry continues to be regulated by the Shops and Establishments Act and other laws not tailored to the service sector’s requirements. Adding to the burden, these laws are applied inconsistently from state to state.
        
        Indian business and technology services companies needn’t stand by passively and watch their global market share decline. Innovation will be the key to maintaining and even expanding their market share. Business models that continue to focus on low labor costs won’t suffice.
        
        Unfortunately, innovation isn’t the industry’s strong suit. Over the past decade, India’s companies captured more than half of the global business and technology services market, but the country still accounts for less than 1 percent of the patents issued around the world annually. To address the opportunities in new geographic and industry markets and to serve individuals and small and midsize enterprises, business and technology services companies must create innovative products that address the needs of these new customers.
        
        Areas that appear ripe include clinical research, mobile applications and platforms, and energy efficiency. Much as offshore companies remotely manage their customers’ IT infrastructures, for example, they could remotely manage the energy consumed by their customers’ air conditioning and heating systems. If successful, such efforts could contribute $100 billion to $130 billion in export revenues to India’s offshoring industry by 2020, expanding its global market share to almost 60 percent.
        
        India’s technology and business services industry has flourished over the past decade, with export revenues in 2008 reaching $46 billion, which covers roughly 74 percent of the country’s net oil imports that year. Apart from an isolated failure in corporate governance, India’s brand abroad has been burnished by the success of this industry. But it will retain its luster and advantage over up-and-coming locations only by reinventing itself and looking beyond the low-cost labor model that served it well in the past.Over the last decade, Shell has been undergoing an IT transformation that is remarkable for the scope of change it is seeking in one of the world’s largest and most complex organizations—one with 25 business portfolios and operations across more than 100 nations. The transformation is defined by four phases, says Alan Matula, executive vice president and group CIO.
        
        The first was about going back to basics—allowing Shell’s IT leaders to better align IT with the business units by stabilizing operations, establishing project discipline, and tracking costs, people, and assets. Matula says that this solid foundation is essential to any successful IT transformation.
        
        In the second phase, the targets were costs and complexity. Shell rationalized and consolidated infrastructure, substantially reduced the number of business applications, improved procurement procedures, and aggressively offshored. It also strengthened governance by recognizing that a real dialogue with the businesses was needed and recruited high-grade talent to conduct it.
        
        Investments in the future formed the third phase: innovation, functional improvements, and business-driven multiyear investment programs to help the businesses meet their targets. Shell implemented strategic sourcing to consolidate hundreds of suppliers, leaving just 11 key partners with whom it outsourced selected functions. Matula says the change in direction allowed Shell to work with suppliers, “doing things that are not only good for Shell but that also are innovative in the marketplace and help the industry.”
        
        Shell is now geared up for the harvest-and-sustain phase, to ensure that the benefits of its IT investment are realized. “IT is more important and intense than ever before and that requires an ongoing effort to transform IT and improve its agility, to make the business more productive and competitive,” Matula told McKinsey’s Leon de Looff in a recent interview (in The Hague) covering Shell’s IT transformation, challenges, and the many lessons learned so far.
        
        The Quarterly: Shell has been in the midst of an IT transformation for several years now. Can you tell us about how it started and about the approach you took?
        
        Sidebar Four steps to a successful IT transformation Step 1: Put transformation on a solid footing by stabilizing IT operations, enforcing project discipline, and tracking costs, people, and assets. Step 2: Take a phased approach—lock in IT efficiencies and reduce complexity early on; then invest in improvements to help businesses meet their goals. Step 3: Get business unit support for major change by maintaining precise accountability for the delivery of benefits and by monitoring progress frequently. Step 4: Select or recruit top IT talent for the interface in concert with business units to help build the trust needed to sustain transformation.
        
        Alan Matula: It was a phased approach that started in the early 2000s with a drive to get back to basics. I was very fortunate to be a part of that effort from the beginning, continuing over the four phases, since I held positions as a business CIO and now as group CIO. The phases addressed specific goals: back to basics, rationalization and consolidation, investing in the future, and harvest and sustain. (For more, see sidebar “Four steps to a successful IT transformation.”)
        
        Alan Matula: You have to start out with a solid foundation that builds credibility with the business. That includes stabilizing operations, implementing project discipline, and tracking costs, assets, and people. If you don’t have the foundation, then don’t even start.
        
        Sidebar Alan Matula biography Vital statistics Born November 11, 1960 2 daughters Education Graduated with a BS in quantitative business analysis from Indiana University Earned an Executive MBA from Houston Baptist University Career highlights Royal Dutch Shell CIO (2006-present)
        
        Various information technology positions, Shell Oil (1982–92) Fast facts Nonexecutive board member of Airbiquity Enjoys sports, outside recreation, and spending time with friends
        
        Alan Matula: Right from the start, we deliberately put “business at the center” of what we do. But the key is that we have been able to strike a balance between an IT foundation that is standard across business units, while differentiating and catering to specific business needs—areas like high-performance computing for our exploration businesses.
        
        The Quarterly: Did the move back to basics stir up any resistance from the businesses? And if so, how did you deal with that?
        
        Alan Matula: We made the application portfolio transparent, showing the businesses the real cost and the complexity of what they had. They figured out pretty quickly that it didn’t fit the changing business models and competitive conditions in the industry, or the globalization agenda the company had set for them.
        
        Alan Matula: You have to remember, we came from a nation-based operating model that was successful for many years. We’re turning that model on its head. So you basically go nation by nation, region by region, and replace a lot of the legacy assets and start to initiate your standardization agenda to attack costs and complexity.
        
        The Quarterly: CIOs debate whether IT should follow business change or lead it—in your case, for example, by furthering Shell’s globalization through IT standardization. What is your view?
        
        Alan Matula: I see it somewhat differently. IT is like cement to the standardization activities. If you don’t cement changes with IT, then over time they will erode and revert back. IT provides the transparency that you need for driving standardization in a large, diversified corporation like Shell.
        
        Alan Matula: We went from hundreds of national enterprise resource programs to about six to eight core platforms that do the heavy lifting in terms of business transactional capability. For example, we now have one HR system, one health system, and three or four big application landscapes per business sector.
        
        Alan Matula: In addition to the core platforms, we divided the business into portfolios around key sectors such as upstream, downstream, and corporate functions. But to really align IT directly to the business strategy, you need to go one layer deeper. So for downstream operations, that means retailing and manufacturing; for upstream, it’s exploration and production. In total, we have 25 business portfolios.
        
        Alan Matula: That’s where we’ve put our key talent in terms of IT capability. Because of their business domain knowledge, these are the people who really get a seat at the table with business leadership teams. They understand the business strategies and can create the differentiating IT on top of the business transactional layer. A lot of what we consider competitively differentiating is kept in house. Even though you read a lot about outsourcing, the key differentiating systems are still kept and managed specifically within Shell.
        
        The Quarterly: Stepping back a bit, how does a CIO build the business case—one that demonstrates the benefits of a broad transformation that involves major investments and a lot of change?
        
        Alan Matula: We’ve learned for sure that when you make the business case, you have to ensure accountability for delivery of the benefits. You need the names of the people who are going to extract the gains. Then you need to track benefits on an ongoing basis. We’re piloting quarterly monitoring where you look at an applications portfolio and examine where benefits are materializing and where they aren’t.
        
        Beyond that, you have to have the right people at the business interface. They are invaluable. You will not find a CIO who doesn’t struggle with finding talent for the business interface. We’ve invested in our IT people through a learning program delivered by a business partner program.
        
        Alan Matula: We review each of our major portfolios every 18 to 24 months, focusing on the key technologies needed to stay in the game as well as what is needed for competitive differentiation. We have done some very innovative things—in telematics, for example, using wireless technology to monitor auto fuel consumption and efficiency. But the key to innovation is having a real dialogue at the business-portfolio level to understand where technology is important. That means a change of mind-set and skills, moving from a focus on the basic infrastructure of business administration to a position as close to the business leaders’ needs as possible.
        
        The Quarterly: What was the reasoning behind consolidating suppliers and outsourcing? How did your thinking evolve?
        
        Alan Matula: We actually tested outsourcing early in phase two, but we backed away from it because we didn’t want to outsource IT problems we hadn’t fixed. Once we were comfortable with our progress, we started the third phase. We spent almost a year bringing in suppliers—10 or 15 of them—exploring what was working, what wasn’t, and what needed changing. It was then that we latched onto multisourcing as a model. For example, in infrastructure, we now have three suppliers that are capable of providing services across the board. This provides us with flexibility and agility as we evolve the delivery model to respond to the changing IT market and Shell business needs. We benchmark the suppliers, and we have written contracts that are very flexible.
        
        Alan Matula: Things do not stay the same forever. But to give you a sense of our thinking, before we consolidated our infrastructure suppliers, we had about 1,500 different contracts, covering licenses, hardware, and services, with an outsourcing model that was very fragmented. We’re now down to three infrastructure suppliers, and these are an integral part of what we call our ecosystem of key suppliers.
        
        The Quarterly: Was there a driving philosophy behind the new outsourcing model and the new ways of working with suppliers?
        
        Alan Matula: We started with the idea that we wanted 70 percent of our spending to be external. Of that, we wanted 80 percent to be focused on the top 11 suppliers. We put those 11 into three groups: First, there are the foundation suppliers, those in which we make long-term bets—Cisco, Microsoft, Oracle, and SAP. Then there’s the infrastructure group, with three bundles—AT&T, HP, and T-Systems—for networks, end-user computing, and hosting of storage, respectively. And finally we have four application services suppliers—Accenture, IBM, Logica, and Wipro. What we’re doing differently is bringing all 11 of them together to work as a collective.
        
        Alan Matula: About two years ago, we began meeting with the ecosystem suppliers every quarter. We started out slowly, defining golden rules and how we operate, with the ambition of actually doing things together that not only are good for Shell but that also are innovative in the marketplace and help the industry.
        
        Alan Matula: The boundaries between the traditional suppliers are blurring. Some traditional hardware suppliers are going into software and vice versa. Others are getting into the data center business. As we started to give our top 11 suppliers some big challenges, that gave rise to some innovative approaches. You’re starting to see all suppliers wanting to play differently by collaborating to improve the overall delivery to Shell. I think it’s going to be an exciting time as we continue to push current industry norms to drive greater degrees of collaboration, change, and innovation.
        
        The Quarterly: You think of IT in terms of phases. What is the next frontier for IT transformation at Shell?
        
        Alan Matula: The next phase in our transformation will be a shift to harvesting the benefits from the assets we have put in place—a new system for HR should last 15 years. So the goal now is to leverage these assets to improve business performance. That’s a different model of change. It forces us to be more intimate with the business, actually putting business operations and IT operations side by side. In the project mode of recent years, you could keep some distance as you restructured, but in the harvest-and-sustain mode, you have to work to optimize the business processes, with IT delivering business performance improvements. The structural approach will be different, and that’s what we’re now exploring.
        
        The Quarterly: Going forward, since you’re trying to capture benefits outside IT, you need a different steering mechanism and metrics beyond just reducing IT unit costs, right?
        
        “It’s about smarter demand management, to make sure you are using only the IT that you require and are doing only the IT projects that you need to do.”
        
        Alan Matula: The pressure to reduce unit costs will never go away in IT. But when you get to harvest and sustain, it’s about smarter demand management, to make sure you are using only the IT that you require and are doing only the IT projects that you need to do. It’s also about a different set of metrics on the business side, and the ultimate art of this is talking to the businesses in their own terminology, to show exactly where IT contributes to their goals. We’re testing a couple of portfolios to see if we can do this. It’s something that every CIO would love to be able to do.
        
        Alan Matula: At the beginning of each year, we carve out key technology areas or domains that we want to mine—things like sensors, high-performance computing, new ways of working. We work with the external providers as well as internally searching for areas where we can make disruptive, differentiating strikes. In terms of structure, we have put all IT technology research into a projects-and-technology organization with a chief technologist whose job is to look for those technology strikes that add the most business value. We want to send the message that the CTO is a business partner at a very different level from standard IT. The projects-and-technology organization is split from the rest of the IT function, and there’s a cut line between the big technology plays and supporting staff—20 percent of the total staff—versus the more incremental activities that remain in the traditional IT line, or 80 percent of the staff.
        
        The Quarterly: You have recently changed how you govern IT, with the business unit CIOs now reporting to the IT function and no longer to the business heads. Is this the optimal model?
        
        Alan Matula: It’s really about maturity. In the early phases, there was no way you could consolidate IT under one function. We needed to build credibility with the businesses, and they needed to take ownership of their project portfolios. In today’s phases, maturity starts to blossom. The CIOs get more and more credibility. As long as they still have that seat at the business table, are heavily engaged, and serve on business forums and leadership teams, then that is what is really important. If that starts eroding, then we’ll go back to the previous model.
        
        Alan Matula: That’s right. I now have more levers to pull when it comes to costs and agility. The new governance model has allowed us to move faster and make decisions without working across different business lines to get everyone to agree. Furthermore, we can move IT resources around easier and more quickly.
        
        The Quarterly: In the financial crisis, some CIOs have focused primarily on cutting IT spending, while others have focused on IT’s important role in cutting business costs. What has been your experience?
        
        Alan Matula: You have to do both. We have, for example, made use of the network for telephone calls, and we have transformation projects in place that use technology so we can work virtually. But it’s a balance. You need to keep driving down pure IT costs on a unit basis while also providing technology that drives business costs down. Of course, it’s always hard to tell that latter story, since it’s often difficult to determine the full cost of a business process.
        
        The Quarterly: While you have pushed standardization and sourcing to the maximum level, is it still possible to squeeze more costs and inefficiencies out of IT operations?
        
        Alan Matula: There are new models emerging, based on cloud computing and software as a service, that are going to have an impact. Beyond that, we have to be a lot more selective about what projects we do, and most of all we have to work for continuous improvement to get the full potential from our IT assets.
        
        The Quarterly: What are the lessons you have learned from this transformation? What might you have done differently and what would you tell other CIOs?
        
        Alan Matula: IT is more important and intense to the enterprise than ever before, and that essentially requires an ongoing effort to transform IT; there is always another phase. To support that mental model, the first thing is to never lose the perspective that you’re here to make the business more productive and more competitive. Our catchphrase, “business at the center,” keeps us grounded. Our position today is a reflection of the tight integration that we have with the business, combined with the efforts of key support functions like HR, finance, and procurement.
        
        A second thing is that you’re only as good as the talent that you have. For instance, in the robust sourcing of infrastructure and applications we have put in place, the people at the interface are very important. They manage the critical supplier relationships with CEOs and top executives at these firms, and they have the technical know-how to help guide the suppliers.
        
        Finally, if you don’t have the basics right, you won’t have any credibility. It only takes one bad “go live” on a project or a flaw in your basic delivery capabilities to set you back very quickly.The scale of corporate IT infrastructure has increased dramatically over the past decade and a half. At many companies, it has moved from basements with a few dozen servers to sophisticated data centers with thousands or tens of thousands of them. Networked storage hardly existed in the early ’90s but today consumes tens of millions of dollars in large IT organizations.
        
        There are good reasons for this expansion. Infrastructure runs the applications that process transactions, handles the customer data that yield market insights, and supports the analytical tools that help executives and managers make and communicate the decisions shaping complex organizations. In fact, infrastructure has made possible much of the corporate growth and rising productivity of recent years.
        
        Yet the very ubiquity of these computing, storage, and networking technologies makes some executives regard IT infrastructure as a commodity. That’s a mistake. Yes, components such as servers and storage—even some support processes, like the monitoring of applications—have been commoditized. Even so, an effective infrastructure operation creates value by making sound choices about which technologies to use and how to integrate them. A technology product purchased from a vendor may be a commodity, but the ability to bring together hardware, software, and support to provide the right combination of cost, resiliency, and features for a new application isn’t.
        
        Especially now, when every expenditure and budget item receives careful scrutiny, infrastructure leaders must engage with business executives and application developers to expose potential sources of value, agree on priorities, and measure not only the cost but also the impact of infrastructure.
        
        There’s ample evidence that the creative use of infrastructure has helped leading companies to make themselves more efficient, redefine their business models, and improve the customer experience.
        
        Real-time data collection. Insurance companies in Britain and the United States use GPS devices and sensors to record the speed of cars and even the damage to them. In manufacturing, radio frequency identification (RFID) tags now provide insights into the way goods move through supply chains and thus reduce inventory levels. In both cases, infrastructure supports and manages the sensors and other devices needed to capture information reliably and inexpensively.
        
        Large-scale analytics. Pharmaceutical companies and manufacturers deploy low-cost computing grids that, respectively, make it possible to develop and test drugs and to develop products that would have been inconceivable even a decade ago.
        
        Speed to market. Across industries, fast reaction times give companies advantages such as the ability to set up sales offices in rapidly growing territories quickly, to give customers strong off-site support for their initiatives, or to meet demand for services when online interactions surge. The best infrastructure units can support all of these goals—and more.
        
        The customer experience. The best retailers and service providers let their customers interact with them via cell phones, call centers, and kiosks, as well as in person. To get the customer experience right, companies must be able to switch and route consumers across different types of networks flexibly. Only a well-tuned infrastructure can provide that kind of flexibility.
        
        Employees’ productivity. Managerial, sales, technical, professional, and clerical personnel do most of their work on the corporate infrastructure, from desktop productivity tools to smartphones. Infrastructure organizations that aspire to create value must make decisions about issues such as how to balance security with ease of use, where to deploy videoconferencing equipment, and which types of personal devices make the most sense.
        
        Developers’ productivity. Too many applications developers spend up to a third of their time as amateur systems engineers: they devote hours to consulting with server and network teams, grappling with incompatibilities, and struggling to choose technologies that bridge the gaps. That time could be better used modeling applications for business processes or writing code. One investment bank created a virtual-development environment, freeing up tens of thousands of developer hours each year.
        
        What must you do to make business leaders understand the value of infrastructure—without seeming to be protecting your turf? We’ve found that several approaches work well.
        
        Be credible on the basics. Now more than ever, business leaders demand solid execution on costs and service levels before they will seriously consider moving to the next level. They see value creation and innovation as a complement of efficiency, not a substitute for it.
        
        Understand the pain points. Infrastructure touches every part of a business. Use that central position to figure out which groups struggle with analytics, need to open (and close) sites more quickly and cheaply, or have the greatest need to get more value from their development teams.
        
        Be proactive. Go to business leaders with ideas that they can evaluate and refine—before they ask for them.
        
        Retain funds for R&D. Even in tight times, the ability to offer IT infrastructure innovations regularly, before the business demands them, protects you against being seen as a purveyor of commodities.
        
        Invest in talent. Supplement your team’s depth in technical and operational matters by adding financial and business analysis skills. To do so, you’ll have to invest in working to develop employees who can explain the IT infrastructure’s business value and work with business partners to deliver it.
        
        The IT infrastructure organization can be positioned not just as an efficient taker of orders but also as a partner in determining and executing a company’s business strategy.If there’s any issue that routinely frustrates executives in many organizations, it’s how to get a true fix on the value that information technology adds to the businesses it serves. IT is undoubtedly central to creating value and therefore continues to account for a rising share of total investment. But defining, measuring, and maximizing that value remain elusive. To throw light on this crucial issue, McKinsey collaborated with CIGREF to study the best practices of major French and international companies across various sectors.
        
        We interviewed 11 CIOs from French companies over a period from March 2007 to November 2007. The in-depth nature of these interviews provided valuable insights, as it allowed us to draw directly from the experiences of CIOs—many of whose companies have successfully used IT to gain competitive advantage. Analyzing their approaches to information technology helps to show how it can promote economic performance. We complemented these insights with international case examples.
        
        IT generates value at two complementary levels (Exhibit 1). The core asset value includes tangible items such as hardware and software, as well as softer benefits such as the IT organization’s processes and skills. IT’s vitally important value-in-use varies with a company’s core business priorities, such as whether it aims for an organizational transformation or operational excellence. A different set of metrics is needed to measure value-in-use, to account for both its economic and strategic dimension.
        
        Exhibit 1 IT–generated value IT generates value at two complementary levels: the core asset value (eg, hardware and software) and the vitally important value-in-use. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Take the example of a group focused on optimizing investments among its various businesses—say, a banking group with multiple business units such as retail banking, consumer finance, capital markets, asset management, and the like. The economic value expected from the IT department can be measured through the improvement in the overall cost-to-revenue ratio, while the strategic value can translate into a competitive edge in terms of investment or acquisition capacity. (Since 80 to 90 percent of all synergies from banking mergers involve reducing the cost of operations, IT is indeed a key enabling factor during an acquisition.) The indicators that are tracked will be mainly financial, such as the ratio of IT spending to revenue, and will then be compared with the operating ratio—for example, operating costs over revenue (Exhibit 2).
        
        Exhibit 2 Operating costs versus IT spending In the context of optimizing investments, the value of IT derives from dynamic control over the impact of IT investments on each company’s operating ratios. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Similarly, in companies for which the priority is operational excellence (understood as quality and productivity of processes), the business value from IT will be measured in terms of key performance indicators (KPIs) at the process level. For example, IT will be seen as valuable if the systems helped to reduce the delay for processing an insurance claim or to ensure a no-error delivery of supplies to the production line (Exhibit 3).
        
        At one global logistics company in our study, IT greatly improved supply chain operations—a key factor in a radical transformation—by helping the company to optimize its parcel-loading and truck-routing activities and to develop new value-added services, such as same-day delivery and made-to-order solutions for customers. IT also provided important data for more efficient risk management and better pricing.
        
        Exhibit 3 Performance measures For CIOs focused on operational improvements, IT’s value is measured mainly by process performance indicators: productivity, on-time delivery, and quality. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Traditionally, a CIO’s main responsibility has been using standard practices and performance measures to maintain IT’s asset value. Developing value-in-use is a different ball game, however, and CIOs need to examine new levers found at points where the IT department and the business units intersect (Exhibit 4). To succeed, CIOs must take on new roles—bridging functional silos—that may take them beyond their comfort zones. For one thing, they will need to collaborate with executives in the business units to work on major transformation projects, to coordinate strategic planning, or to manage investments collaboratively (see sidebar following this article, “Next steps: Identifying the challenges”).
        
        Exhibit 4 Business performance through IT Developing value-in-use requires a CIO to examine new levers found at points where the IT department and the business units intersect. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Cementing new alliances within the organization is critical (Exhibit 5). A CIO in charge of optimizing IT investments at the group level will need to assume responsibility for managing a portfolio of investments. To do so effectively, the CIO will have to join forces with the CFO, who has expertise in maximizing returns on investment. If the corporate goal is operational excellence, HR is more likely to be the CIO’s preferred ally. This is due to the critical role of change management. Take the example of deploying a new enterprise-resource-planning (ERP) system: the critical challenge is ensuring that the target processes are codified correctly in the system, and that when it is implemented, the end users are sufficiently trained to effectively leverage the potential of the new tool. This requires a joint effort from HR and IT to synchronize and coordinate their tasks from the initial design to the rollout and subsequent life of the system.
        
        Exhibit 5 Building alliances The CIO builds alliances with peers in areas relevant to supporting the company’s priorities. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The businesses that are the best at creating value-in-use, we found, embed their IT governance within the broader governance practices. In practical terms, this requires IT representatives to participate in company forums that traditionally have been the exclusive domain of business unit leaders. At successful companies, certain core business processes, such as managing the business project portfolio or determining the allocation of resources, dovetail with IT processes. This notion of an integrated business–IT governance model can also apply the other way around: we have witnessed examples of companies where strategic planning for IT actually serves as a platform for broader strategic planning by establishing mixed business–IT forums (Exhibit 6).
        
        Exhibit 6 IT–inclusive strategy The best companies at creating value-in-use embed their IT governance within the broader governance practices. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Sidebar Next steps: Identifying the challenges At the best companies in our study, the CIO, the CEO, and the business units essentially cocreate value-in-use when they integrate the elements of our framework (Exhibit A). But to tap the potential reservoir of value, the new partners must have a clear view of the challenges they face (Exhibit B). Exhibit A Dynamics of value creation through IT We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com Exhibit B Creating value through IT: Benefits and challenges We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com CIOs need to understand what the business units expect from the IT organization and to articulate IT goals in terms that business leaders can grasp. That means eschewing technology jargon, making innovative proposals, and taking firm positions on cross-functional projects. To be secure in these new roles, CIOs must develop a range of skills that transcend their IT core competencies; to give the emerging collaborative effort better grounding, they should create forums where IT and the business units can set common priorities. For CEOs and business unit leaders, the main issue is mind-sets: they need to stop thinking of IT as a service provider and consider ways to build alliances with IT executives. IT priorities should be set in clear business terms. Leaders of businesses should proactively draw their IT counterparts into strategic and operational planning sessions. When this approach works, it produces a range of benefits. Fresh synergies between IT and the business units create a wider palette of skills for both as they take ownership of shared projects and increase the intensity of their interactions. With leaders from the two groups finally reading from the same script, communications become more efficient, since less translation time elapses between the formulation of business plans and their execution by IT. Of course, the real bottom line is that these benefits combine to raise IT’s value-in-use across the enterprise.
        
        A simple framework summarizes the best practices we observed in our interviews (see Exhibit A in sidebar, “Next steps: Identifying the challenges”). The value-in-use of information technology emerges when the IT department, building on a foundation of core performance, attacks problems and seeks solutions in areas that interest the business units and IT alike. Alliances with business leaders create new roles for CIOs and increase their scope of action. Governance practices that bring IT and business leaders together institutionalize this new way of operating.Amid the economic downturn, companies are searching for every opportunity to cut costs. IT represents an important part of total spending—5 percent or more in some industries—and its direct contribution to revenues and profits is often difficult to assess. As an unsurprising result, many CEOs and CFOs are eager to squeeze their CIOs’ budgets.
        
        But finding substantial savings isn’t easy. Many CIOs have already spent years reducing costs in operations, procurement, and outside services. Among many other things, they have consolidated data centers and help desks, virtualized servers instead of buying more expensive new ones, rationalized procurement processes, postponed upgrades, and outsourced services to less expensive offshore providers.
        
        Nonetheless, significant additional reductions and efficiencies are possible if companies take a broader look at the way they manage the IT architecture as a whole. The key to these economies is bringing business and IT leaders together in a combined effort to rationalize not only business applications and processes but also the core IT infrastructure and operations. At one large consumer products company, for example, such a joint initiative to combine, consolidate, and rationalize disparate IT systems across business units led to a drastic reduction in the size of the IT staff (by more than 50 percent in the application-management area) and inventories of spare parts, increased leverage in negotiating discounts with suppliers, and the faster completion of new IT initiatives.
        
        The IT architecture of a company is a formal description of its business operations (processes and functions), the business applications and databases that support them, and the equipment and services that run the applications. A complete IT architecture has six layers (Exhibit 1). In the best cases, companies codify it in a compendium—a blue book—that details the workings of the six layers, as well as the processes, roles, and responsibilities for managing the whole. This document should also provide a road map and rules to guide upgrades and additions.
        
        Exhibit 1 Mapping IT architecture An IT architecture has six layers. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Most companies have an IT architecture, but few control it. Instead, it grows organically, and the result is often duplicated systems, proliferating and inconsistent data, and makeshift integration. To make matters still more complicated, at most large companies, even within divisions, many IT initiatives are driven as much by short-term business wants and needs as by any long-term blueprint. That operational reality is especially evident for applications software and the business processes it supports. Such software is usually designed and deployed to suit the needs of one division or business unit, with relatively little regard for the impact on a company’s overall IT architecture.
        
        CIOs at the corporate or division level generally do have substantial control over the core IT infrastructure components: servers, storage systems, and the associated infrastructure software. But the business applications, processes, and business model sitting atop the IT infrastructure often reflect the wants of the leaders of business units and functions, who understandably focus more on their own needs than on overall IT efficiency. Across a global company, the result is often an unwieldy, heterogeneous IT environment where incompatible (and often duplicative) hardware, applications, and processes sprout year by year, in every corner of the organization, in response to specific near-term needs.
        
        At a large financial-services company, for example, we recently found seven different payments systems with 20 custom-built applications, which mostly undertook the same functions for purposes such as payroll, taxes, and suppliers. The company had gradually created the systems and applications to meet its major needs, and the outcome was a complex, inefficient, and expensive operation. The IT architecture team had little influence over many ongoing IT projects, so only a small fraction of them were fully in line with corporate standards and guidelines.
        
        Similar inefficiencies characterize the IT operations of companies in every industry. These patchwork systems require substantially more time and money for development, support, and maintenance—at the expense of budgets, new IT capabilities, and business innovation. At large companies, eliminating these duplications and inefficiencies can reduce IT spending by tens or hundreds of millions of dollars while improving the quality of the IT operation and the satisfaction of those who rely on it. The CIO alone, however, cannot reduce these costs; business leaders too must sponsor and participate in the transformation.
        
        Today’s global economic crisis has created a golden opportunity to make order-of-magnitude reductions in IT costs by modifying the corporate IT architecture. What’s needed is a clearly defined IT blueprint with organization-wide guidelines for the most appropriate and efficient systems, applications, and processes. Developing and enforcing these guidelines helps companies to create a consistent and standardized infrastructure and to minimize unnecessary complexity, duplication, and costs (Exhibit 2).
        
        Exhibit 2 A clean-system landscape A clearly defined IT blueprint with company-wide guidelines promotes consistency and standardization. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        That’s why CEOs must engage a cross-company team of business unit and IT leaders in a no-holds-barred program of architectural review and transformation. We have found that a phased joint approach, focused on a series of cost reduction levers, can reveal and realize substantial savings far beyond the norm for IT-only initiatives. In fact, companies that take this route can also create more flexible and efficient architectures that will help them thrive when the downturn ends.
        
        To create an efficient IT architecture, business leaders and the CIO must jointly evaluate the business requirements and processes that underlie the existing architecture and then explore more efficient alternatives; without a high degree of collaboration, companies probably won’t adhere to even the best architectural designs. In our experience, the right starting point is a high-level business–IT task force that provides cross-organizational governance and accountability. This team’s main responsibilities are to review the existing IT architecture and create a baseline for the new initiative, to define a process ensuring that systems and projects comply with the desired architecture, and to identify short-, mid-, and long-term opportunities for savings and improvements. The team should include all key stakeholder groups, with representatives at a sufficiently high level to make strategic decisions on their behalf.
        
        In the interest of minimizing disruption and maximizing benefits, we suggest a three-phase approach, beginning with the easier changes and working gradually toward more substantial ones (Exhibit 3). Staging them carefully makes companies far more likely to build internal collaboration at an appropriate pace, to generate early gains that can finance subsequent initiatives—and to avoid the significant risks of an all-at-once, “big bang” approach: excessive up-front investment, an internal backlash, or system-wide failure, for example.
        
        Exhibit 3 A three-phased approach A three-phased approach to IT transformation minimizes disruption to the business. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        In the first phase, the team’s task is to identify obvious targets and generate quick wins through cost reductions that help build momentum for larger initiatives. In our experience, three levers are important at this point.
        
        Rationalize software licenses. An inventory of licenses should uncover idle, underused, and even incorrect ones. When business managers participate in the review, CIOs can determine how many licenses are truly necessary, retire those that aren’t, and then negotiate deeper discounts by consolidating licenses.
        
        Cancel noncompliant projects. An assessment of the degree to which ongoing projects support both the business and IT strategies can reveal candidates for continuing support, revision, or termination. In ordinary times, this type of review often highlighted projects that business leaders regard as important enough to justify exceptions to architectural rules. The exceptions bar should now be set much higher. Together, the team can segment projects into four groups: (1) projects that have high business value and directly support the IT strategy ought to continue; (2) those that have high business value but don’t comply with the IT strategy should be reshaped; (3) those that comply with the IT strategy but have little business value ought to be delayed; and (4) noncompliant projects with low business value ought to be cancelled.
        
        Decommission little- or never-used applications. Candidates for retirement include business applications that have rarely or never been used over the past year. Different approaches may be required: depending on usage and need, some may be retired immediately, others replaced by newer applications, and still others phased out gradually.
        
        A telecom operator, for example, recently observed that violations of its IT architectural guidelines were rising sharply because managers of business units—who weren’t directly affected by the failure to comply with the guidelines and had little incentive to do so—demanded that IT support new business initiatives quickly. The violations usually made the IT operation more complex and therefore raised its long-term costs.
        
        The company decided to focus on potential IT architecture violations at the start of any IT project or the release of a new application. The new approach—which involves assessing the overall implications of violations, including their possible additional cost—enables the company to support new business requirements quickly but requires any violations they create to be fixed in a later phase. The company can therefore assess the business and IT cost–benefit trade-offs in each case before moving ahead with projects.
        
        In the second phase, the team should focus on making the whole IT architecture less complex. This more ambitious undertaking is essential to reverse the ad hoc expansion of customized systems, applications, and processes and to begin enforcing a more complete adherence to the desired architecture. The main objective now is to decide if different pieces of the existing IT setup are truly needed rather than trying to optimize them. Often, the team will find that simpler off-the-shelf systems and the reuse of existing components can support business requirements. Companies can use a number of levers during this phase.
        
        Enforce out-of-the-box solutions. Too many projects adopt customization as a first rather than last option. A company can’t make all its business units embrace standard, completely uncustomized applications immediately, but given limited resources the team should require out-of-the-box solutions in the vast majority of cases, allowing customization only when absolutely necessary to meet legal requirements or provide meaningful competitive advantages. Functions such as finance and accounting, human resources, and purchasing, which typically don’t play a role in direct competition with other companies, are prime candidates for savings in this phase. It’s hard to change organizational mind-sets and behavior, but many IT projects fail because of excessive customization, so its end can make companies substantially more efficient and effective.
        
        Encourage reuse. Too many companies spend precious IT resources reinventing the wheel. A serious review of the existing project portfolio will probably uncover a number of opportunities to reuse existing solutions and build a common repository of services and solutions. The move to a service-oriented architecture (SOA)—which describes a system in terms of the business capabilities it requires and a uniform way of accessing and interacting with them—is an important part of this shift.
        
        Consolidate databases and develop an integrated data model. As companies grow, they add new databases and applications for online sales, customer relationship management, and billing to support business units and functions whose needs actually don’t require different databases. In fact, nonintegrated databases greatly raise costs and result in inefficient processes, duplicative development efforts, longer times to market, business errors, and missed opportunities.
        
        Standardize technologies. In many companies, the great diversity of technologies—including programming languages, operating systems, and integration tools—creates tremendous inefficiencies. A careful review will point out redundant versions, unsupported technologies, and nonstandard tools that should give way to fewer, more standard systems. The cost savings come from simpler and consolidated procurement, as well as lower support and maintenance expenditures.
        
        Reduce interface complexity. An IT staff can spend as much as 30 percent of its development time on applications making all of their interfaces work, largely because customized applications have so many point-to-point interfaces. Standardized interfaces, such as the enterprise service bus (ESB), can greatly ease the burden of system integration and minimize the chore of dealing with local changes. The team should start by identifying and focusing on the key interfaces driving most of the effort.
        
        Consolidate systems that do similar things. Different business units in the same company often have their own versions of essential systems, such as payments and Internet applications. Consolidating these systems at the corporate level can bring substantial savings and make processes simpler and more efficient.
        
        In addressing each opportunity, the CIO and business leaders must look hard at trade-offs between short-term convenience for business units and short- and longer-term costs and complexity for the company as a whole. Facing such realities isn’t easy, but the economic crisis should create a sense of urgency. Some of these cost-cutting opportunities will call for investments, and every one of them will demand a solid business case.
        
        At one retail bank, for example, a comprehensive assessment to reduce architectural complexity identified major cost savings in most areas. A team of business and IT executives helped the company find more than 50 unused applications to decommission, 150 redundant applications to consolidate, 800 point-to-point interfaces to put on an integration platform, and 400 applications to connect with a data integration platform.
        
        Most of these changes required near-term investments—in several cases, such as eliminating or consolidating larger redundant applications, fairly substantial ones. Not all consolidation proposals showed a positive return. What mattered most was the team’s decision to insist that the leaders of projects prove the business case for each of them. In all, the effort is on track to provide a return on investment of more than 50 percent, a reduction in time to market of at least 30 percent, and additional organizational benefits, including better alignment between business and IT.
        
        In times of crisis, companies must consider transforming or even completely reinventing themselves. IT can play a central role in implementing big changes in the way they operate and bring products or services to market. The third and most ambitious phase of architectural transformation is about making these bold changes. As companies look beyond the downturn, they should consider changing their IT in more radical ways that can drive or support strategic innovation and fundamentally new areas for growth. Two levers are relevant.
        
        Assess alternate operating models. Most companies have a global business, but far fewer have developed a truly global IT operating model that supports the corporate strategy. A comprehensive review of the IT value chain should identify the level of sourcing, harmonization, consolidation, governance, and IT enablement necessary for each essential business capability. This review can lead to a new, more effective blueprint and architecture model for IT.
        
        For example, managing and minimizing risk, which is especially important in downturns, depends heavily on getting the right data from the entire business to support timely decision making. Business and IT leaders must work together to design the data-management model that promotes the most effective data-driven decision processes.
        
        Shape the future. Business leaders should work closely with IT to explore investments in a wide range of emerging technologies that support new ways of working, such as using the Internet to cocreate products with customers and suppliers, online collaboration among employees, and data-driven management. New tools and processes can accelerate globalization in product development, find profitable niches in declining markets, and increase productivity.
        
        The triggers for this more ambitious approach to architectural transformation can vary. At one manufacturing company, it was an acquisition. The company had just purchased another international business, vaulting the combined entity into the top five in its industry, with more than 20,000 employees and more than $13 billion in annual revenue. Looking to capitalize on this enhanced position, the company’s leaders realized that they couldn’t succeed with two different operating models relying on archaic information systems.
        
        After defining a new, integrated global operating model for sales and distribution, the supply chain, product-life-cycle management, and postsales service, the company implemented it with a streamlined IT architecture. The impact was substantial. Among many other benefits, the company reduced the time required to quote special-bid prices by 90 percent and to create custom products by 80 percent.
        
        The architectural transformation at a national oil company was sparked by its management’s desire to turn it into one of the world’s top oil and gas businesses. The company’s IT systems couldn’t support such growth. Too many systems were customized to local needs. Too few business processes were in compliance with company standards. Data were inconsistent from one business unit to another, so cross-divisional collaboration was arduous. And IT rarely matched best industry practices.
        
        With a clear mandate from top management, IT and business leaders focused on core business objectives requiring a reformed architecture. Under the new plan, IT would support world-class processes, facilitate corporate-wide decision making by supplying better data, help the company develop better insights about customers, and provide an accurate and up-to-date view of its financial, managerial, and logistics position. The company is developing a unified information system to link all divisions and reviewing with greater discipline all requests for new IT projects. IT is exerting more control over critical functions such as finance, human resources, and sales.
        
        The downturn gives IT and business leaders an important opportunity to collaborate in reducing costs and reshaping the IT architecture for competitive advantage. When companies look solely to the IT function to find savings, the results are often limited. A joint IT–business team can achieve much more.
        
        Bringing IT and business leaders together can also help IT create new ways to make the business grow. Greater flexibility, faster times to market, and more efficient and effective business processes will outlast the downturn. For companies that see the present troubles as a chance not only to control their costs but also to reposition themselves for faster growth once the turnaround begins, restructuring the IT architecture can be among the most valuable moves.As economies around the world emerge from the current downturn, many executives understand that what follows probably won’t be just another turn of the business cycle. This new period will see a restructuring of the economic order. Some are calling it the “new normal,” marked by persistent uncertainty, tighter credit, lower consumer spending, and greater government involvement in business.
        
        For executives who run major IT organizations, the implications are clear: they will have to make the IT function dramatically more productive, use IT more effectively to meet larger company goals, and embrace disruptive technologies that will shape the new economic terrain. Drawing upon our experience with clients, recent McKinsey surveys of executives, and a range of interviews with experts, we have analyzed what the new normal means for CIOs in Europe. While some of the forces impinging on them are specific to that region, many of our findings are applicable to IT leaders elsewhere as well.
        
        First and foremost, CIOs will have to overcome hurdles that have limited IT’s performance in recent years:
        
        They must promote a much closer alignment between IT and the business units by embracing new organizational models that call for joint decision making. IT leaders will need better business skills, not just technical know-how.
        
        IT productivity efforts must leap beyond cost cutting at the margins. CIOs will have to make fundamental changes in the way IT operates and campaign for technological improvements that will transform cost structures and operating models throughout the enterprise.
        
        IT leaders must join with their business counterparts to seek out and implement technology-based innovations that will give companies long-term competitive advantages in a tougher economic climate.
        
        In the past, IT performed satisfactorily if it made marginal progress in these areas. In the new normal, it must truly excel in all of them—the performance bar is higher, and the expectation that IT should contribute to corporate success is more insistent.
        
        While recent data suggest that the economic downturn may be bottoming out, rapid, robust recovery may prove elusive. Fewer than half of European executives—similar to their counterparts in other developed markets—expect their companies to perform better in 2009 than 2008: 38 percent expect profits to increase in 2009, compared with 42 percent in North America and 44 percent in Asia-Pacific. In contrast, executives in developing markets (including China and India) are more optimistic, as 53 percent expect profits to increase in 2009.
        
        In this environment, overall cost pressures on companies will remain unrelenting. IT organizations will therefore have to do their part in reducing budgets through productivity savings, as well as self-funding investments in everything from new servers to improved IT architectures. In fact, Europe’s IT organizations appear to face higher cost pressures than their counterparts in other regions do: in another survey, 82 percent of the respondents from European companies expected flat or falling IT budgets for operating expenses in 2009, compared with 68 percent in North America, 80 percent in the Asia–Pacific region, and 62 percent in developing markets.
        
        IT will also be required to help improve both the efficiency and the effectiveness of business operations (such as payroll and transactions) throughout the enterprise—and dramatically. Our survey of IT and business executives found that for European and non-European IT organizations alike, making business processes more efficient is the top priority and making them more effective a close second. Banks, for example, suffer from lower leverage and thus lower revenues in the aftermath of the crisis and must reduce operating costs substantially. Some institutions are therefore using powerful new cross-border IT platforms to gain efficiencies and provide more and better banking products through IT-backed self-service.
        
        Despite the pressures, companies can’t lose sight of the opportunities for the kind of transformation that would help them establish market leadership in the new normal. Our research has shown that 47 percent of market-leading global IT companies before the 2000–03 recession didn’t hold onto their leadership positions after it. In a positive sign, 31 percent of European executives—when asked to list their top priorities—included the development of new products and services in response to changing consumption patterns, and 22 percent included the search for new markets in response to changes resulting from the economic crisis.
        
        While the new normal creates a novel set of challenges for CIOs, the problems that made IT less productive before the downturn haven’t disappeared. In some cases, their impact has deepened as a result of aggressive cost cutting and unresolved structural issues. At many companies, the IT function and the business side fail to coordinate their activities sufficiently, which makes organizations less efficient and effective and impedes the collaborative effort needed to adopt and apply game-changing technologies.
        
        Responding to our survey, 71 percent of European IT and business executives said that IT must be tightly integrated with business strategy, but only 27 percent thought that this actually happened at their companies. In addition, fewer CIOs in Europe than in other regions report to the CEO (Exhibit 1): only 31 percent in Europe, versus 56 percent in North America, for example. This finding suggests that European companies continue to think IT leaders should focus on back-office operations rather than strategy and growth efforts.
        
        Exhibit 1 The weakest link: CIO to CEO Heads of IT in Europe are less likely to report to CEOs than IT chiefs in other regions. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Many of the European IT executives surveyed believe that there is room to improve the effectiveness of traditional IT activities, such as managing the IT infrastructure (38 percent), strategic sourcing (68 percent), and IT performance (60 percent). Business executives believe that IT could support their units more effectively by forging better partnerships to reconceive and upgrade existing processes and systems (81 percent) and by innovating with new technology-supported capabilities (77 percent).
        
        In an increasingly tough operating environment, structural factors make the tasks facing Europe’s CIOs even more difficult than those of their counterparts elsewhere. European markets remain fragmented by language and culture, and their laws and regulations still differ substantially, despite EU standardization efforts. What’s more, many European companies have long used M&A to enter new markets, so their operations are larded with complex legacy systems and governance issues. One major telco, for example, operates in almost 20 European countries, with separate IT platforms and data centers that prevent it from achieving economies of scale.
        
        In pan-European companies, country-level CIOs tend to make IT decisions individually, impeding efforts to improve company-wide systems. Government regulations may impose new demands on IT, such as stringent requirements for safeguarding personally identifiable information. Labor laws, which tend to be less flexible in Europe than in some other areas, make performance-based incentives and IT projects harder to manage. Partly because IT-enabled staff reductions would have been difficult to realize, one European pharma company chose to continue operating some parts of its finance operation manually rather than invest in IT systems.
        
        Seventy-four percent of the European IT and business executives we surveyed believe that their companies are very or extremely susceptible to disruptions stemming from IT (Exhibit 2)—a percentage higher than those in other regions—yet only 48 percent believe that their companies are very or extremely well-prepared for them. As for IT’s ability to transform the competitive landscape, some companies have yet to recognize the role of technology in helping them succeed: a third of the European IT and business executives we surveyed didn’t view IT as being among the top three levers for creating competitive advantage.
        
        Exhibit 2 Unprepared Respondents perceive a gap between the exposure of their companies’ IT-based disruptions and the preparedness of those companies to deal with them. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        To meet the new demands, CIOs should start with efforts to tear down the remaining walls between IT and the business in order to focus on ambitious targets such as upgrading IT operations and enabling IT to improve corporate performance. Demonstrating early successes helps CIOs earn the right to address more far-reaching goals by leading the company-wide adoption of new technologies, such as Web 2.0. A flexible and focused IT organization will be better positioned to enable top-line growth and more open to innovative technologies and the new business models they imply.
        
        The imperfect relationship between business managers and their IT counterparts is a long-standing problem. But the new normal brings more urgency to finding a solution—one that will demand better governance, as well a broader range of management skills among IT executives. Step number one should be establishing a joint-governance model for IT and the business to facilitate better decisions and alignment around priorities.
        
        These governance practices should, for instance, promote joint decision making, which will give IT better insights into the needs of the business and help business managers understand IT’s capabilities and potential. Here’s one illustration of why this is so important: in the new economic landscape, customers will wield more power than ever before, and IT systems can provide the interfaces (such as online self-service) for reaching them. It’s therefore essential for IT managers, at all levels, to understand the needs of the business’s customers—not just those of IT’s internal customers—and to think creatively about how to help the business meet them.
        
        Joint participation in decision making will help IT to anticipate the evolving needs of the businesses it supports and to deploy its resources accordingly. At one utility company, for example, the trading function’s IT team provides 24-hour support. As a result of this close collaboration, the team has significantly shortened the time required to develop features for new trading instruments, and trades therefore adjust more rapidly to shifting market conditions.
        
        When a company chooses its IT leadership, it must recognize that technical skills alone are no longer sufficient. To be valuable partners for business unit leaders, their IT counterparts must not only be well-grounded in strategic planning, finance, and executive-level communication but also have deep industry knowledge and experience. Recruiting remains critical to filling talent gaps, but companies can develop capabilities across functional areas by rotating IT leaders through business roles and business leaders through IT roles.
        
        Since the downturn began, many CIOs have scrambled to control costs by delaying investments where possible and pushing service providers to cut prices. Some CEOs are raising cash through the sale and leaseback of assets such as data centers. But as competition intensifies, a more fundamental restructuring of IT operations will be in order.
        
        Certain companies are rethinking their current approaches to procurement in hopes of replacing the current model of capital spending on infrastructure with a more flexible approach to operating expenditures. Cloud computing and software-as-a-service, for example, allow companies to purchase computing power and application services that scale with demand and thus to avoid large capital outlays on infrastructure capacity to meet peak loads. The cash savings from such efforts can be critical for self-funding additional IT investments: shifts in certain basic IT operations, for instance, could finance a streamlined IT architecture that will improve long-term productivity (see sidebar, “Further reading on IT management”).
        
        IT can achieve even bigger productivity gains—up to ten times bigger—by enabling major improvements in the way business units work, thus radically transforming their cost structures and service to customers. Financial institutions, for instance, can generate savings by extending high-performance IT systems and platforms across regions and borders. As much as 90 percent of the synergies from banking mergers flow from reduced operating costs, which in turn are related directly to the consolidation and standardization of IT processes. After launching a common cross-border IT platform, for example, one European bank cut its operating costs, especially those incurred running the banks it had acquired, far below those of its peers. In one acquisition, it achieved 95 percent of the expected total synergy savings in the first year, providing ample funding for further investments and new acquisitions.
        
        Technologies for collaboration enabled by IT—including the now familiar Web 2.0 tools, such as wikis, blogs, and social networking, as well as others that facilitate live communication and the sharing of documents—can help make knowledge workers more productive. In a recent survey, most respondents reported that they had achieved measurable business benefits from their use of collaborative technologies, but work remains before companies can realize their full benefit.
        
        To meet the demands of the new normal, companies must adopt technology-based innovations in products, services, processes, and business models. They’ll need to develop the ability to identify transformative opportunities, along with a heightened awareness of the competition’s possible disruptive maneuvers. CIOs and business executives can improve their competitive intelligence by participating actively in technology forums, networking with their partners in academia and start-ups, and assuming a perspective that takes them beyond their comfort zone in thinking about business sectors and geographical markets. They must also foster and reward experimentation by role modeling the new mind-set, clearly communicating the new objectives, investing to give executives and staffers alike higher-level skills, and creating new incentives.
        
        Some CIOs in Europe are already navigating these disruptive currents. A major European utility revamped its business model by installing interactive “smart” meters across its entire customer base to provide a flow of detailed data on energy usage and customer behavior. The company used this information to reduce its losses from unbilled delivery, saving an estimated €600 million annually on a €2 billion investment. With a better reading of the needs of customers, the utility could also offer new pricing models (for instance, hourly or weekend rates) to attract and retain them in a deregulated energy market.
        
        A major European fashion retailer uses real-time information to achieve a cycle time of one to two weeks from initial design to final sale of new clothing. Its designers use real-time data from retail sales to gain insights into which fabrics, cuts, and colors are in highest demand and use that information to design new clothing lines or modify existing ones. The retailer also exploits real-time information gained by testing products in representative stores to determine production quantities and reallocate slow-moving stock to locations where demand is stronger. In this way, the company limits its markdowns to half the industry average.
        
        The traditional IT mind-set aims to capture the value of technology through top-down planning, formal structures, and clearly defined processes. In the new normal, the mind-set for success will emphasize a bottom-up search for value through experimentation with customers and partners. Winning CIOs in this new era will view uncertainty and an extremely demanding operating environment as opportunities to challenge prevailing assumptions about the role of IT.The Great Recession has caused plenty of pain in the European insurance industry. Rock-bottom interest rates have reduced returns on the insurers’ holdings, while economic disruption continues to increase the volume of claims payouts.
        
        Not all companies are suffering, of course; indeed, many are thriving. For insights into the factors that distinguish the top performers from the rest of the pack, we surveyed more than 80 life insurers and property-and-casualty (P&C) insurers across Europe. We were particularly interested in identifying the factors that most directly contributed to their superior cost performance, including product development, marketing, sales support, operations, and IT, as well as other support functions. Despite the strong economic headwinds, we found not only that companies in the top quartile of our survey outperformed their peers on costs but also that this operational strength—contrary to conventional wisdom—produced more robust revenue growth.
        
        One of the more startling survey findings was the size of the cost difference between top- and bottom-ranked players. For top-tier insurers, the total operating cost per policy, across virtually every business function, is at least half that of companies at the bottom of the stack (Exhibit 1). Bottom-quartile players’ unit costs can be twice as high as those of top-quartile ones.
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Some of the biggest differences result from the top companies’ much better cost management in sales support, operations, and IT. The consistency of this gap also runs contrary to the industry’s common wisdom: companies may squeeze costs in one area but must compensate elsewhere, with higher costs, to maintain revenues or buttress customer satisfaction. Instead, we found that top-tier insurers sustained their low-cost position without sacrificing top-line revenue growth. Leaders in our survey increased their gross written premiums—the industry’s standard revenue measure—two percentage points faster than the broader market index did.
        
        The survey also calls into question other industry credos (Exhibit 2). The first is that larger insurers have an implicit cost advantage. Our survey data show that while size provides some economies of scale, it often creates administrative and operational complexities that impede even larger efficiency gains. Second, it is often assumed that variations in the wages and regulatory regimes of different countries strongly influence the performance of the insurance companies that do business in them—in other words, that high wages and stiff regulations depress profits. Yet our findings indicate that top performers use operational excellence to dampen the effects of high wages and regulatory constraints. By moving back-office functions to cheaper near- and offshore locales, these companies deftly use labor cost arbitrage to create a lower average cost base. As a result, their cost performance is superior across all geographies.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Finally, we found that the best operators, despite their vigilance on costs, don’t allow that frugality to damage vital functions, such as customer service and due diligence for issuing new policies. When one large insurer found that its cost base was significantly higher than that of industry leaders, for example, its managers attributed the difference to the additional spending needed to ensure longer-term growth and profitability. The company assumed that its lower-cost competitors cut corners in areas such as risk assessment and undertook fewer medical checks to authenticate claims or confirm eligibility. Yet our analysis shows that low-cost leaders did not cut back in these areas and actually achieved higher levels of profitability than the company in question.
        
        Fragmented IT and operations raise complexity and costs. The top-tier companies did more for less thanks to their operational excellence. In fact, among P&C insurers alone, top-quartile players’ profit margins, based on the industry standard combined ratio, were four percentage points higher than the average of the companies we surveyed.
        
        Behind the low scores of our survey’s poor performers, one factor predominates: operational weakness. These companies are more likely to have an ungainly operational footprint, with limited cohesion among operating centers and fragmented IT systems, so they find it harder to streamline processes, share common tasks, and achieve economies of scale. Such deficiencies, which often disrupt a range of processes, lead to high backlogs, as well as frequent work-arounds and manual interventions—factors that slow down response times, raise costs, and hurt customer service.
        
        We have identified three core operating principles that differentiate top-performing companies. These offer a roadmap for insurers seeking to improve their competitive cost position and growth prospects.
        
        Top performers are more likely to centralize their IT and back-office processes and to consolidate corresponding reporting lines under a single point of contact, often the COO, improving both productivity and governance. They are more likely, for instance, to pool medical experts instead of segregating them by location and channel. This approach reduces the number of resources needed—in some cases by as much as 40 percent—while increasing the quality of the guidance.
        
        Taking this approach, one insurer—ranked in the bottom quartile back in 2007—began a three-year transformation to consolidate its back-office support functions and associated IT platforms and to shrink a sprawling network of over two dozen operational centers. These moves shaved 27 percent and 25 percent from the cost of operations and IT, respectively, and bumped the company from the bottom to the upper-middle quartile in cost performance. The savings were reinvested in several new product initiatives, which eventually raised the company’s market share in several key areas.
        
        Top performers make wider use of automated processes, often deploying self-service systems that help customers, insurance agents, brokers, and independent financial advisers process simple transactions (such as low-risk claims) with minimal resort to adjusters.
        
        One P&C insurer cut its processing times by 30 to 40 percent when it automated high-volume, low-risk claims for the replacement of damaged vehicle windshields and the like. Using computer-based, straight-through processing techniques that combine separate steps into an integrated work flow, the company’s process is now completed electronically, without manual intervention. Another insurer, faced with rising claims losses, decided to automate its loss-management process. By standardizing decision making through rules-based programs about eligibility requirements and new compliance guidelines, the company reduced its leakage from payment errors by 4 percent of revenues. These savings, which allowed the insurer to keep down prices for its most popular products, gave it a leg up on the competition and helped raise its revenues.
        
        Many low-cost leaders have applied lean-manufacturing techniques to alleviate backlogs and improve response times. That design focus allowed these companies to streamline their core activities across the business, to eliminate redundancy, and to route tasks and manage work flows more effectively.
        
        One life insurer, for example, seeking to help its brokers, revamped the redemption process to speed up payout times, a key driver of customer satisfaction. A diagnostic revealed several problems, including frequent back-and-forth steps needed to complete documentation and a system that often jumbled together urgent and routine requests, making it hard for adjusters to prioritize tasks. The result was often weeks-long delays in the vetting and approval process.
        
        In response, the insurer created two separate work streams, one for administrative or routine requests that were less time sensitive, the other for payouts. The company also established strict performance targets governing the payout timetable. For each step in the process, it set time limits, such as the number of days a given piece of documentation could rest with an individual agent or process owner. These changes shortened the payment process to 8 days, from 18, while improving overall productivity by 25 percent. In addition, the company now measures customer satisfaction, obtaining feedback directly from the broker network and using that information to further refine the process.
        
        As our survey shows, organizations that take the steps we recommend stand a strong chance of outperforming their peers. While these findings center on the insurance industry, we believe that the same thinking can be applied to other service industries in which IT and operations can play a key role in speeding time to market and improving the way companies meet their customers’ needs.Lean-related improvements in operational efficiency can significantly enhance a company’s performance, boosting productivity, efficiency, and margins by optimizing workflows and processes.
        
        At one large European bank, executives wanted to speed the account-opening process for corporate customers. Poor IT integration and fragmented oversight meant that manual entries, overlapping requirements, and high volumes of paperwork were the order of the day—bogging down opening times, adding costs, and frustrating potential customers. Conflicting rules for accounts and marketing brochures using different names for seemingly similar products confused internal teams and clients alike.
        
        This predicament seemed tailor made for the lean approach. The bank’s leaders wondered if it could help automate the account-opening process, streamline product options, and better integrate the customer-relationship-management (CRM) function—a long overdue improvement. Initial assessments indicated that such a project would pay back its expenses in under three years, with potential labor cost savings of up to 50 percent for the process of opening corporate accounts.
        
        While lean techniques could address these issues, the bank had experienced roadblocks in previous lean programs, which in the end failed to deliver the hoped-for returns. In one case, the process just moved too slowly. The division manager charged with leading the project found that integrating operational changes one department and branch at a time—each with its own people, processes, and technology—would take nearly a decade. In another instance, management tried to avoid a lengthy implementation by taking a “quick and dirty” approach, using lean only in select areas. While the rollout was faster, the project failed to integrate IT systems, processes, and applications at the local level. Operational changes failed to gain a toehold as employees fell back into their old ways of working, and the program fizzled once the start-up phase ended.
        
        In assessing these past efforts, management realized that it had squandered opportunities to use IT effectively as a change agent, not only to convert a patchwork of silo-based activities into an integrated whole, but also to provide for improved governance through a common framework of performance measures. During earlier change programs, IT had remained largely peripheral, merely advising on system, software, and application questions; primary process considerations were left to business and functional managers. Cultural issues—such as the different working methods, priorities, and time horizons of the lean and the IT teams—compounded the operational fragmentation.
        
        Senior managers, determined not to launch yet another stalled implementation effort, knew that IT had to play a central role in the lean redesign of the corporate-account program.
        
        Senior leaders can play an important role in assembling a lean program by involving the chief information officer more closely in designing the transformation. The bank’s CIO started by determining how long it took for a customer to open an account under the current system. Modeling software allowed the IT team to optimize staffing and flow in the call center channel, to chart bottlenecks, and to test alternative ways of streamlining that step. The IT team’s goal, in tandem with the lean team, was to create a single, unified IT system replacing a series of separate applications.
        
        To speed up implementation, the CIO’s office worked with risk, legal, sales, and other relevant units to craft a set of core requirements, such as minimum account thresholds, line-of-credit provisions, and reporting and disclosure rules. It automated the resulting decision-making flow across the affected functions, trimming a number of process steps that produced significant savings (exhibit). This effort reduced the paperwork that clients and employees had to complete and turned a sequence of manual entry forms into an integrated electronic document. With improved oversight in mind, the IT team also developed an online implementation-tracking system that gave management a quick overview of how the implementation effort was going across more than 1,000 branches.
        
        Exhibit Simplifying One bank’s savings from simplifying the account-opening process financed the automation of further steps. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        To reconcile conflicting product descriptions, the team used data-management software to standardize core definitions (such as lending and capital management) and the range of features clients could select. It linked this program to the bank’s existing CRM system so that account managers could better track client activity and customize the service offering appropriately. On the front end, clients gained a redesigned Web site that presented the bank’s product portfolio more clearly.
        
        Wiring IT into the lean-improvement effort made it easier for employees to sustain these successes. With a unified account-opening system in place, reverting to earlier practices was harder—by default, the new system became the standard operating procedure. Automating processes also had the advantage of reducing the number of errors entering the back office.
        
        The bank’s leaders were pleased that the project reduced labor costs by 50 percent for the process of opening corporate accounts. Cost savings financed the rest of the program: for phase two, involving additional improvements to the CRM program, the bank applied €1.5 million of the total €4 million in savings from phase one.
        
        Allowing IT to play a central role in developing and driving the implementation of lean projects can help organizations in many industries better address two problems that have long plagued such initiatives: high complexity and poor sustainability. But that won’t happen unless the lean and the IT teams work closely together to improve both the rate of success and the rate of return.
        
        The benefits compound when companies use the lean approach to create new sources of value. During an IT–lean collaboration at one bank, project managers saw that queuing times were an issue for premium customers. In collaboration with the technology team, these managers saw to it that the chip on the cards of premium customers was customized so that the bank’s staff became aware of them when they entered the system and they received priority in the queuing system.
        
        Although the nature of a business and its underlying processes can raise the degree of difficulty, IT-based lean improvements have worked well in a variety of industries. Better workflow integration at a call center, for example, lowered overall call volumes and uncovered incremental capacity of 20 percent. A software company that wanted to cut the cost of field services used IT to model the impact of dynamic dispatching—an approach that identified ways to slash travel times by 40 percent.
        
        Companies are learning that lean and IT are complementary in the effort to streamline, standardize, and integrate process improvements. Because IT can help not only to coordinate program deliverables but also to spot opportunities to lower costs and boost innovation, CIOs are often well placed to lead the joint effort.Periodically, a dramatic change in an industry enables CIOs to step up and play a decisive role in corporate affairs. We see such a seismic shift in the US health insurance industry, which faces the most sweeping changes in its half-century history. The ranks of the health care payers comprise more than 350 companies, with combined revenues of $500 billion and combined IT spending of $13 billion annually. These businesses range from statewide organizations, such as some of the Blue Cross and Blue Shield companies, to multistate companies, such as Aetna and Cigna.
        
        Three principal regulatory currents are producing the impending change: the recently passed federal health care reform bill, new health care IT mandates from last year’s US stimulus package, and ICD-10, a long-overdue expansion of coding standards for the exchange of health care data across payers and providers. Each of these forces will alter the way health care payers deliver services to patients and process the large flows of payments at the heart of the business. IT systems are central to both.
        
        Across the industry, most of the payers’ IT functions are not fully prepared to assume the enormous challenges posed by the required changes. Payers face stark choices. They can make major investments to upgrade systems in order, to meet the new requirements, or consider new operating models based on strategic partnerships utilizing other companies’ IT capabilities. How payers choose to address these questions will determine not only the success of individual companies but also the shape of the entire industry as it faces disruptive regulatory change.
        
        The Patient Protection and Affordable Care Act became law on March 23, 2010. Some of its provisions will be implemented within the first 12 months of enactment, and others will be phased in through 2014. The legislation anticipates 30 million new individuals will join insurance rolls, while an additional 100 million will be shifting policies. The law will usher in a fundamental change to the industry’s business model.
        
        Today, payers mostly follow a business-to-business model: 90 percent of all private policies are paid for by employers that negotiate prices and terms of coverage. The recent legislation mandates new insurance exchanges, subsidies, and tax credits that will lead millions of consumers to contract directly with the health insurance payers. These companies will need to develop new consumer-marketing capabilities and consumer IT on a large scale—for instance, advanced Web capabilities, insurance exchange connectors, and customer-relationship-management (CRM) systems.
        
        The new legislation also mandates dramatic changes to risk pooling and pricing. Going forward, all individuals will be able to purchase policies regardless of previous medical conditions, and the variability in pricing among policies has been severely restricted. All of a payer’s pricing- and risk-management systems will need to be redesigned. Successful payers will also have to provide products that appeal to previously uninsured healthy young adults, on the one hand, and to many more chronically ill individuals, on the other.
        
        At present, many payers have little expertise in the market segmentation and targeted marketing they will need. Similarly, they have little experience matching pricing to risk in these broad new markets. Both requirements will call for new consumer-marketing and consumer IT capabilities similar to those that successful credit card companies use.
        
        In 2009, the US Congress passed the American Recovery and Reinvestment Act (ARRA), which contains special provisions for health care IT. The goal, in the words of President Obama, was to “make sure that every doctor’s office and hospital in this country is using cutting-edge technology and electronic health records, so that we can cut red tape, prevent medical mistakes, and help save billions of dollars each year.”
        
        Starting in 2011, the ARRA will provide $37 billion to hospitals and physicians making “meaningful use” of electronic health records, with penalties for those failing to do so by 2015. To improve the performance of health care IT, the act targets $9.4 billion for areas such as telemedicine, data sharing, and broadband technology. An additional $2 billion is available to help build an infrastructure that provides for the ready exchange of electronic health records among the providers’ systems. Additional funding covers research on the effectiveness of treatments. The act also establishes standards for privacy, among other things.
        
        These reforms will first affect providers, as over the next decade health care will become rooted in readily available, comprehensive medical records and IT-based clinical decisions. Much is at stake for payers as well with the advent of extensive electronic health records. The ability to identify and propagate effective medical treatments and drug therapies will be important levers in the payers’ efforts to improve the health of patients and to cut health care costs. To promote those goals, the payers’ CIOs will need to build substantial new systems that can readily interface with health information exchanges and analyze electronic health records. Using advanced analytics, payers can ensure that their networks use only medically proven and cost-effective treatments.
        
        The modern data format documenting diagnosis and procedure codes—ICD-10—was released by the UN World Health Organization in 1994. Many developed countries, including Canada, Germany, and the United Kingdom, already use it. But it is overdue in the United States, where it will replace ICD-9 and expand the available number of medical codes by a factor of eight. This change will enable a much more detailed description of diagnoses and treatments.
        
        While ICD-10 promises to improve the accuracy of medical management and claims, its adoption will force payers to undertake an effort likely to exceed that of the Y2K campaign as they expand the fields in their databases and support the new coding structure. ICD-10 will require payers to upgrade most of their existing IT applications, including those for adjudicating claims, managing medical cases, contracting with providers, preventing fraud, billing customers, and paying providers. Yet while the costs of adopting ICD-10 are significant, the potential regulatory penalties for failing to adopt will make it a necessity.
        
        To address these changes, CIOs will need to transform more than 90 percent of a typical payer’s IT architecture and to help other executives make the corresponding changes in their business processes (Exhibit 1). We have indentified five core areas where improvements will be necessary.
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        To succeed in the retail market, payers must develop both new capabilities (such as online sales) for interacting with their customers and new marketing analytic capabilities (for instance, customer database management).
        
        The payers will have to create an integrated front-end sales system that appeals to today’s demanding consumer. Currently, payers primarily use the direct sales force to service large clients. In the future, they will also need online channels to attract and manage new individual customers at the retail level. To do so, these companies must build new capabilities few of them currently have—for example, to offer quotes and issue policies instantly, much as the most advanced auto insurers do today. Web sites will also be an important tool for helping individual customers to achieve healthier lifestyles. Payers that excel at these capabilities will cut medical costs, something that will be critical to success in the retail market.
        
        Payers will also have to compete successfully in the new health insurance exchanges. Lessons from the Massachusetts Health Connector demonstrate that a seamless data interface is essential. Payers will need IT systems that can present their products, services, and prices in a compelling way on these exchanges. They will have to support the flow of inquiries and orders from the exchanges in a robust fashion and to provide strong customer service on those orders. All of this will fall to CIOs and their teams.
        
        Success in the retail market will require payers to develop new marketing analytics as well. One of them, the ability to calculate the likely lifetime value of a customer, helps the best consumer-marketing companies segment their service and product offerings to customers. A lifetime-value view—a key to effective CRM—will give payers new capabilities for identifying, targeting, capturing, and retaining attractive consumers.
        
        To achieve such a capability, companies must track all interactions with individual customers and use the information to segment them into groups with materially different costs to serve and profit margins. Many payers now have multiple, siloed customer databases, which make it difficult to track individual members over their lifetimes or to recognize that they have experienced a major life event, such as a change in marital status, the onset of a major disease, or retirement. Achieving a lifetime-value view of consumers will require the CIOs of the payers to develop a single integrated view of their members and to identify all transactions associated with them.
        
        In the new environment, payers must be able to create new products—swiftly and flexibly—that are fine-tuned to the evolving marketplace. This goal will require the simplification of the IT systems that support product development and deployment. The resulting near-zero setup costs and reduced ongoing maintenance for new products will help accelerate product innovation.
        
        A typical payer today has a complex array of IT applications—often a patchwork of legacy and customized programs with hardwired interconnections. Payers have traditionally built products (for instance, GM’s employee health plan) from the ground up to meet the specific needs of a large employer group. They have not taken advantage of more modular approaches that might allow them to generate new products more cheaply and quickly, in a largely automated way. The payers’ CIOs will therefore need to create simplified IT architectures and product-development platforms that can be configured more easily than those of the present.
        
        Payers today have sophisticated actuarial risk-assessment systems that estimate the cost of providing health care to widely differing individuals and groups. The ability to price policies to reflect specific risks is now a key competitive differentiator among payers. But the health reform legislation restricts this practice significantly—for instance, by prohibiting the refusal to insure applicants with preexisting conditions. It requires the use of simplified pricing bands for all members, with a much lower price ratio from the cheapest to the most expensive policies. Implementing this regulatory change will require the payers’ CIOs and their business colleagues to retool the entire set of actuarial and pricing systems. In the future, payers will need to compete not through pricing or by excluding risk but by differentiating products and identifying customers in a sophisticated way. The CIO will be a key contributor to this change.
        
        The successful use of electronic health records, health information exchanges, and comparative-effectiveness research will ultimately improve the practice of medicine and cut medical costs. By making all relevant patient information available at the point of treatment, these developments will reduce duplication in medical diagnoses and improve patient outcomes through superior understanding.
        
        Successful payers will accelerate this change within their networks of providers and leverage the new information to improve their medical-management capabilities. CIOs will need to champion the right IT investments to achieve the maximum impact. The wealth of information flowing between providers, when combined with the payers’ own claims database and medical-management capabilities, will enable payers to have a positive influence on the practice of medicine in their provider networks. Success will hinge on inducing the best outcomes for patients at the lowest cost.
        
        Compliance with the ICD-10 standard is mandatory and will be essential to exchange standardized data. Payers will need to achieve ICD-10 coding standard by October 2013. Failure to meet this date could result in significant penalties, so the payers’ CIOs must push the changes through their organizations.
        
        Data structures will change significantly, as the new ICD-10 codes contain seven digits versus five with the previous ICD-9 codes (Exhibit 2). Significant changes will also have to be made in program logic, since the new codes do not map directly with the old ones. In fact, as noted above the number of codes will increase significantly—a tremendous growth in documentation of the medical details of diagnosis and treatment.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Most challenging, perhaps, CIOs will need to facilitate major process changes not only within the payers’ organizations but also outside them. ICD-10 will substantively influence a payer’s medical-management, network-contracting, claims-processing, customer service, actuarial, underwriting, and business intelligence functions. It will also require collaboration among payers and health service providers so that they can adopt the new codes simultaneously.
        
        Tackling any one of these developments would represent a substantial test for even the most capable CIO. Combining and managing them in the short time frame set by the law represents an unprecedented challenge. Among the payers we have studied, two strategies have emerged. We have highlighted the experiences of two companies to illustrate these differing approaches.
        
        Payer A is a large payer with a strong network of relationships with providers. Most IT functions remain in-house, including the development and maintenance of applications and the management of the IT infrastructure. External vendors are called on periodically when extra help is needed to develop applications.
        
        A large number of Payer A’s members have lost their employer-sponsored health insurance during the recession, so the company faces a challenging environment. Many individuals will be entering the market over the next few years as a result of the new mandates, but these consumers will be cost conscious and looking for innovative, standardized, and inexpensive products not currently offered by the payer. Additional IT investments to meet other legislative demands will squeeze already-constrained budgets.
        
        In the face of existing cost pressures, the CIO has already pushed for improvements in overall IT efficiency. He has reduced IT costs 18 percent by improving efficiency in a number of areas, such as desktop support, contractor expenses, and the productivity of application developers. These moves will help finance new investments. In addition, during the past four years the CIO has consolidated the payer’s claims platforms around a modular architecture, which eliminated legacy elements that constrained product-development flexibility.
        
        The CIO and his group now are focusing their efforts on implementing specific IT capabilities mandated by new legislation. A command center with a rapid-response team will be at the center of those efforts. The command center is responsible for creating a coordinated plan to address all the challenges of the legislative changes, including supporting the meaningful use of health IT in their provider network.
        
        This CIO is working with the company leaders who interface with health service providers to codify new rules that will determine which medical procedures, tests, and drugs will be eligible for payment. The new rules will be incorporated in jointly administered payer–provider IT systems. Adherence to these requirements (for example, the elimination of duplicate tests and the obligatory use of generic drugs) could cut the payer’s costs by 2 percent, or $300 million, and thus allow for significant market share gains.
        
        Work on a new user-friendly and technically robust portal for members is nearing completion. The portal is structured around patient codes, or “identifiers,” that track the health and medical spending of individuals over their lifetimes. The CIO has also installed new product-development tools that will take advantage of a rationalized IT architecture to lower costs and speed up time to market. Still in the planning stages is a new pricing- and risk-management system, which will be in place by 2013, when new regulations take effect.
        
        Payer B is a midsize company with fewer than one million customers. Some years ago, the company chose to outsource most of its IT functions. The outsourcer operates the payer’s IT infrastructure and develops and maintains software applications. The outsourcing arrangement also covers other services, such as claims processing and membership enrollment, as well as the payer’s IT interface to its provider network.
        
        Outsourcing may be an attractive option for many smaller, capital-constrained payers. It transfers operational risk to the strategic partners, many of which have extensive IT experience and leading industry talent. Partners are responsible for compliance with government mandates and cost management. And thanks to substantial economies of scale, the outsourcer offers IT services at a cost well below what Payer B could have achieved in-house.
        
        Under this model, the role of the payer’s IT organization largely consists of planning and vendor management. The payer sets its overall product, marketing, and technology strategy and retains its relationships with customers. The partnership allows Payer B’s management to concentrate on business growth, product development, and strategies for the new health care environment. Payer B, for example, is developing a new consumer sales strategy, while relying on its outsourcer to provide all the technology needed to help it connect to the new health insurance exchanges.
        
        These are challenging times for the CIOs of US health insurance payers but also times of great opportunity. Forward-looking CIOs who can work effectively with their peers, both within their own organizations and across the new landscape, will be critical to the success of payer organizations in the volatile US health care environment that’s emerging.New regulations that require US health care providers to use electronic health records (EHR) and adhere to strict data-coding standards will force hospitals to spend billions of dollars over the next decade to upgrade their IT systems. The spending requirements risk squeezing hospital capital budgets already under strain from steadily rising costs. With government incentives covering only a small portion of the total, providers will be forced to recover quickly their investment dollars from operating changes.
        
        Our research shows that automating and standardizing health care information can bring benefits that extend beyond meeting demands for compliance. A provider that creates a best-practice IT platform to house and share medical records, to manage hospital resources more transparently, and to define precise guidelines for medically authorized tests and procedures can generate significant operating efficiencies. Such a platform minimizes paperwork, reduces the number of unnecessary treatments, and lowers the risk of drug and medical error.
        
        The productivity and resource savings often pay back the initial IT investment within two to four years while also producing better health outcomes for patients. We estimate that total savings across the US provider landscape could be on the order of $40 billion annually. (By comparison, about $1.3 trillion a year is spent on inpatient and outpatient services across the United States and about $80 billion on health care IT.) Achieving such a positive return on investment (ROI), however, requires distinctive change-management skills among hospital leaders, better governance, and sustained engagement from key clinicians.
        
        Estimates suggest that a wave of US legislation and regulatory changes will affect up to 80 percent of the existing hospital IT applications. Among the most far-reaching of these developments are provisions, laid down by the American Reinvestment and Recovery Act (ARRA), requiring health care providers to implement IT capabilities such as electronic health records and computerized-physician-order-entry (CPOE) systems. While some providers use electronic health records on a limited basis, the new regulations standardize what is expected from them and make their use mandatory.
        
        An accelerated timetable means that US health care providers have until the end of 2015 to make the investments or face fines starting at $2,000 a bed in the first year and up to $35,000 a bed by 2019. In addition, both revisions to the Health Insurance Portability and Accountability Act (HIPAA) 5010 and the switch to ICD-10 require providers to apply strict new data-coding standards—no small task given the number of databases, hospital systems, and clinicians affected.
        
        To meet these various requirements, US hospitals will need to spend approximately $120 billion, at an average cost of $80,000 to $100,000 per bed, for the required project planning, software, hardware, implementation, and training. Although the ARRA provides financial incentives under Medicare, these reimbursements offset only approximately 15 to 20 percent of total expenditures. For an average provider, the result is a spending gap of about $60,000 to $80,000 a bed (exhibit). With costs already rising by approximately 10 percent annually—and outpacing revenues—these investments will place new financial burdens on hospitals.
        
        Exhibit We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Many of the mandated improvements could bring wider benefits, especially in enforcing the behavioral changes needed to standardize provider practices. Electronic-health-record systems provide clinicians and other staff members with online access to patient data and decision support, such as lab reports and treatment order sets. Computerized physician order entry, a major component of those systems, requires physicians, nurse practitioners, and other specialists to follow a menu of defined procedures when requesting services, tests, or drugs for patients. Combined with clinical-decision-support (CDS) tools that give physicians best-practice guidelines for medical procedures and with stricter coding classifications, electronic health records not only broaden access to medical information but also serve as a forcing agent to spur the adoption of standard operating procedures and best medical practice.
        
        These developments could reduce medical errors and foster better health outcomes while decreasing waste and administrative time. They could also strengthen the financial health of hospitals. Our research shows that optimizing the use of labor, reducing the number of adverse drug events and duplicate tests, and instituting revenue cycle management can help typical hospitals generate savings of some $25,000 to $44,000 per bed a year. On an industry-wide basis in the United States, this translates into $30 billion to $40 billion annually.
        
        Electronic health records and related technologies can be applied to improve the delivery of health care in several core areas.
        
        Many hospitals continue to rely on manual charting, paper records, and outdated software to manage bed counts, schedule staff, and reserve key resources, such as operating rooms and imaging machines. Electronic health records and computerized physician order entry bring these elements together online, automating charts, records, and medical information about patients and directing medical staff toward protocols clinically proven to be more effective in treating illnesses.
        
        When these technologies are linked to bed-management and equipment-scheduling software, doctors, nurses, and administrators can assess current and projected bed counts and optimize the scheduling of key equipment (for instance, x-ray systems) and the level of staffing. This approach reduces not only administrative waste (such as time spent tracking down medical information or calling to secure needed services) but also the level of overbooking, simultaneously improving bed turnover. The results can save upward of $20,000 per bed in labor utilization alone.
        
        Electronic health records and computerized-physician-order-entry systems can sharply reduce the risk of prescription error and negative drug interactions by mapping patient histories with information from drug manufacturers to highlight the risks of prescribing a particular product. Problems with drugs cost hospitals $8,000 to $15,000 per bed each year, or between $1.6 million and $3 million for an average 200-bed hospital. Access to medical information allows physicians to adjust prescriptions or dosages to prevent complications, improve the quality of care, and reduce the human impact of adverse drug events.
        
        Every year, roughly 0.4 percent of hospital services go unbilled, at a cost per bed of just over $4,000. Some of the billing issues result from coding errors or eligibility questions. Coupled with data standards such as ICD-10, computerized-physician-order-entry systems promote the consistent naming, coding, and classification of treatments, allowing hospitals to improve the oversight of all procedures and to increase the first-time pass-through of claims.
        
        When all health records are stored in electronic format and providers gain access to them through health information exchanges, they become more widely accessible to doctors, insurers, hospital administrators, and patients, regardless of location. This kind of visibility gives clinicians a more complete sense of a patient’s history and reduces the need for duplicate tests that can affect the quality, cost, and speed of care.
        
        An average hospital can pay back its initial (and usually onetime) investment in two to four years; cost savings accrue year on year. Health care providers with better-integrated systems often realize even higher ROI.
        
        The realization of the benefits from health care IT investments will require a radically new approach to IT on the part of the CIOs of health care providers, as well as the business leaders and clinicians those CIOs serve. Health care providers will need to use new approaches to achieve an inclusive governance process with streamlined decision-making authority, a radically simplified IT architecture, and a megaproject-management capability.
        
        One midsize US acute-care hospital discovered this truth when it implemented an integrated IT system designed to reduce the number of adverse drug events, improve remote access to data, and increase overall patient safety. Rather than taking a phased approach that would have allowed the IT team to factor in lessons learned as it went along, the team forged ahead on multiple fronts. Delays mounted as the hospital’s vendor struggled under the volume of the new requirements for software applications. The lack of senior-leadership direction and input from physician leaders meant that the system went live with gaps in the standard guidelines, such as basic guidance on aspirin dosages for patients with heart problems. Rather than reducing the number of adverse drug events, the new system actually raised error rates.
        
        As the following examples show, hospitals can benefit from effectively implementing electronic health records, computerized-physician-order-entry systems, and coding standards.
        
        An enormous gain. A regional health care provider in Canada successfully implemented electronic health records and related systems in four large hospitals. Over four years, the hospital system developed standard guidelines for medical procedures and decision support protocols, configured and implemented the new IT system, and rolled it out to the area’s four major hospitals, achieving a 90 percent-plus adoption rate by clinicians.
        
        Although the upgrade’s cost was substantial (approximately Can $100 million), the improvements generated the same amount in annual savings by reducing labor requirements, duplicate lab tests, and adverse drug events. This achievement freed staff members to focus more time on their primary duties, a shift that allowed the hospitals to treat 20 percent more patients without an increase in personnel or reduction in quality. A 20 percent jump in productivity is an enormous gain—in a US-style health care system, that level of performance would translate into more than $100,000 per bed a year in savings. That’s far higher than our conservative ROI estimate for typical successful implementations. Moreover, the use of standardized guidelines developed by the region’s leading physicians helped spread the use of best-practice medical procedures, which improved patient outcomes. In cardiac care, for instance, patients treated with the recommended protocols healed more quickly and spent fewer days in hospitals.
        
        Simple is better. A large health system sought to consolidate its disparate IT assets as it reorganized. The existing IT footprint was diverse, with multiple instances of similar applications, variations in capabilities across care settings, and a diverse portfolio of vendors. These problems raised operating and capital requirements and made it hard to share information across hospitals.
        
        In planning a long-term investment strategy, the health system developed a radically simplified blueprint of its end-state IT. The focus was to migrate to a single platform for each purpose, leverage existing assets, and redirect investment toward new capabilities. By taking this approach, the health system developed a five-year investment strategy eliminating 80 percent of its existing platforms, with estimated run rate savings of 30 percent, while maintaining its historical capital-investment levels.
        
        The benefits of planning and piloting. A large US health care provider with about 50 hospitals across multiple states sought to implement an electronic-health-record system that would achieve high acceptance among clinicians. A handpicked IT team led the project. Because this was as much an organizational-change assignment as it was an IT implementation, the team spent substantial time at the outset planning what processes should be improved, engaging frequently with leaders in the physician community to discuss expectations, and formulating a list of desired changes.
        
        To manage complexity, the team piloted the first few rollouts so it could make improvements as it went along. This approach helped the team complete the subsequent implementation in the hospital system as a whole much more quickly than would have been possible without learning from the pilots. Training materials and change-management techniques were piloted as well to smooth the transition and encourage adoption. Although the implementation is ongoing, after the first three months this phased-in approach has already resulted in adoption levels as high as 75 percent for computerized-order-entry systems. The project is tracking close to time and budget.
        
        As these cases show, three success factors distinguish the best IT implementations among health care providers.
        
        Governance with real authority. Involving key stakeholders, such as clinicians and hospital administrators, early in the IT decision process is critical to ensure buy-in and to inform requirements. To achieve the value at stake, clinicians and administrators will need to change their behavior. In many cases, this is very difficult to do.
        
        The regional health care provider in Canada used a radical new governance model to get highly regarded physicians deeply involved as champions in the process of selecting the IT system and leading the development of guidelines for medical procedures. These physicians also helped the provider engage with the broader physician community to encourage acceptance during the implementation of the project and adoption after it was completed.
        
        Our experience indicates that provider IT implementations of this magnitude can be successful. But they always require a step change in an organization’s approach to governance and change management, from the selection of clinician leaders and champions to the creation of governing bodies focused on specific topics (such as order set development), clear decision authority among stakeholders, and training at the point where IT systems interact.
        
        Radical simplification of architecture. Diverse IT applications and platforms, common among providers today, create a significant degree of complexity, raise costs, and lengthen implementation time lines. The delays and cost escalation can undermine large-scale health care IT implementations. A radically simplified architecture, which eliminates the complexity and reduces the cost of large-scale system implementations, can be a critical prerequisite to success.
        
        Methodical planning and execution. For IT to enable better clinical work flows and medical practices, many elements must come together. The implementation team should address the IT architecture, standards, and changed medical practices in the early planning stages. Implementation should be rolled out in a systematic, modular way, with active tracking of progress and lessons learned. It’s important not to underestimate the amount of time and due diligence that go into planning a megaproject.
        
        The Canadian hospital system, for instance, allocated approximately 30 percent of its total project budget to change management—a figure that is consistent with those in other successful implementations and fits within our estimate of $80,000 to $100,000 a bed. The US health care provider rolled out its IT system in a series of phased pilots across each hospital, an approach that allowed it to capture lessons and use them in subsequent rollouts. A full-time implementation team and “war room” were established to track progress and provide support and training to physicians throughout the change process.
        
        All successful large-scale implementations of health care provider IT systems have used similar sophisticated megaproject-management approaches.
        
        Over the next decade, costs and regulatory mandates will require providers to make significant new investments in health care IT. Given the value at stake, hospital management will be under pressure to demonstrate an appropriate return for every dollar spent. Hospitals that take a systemwide approach to overhauling IT—a governance model with real authority, a radically simplified IT architecture, and a robust megaproject-management methodology—will be well positioned not only to meet their compliance responsibilities but also to lower their operating costs significantly while improving the quality of patient care.What are the prospects for an overhaul of the US health care payments system? The recent passage of comprehensive health insurance legislation only adds to the pressure for transforming the system that manages medical bills, claims, and payments. We foresee big changes in coming years, with billions of dollars of value at stake.
        
        The June 2007 McKinsey Quarterly article “Overhauling the US health care payment system” argued that the greater “electronification” of health care transactions, the growing adoption of standards, and increasing innovation by cross-industry entrants would lead to a major restructuring of the US health care payments value chain. Two and half years later, we are still waiting for that massive overhaul. But we believe that major change in the payments landscape is inevitable because of fundamental industry dynamics, such as the proliferation and increasing complexity of health care transactions, the increasingly prominent role of the consumer in payments, and the rising importance of medical and financial risk management for providers. And the pace of change will only accelerate with the rolling out of the new health care law, as more individuals become insured and begin to generate more health-related transactions and industry participants face greater pressure to reduce administrative costs.
        
        We acknowledge that there has been progress in improving the health care payment system. There has been a steady conversion to electronic-data formats, thanks to the adoption of standards across different transaction types compliant with the HIPAA (for example, claims submissions, eligibility checks, and remittance advice), along with the more widespread use of electronic formats and transaction-processing clearinghouses. We estimate that by 2012, about 80 percent of the projected eight billion core US health care transactions will be in electronic formats, excluding lab and pharmacy, which are already largely electronic.
        
        Also, we have observed movement toward developing technical solutions to health care payments problems. There are numerous innovative approaches on the market aimed at improving the transparency and efficiency of payments, with companies offering products such as online bill-paying solutions, patient liability–estimation tools, point-of-sale consumer payments processing, and structured financing solutions. In addition, large health care IT players (for example, GE and McKesson) and a range of financial institutions (JPMorgan Chase and PNC Bank, for instance) continue to make significant investments in health care payments processing, while large payers and providers explore ways to partner to solve payments issues.
        
        The transition to electronic formats, as well as technological innovation, has laid the groundwork for the more fundamental restructuring of health care payments outlined in the 2007 article. But there is still much work to be done: the system remains highly fragmented and inefficient, consuming a disproportionate share of dollars compared with payments systems in other industries. Unlike scale utility approaches that have emerged in financial services or telecommunications, innovative solutions in health care have failed to take hold at scale—either because of misaligned incentives among stakeholders or because few players have the local-market position to drive adoption across a fragmented provider community. And consumer bad debt continues to rise, resulting in more than $65 billion in uncollected revenues in 2010, according to our latest estimates, putting enormous strain on provider economics. We will see more progress in coming years as industry participants address three major challenges.
        
        The volume and complexity of health care transactions is rapidly expanding. We estimate that the number of HIPAA-compliant transaction sets will grow at a compound annual growth rate (CAGR) of about 8 percent in coming years. In addition, new regulations, such as 5010 and ICD-10 (aimed at creating better-defined data standards), are adding more complexity and forcing the expenditure of hundreds of millions of dollars in investments.
        
        We also foresee an explosion of digitized, stored, and transferable clinical data. Today, less than 20 percent of clinical data is electronic, with little standardization across data fields. We see a rapid shift toward the greater use of electronic formats and standardization, in large part spurred by the electronic-health-record requirements in the American Recovery and Reinvestment Act of 2009.
        
        The complexity of clinical data should not be underestimated—a typical patient-level clinical data set can include more than 800 discrete fields, compared with only about 20 to 30 for a financial transaction. Digitizing, standardizing, and normalizing this data so that they can be used for operational and clinical decision making will require large capital investments and create ongoing operating costs. Few health care industry players have the scale or sophistication to manage these issues on their own.
        
        Resulting in part from this systemwide complexity, industry administrative costs will grow by about 10 percent annually over the coming years—higher than the rate of growth of medical inflation. Tackling these costs will require private-sector action. The US Congressional Budget Office (CBO) has only scored about $3 billion in administrative cost savings by 2014 from the Senate and House versions of the recently passed health care law. This amount will cover only a fraction of total industry administrative costs.
        
        The pressure to manage costs should increase the willingness of industry participants to work together and to try new approaches. Cross-industry collaboration could finally spur the creation of payment utilities such as full-cycle-payment automation (described in our 2007 article). As noted there, we believe in the potential for cross-industry collaboration to create an at-scale payment-settlement utility that knits together health care transaction processing through clearinghouses, the automated clearinghouse payment network, and card network payments for retail payments.
        
        According to our modeling of the 2010 flow of health care funds, consumers now pay more in health care costs than do employers. This cost is split among direct payment of noncovered services, out-of-pocket expenses after insurance, and the consumers’ share of premium expenses. Uncompensated care from 46 million uninsured Americans continues to be an issue, but one that will be mitigated by reform. In fact, the fastest-growing portion of bad debt stems from what insured patients fail to pay after insurers have paid their portion of medical bills. This category will likely grow as more insured patients enter the market following passage of the new health care law. For example, at one multifacility hospital system, we found that, for insured patients, “balance after insurance” is growing at 30 percent a year; for patients without insurance (those who pay for services from their own pockets), that figure is only 19 percent.
        
        This trend will place balance-after-insurance collection issues front and center in the health care payments landscape. Physician practices and hospitals alike could do better through innovative patient financing, better front-end retail-revenue-cycle approaches, and consumer-friendly collections. In particular, there will be greater pressure on physicians regarding balance-after-insurance collections as reform accelerates the shift from in-patient to out-patient and from hospitals to physician- or clinic-based settings. Many of these smaller practices are poorly equipped to deal with the resulting retail-revenue-cycle challenges given current mind-sets, workflow practices, and capabilities.
        
        The need to deal with these issues opens the door for greater penetration of point-of-service, retail-oriented payments approaches. For the physician segment, how these solutions are provided, in particular, is what matters—solutions with bundled financing options or ASP models that are integrated into the physician’s workflow and require little capital investment will probably see the most uptake.
        
        Sidebar Modifying consumer behavior As consumers take on more of the risk associated with health care, the traditional relationship among consumers, providers, and payers is changing. With persistent medical inflation, employers continue to promote greater employee cost sharing to reduce their health care spending. Between direct payments to providers from both self-pay consumers and the insured (and their share of insurance premiums), individuals play a major role in the flow of health care funds. And the recent health care insurance law will only increase the role of individuals, as many previously uninsured people enroll in insurance plans, actively utilize health care services, and become responsible for balance-after-insurance payments. The shift to a more consumer-oriented system poses challenges for providers as well. Few of them (with notable exceptions, such as dental practices) are able to estimate a patient’s out-of-pocket expenses, present a bill at the point of service, and collect payment then and there. Instead, providers send a bill, often weeks after the event, and hope the patient pays. To complicate matters, patients typically receive an “explanation of benefits” statement from their insurer—not a bill, but an estimate of their liability after adjudication, which can be more confusing than helpful, since its timing and content are seldom coordinated with the provider’s bill. Moreover, there are few deterrents to nonpayment. Providers are typically reluctant to pursue patients aggressively for fear of reputational risk. Although medical expenses sent to collections do show up on credit reports, many lenders discount them. As a result, provider collection rates run at 50 to 70 percent for small-dollar liabilities for insured patients and fall to about 10 percent for self-pay patients. Uncollected revenues represent 4 to 6 percent of hospital gross revenues. We estimate that in 2010, bad debt will reach some $65 billion. The prevailing assumption is that consumers are unable or unwilling to pay their health care bills. Our consumer research suggests otherwise. It reveals that the lack of financing options, inefficiencies in billing practices, and consumer confusion are all major drivers of nonpayment (Exhibit 1). In fact, our analysis suggests that the vast majority (more than 74 percent) of insured consumers are both able and willing to pay their out-of-pocket medical expenses for annual liabilities of less than $1,000 a year (Exhibit 2). Indeed, more than 90 percent are willing and able to pay if these liabilities are less than $500. Yet collection rates lag well behind these levels, even for relatively low-ticket payments. Exhibit 1 Why don’t they pay? A lack of options and confusion over what’s owed underlie nonpayment of health care bills. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com Exhibit 2 Willing and able Most consumers are willing and able to pay their medical expenses. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com For annual member liabilities greater than $1,000 a year, the “willing and able to pay” segment drops to 62 percent. The pressure around health care expenses that consumers face will only worsen as rising deductibles and out-of-pocket expenses—as well as the explosion in chronic conditions requiring life-long health care—place heavier burdens on household finances. The industry thus faces a stark choice: improve retail-payment capabilities to help consumers manage their health care financing better or risk having rising costs overwhelm consumers’ current willingness and ability to pay. Our research suggests that the industry can address a significant portion of the bad debt in the system through approaches that not only make payments more convenient, less confusing, and easier to distribute over time through financing but also reposition medical bill payments in the household payment hierarchy. New payment solutions must tackle consumers’ confusion and concerns head on. Automated payments, patient statements (instead of bills and explanations of benefits), structured payments plans or lines of credit, and even incentives and reward points based on principles from behavioral economics should all be considered in building a value proposition that consumers will readily adopt. Such innovative payments approaches could create nearly $60 billion a year in value as well as achieve substantial savings in the administrative costs associated with inefficient processing and collections. A model will create value only if it succeeds in inducing more consumers to pay more of their medical bills than they currently do. It must therefore garner adoption beyond those who already pay their bills and who might self-select into a more convenient payment mechanism. It must also reposition health care expenses at the top of the household payment hierarchy, since these expenses typically fall at the bottom. For example, health insurance premiums follow mortgage or rent and utilities as the third most important expense for most households. Medical expenses, on the other hand, rank seventh (after cell phone and Internet bills), with only 7 percent of households rating medical expenses as a top priority within their household budgets. The payments hierarchy reveals that partnerships between insurers and providers could help improve collections, given the much higher importance of paying premiums. Such collaboration could take the form of integrating balance-after-insurance billing with the explanation of benefits or health statement (online or paper), with or without credit risk-taking by the insurer. Further, we estimate that if currently insured consumers had access to more convenient payment mechanisms and structured payment or financing options to help them smooth spiky medical expenses into tight household budgets, only 10 percent of their bad debt would remain uncollectable. This approach could include card-based solutions. Our research suggests that 52 percent of consumers would be willing to use a credit or debit card in a health care transaction if a good-faith estimate of up-front costs were provided, at payment levels that cover the out-of-pocket costs of a typical physician visit or out-patient procedure (Exhibit 3). Exhibit 3 Charge it, please More than half of the respondents were willing to pay health care bills by using credit or debit cards. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        In addition to transactional efficiency, industry dynamics will likely drive the need for innovative financing methods that better meet the requirements of health care consumers. There is little doubt that millions of households will continue to struggle with managing health care costs, despite consumers’ willingness and ability to pay their bills (see sidebar, “Modifying consumer behavior”). Coming up with compelling consumer payment solutions will require financial intermediaries to step in and provide financing and structured-payment options.
        
        As the health care industry moves toward outcome-based, bundled payments, providers must increasingly leverage payment and clinical insights to better understand and manage medical risk. Most providers remain woefully ignorant of the economics relating to patient segments, service lines, geographies, and payers. To understand profitability at a detailed level in an outcome-focused world, providers must have access to and analyze normalized clinical, claims, and payments data.
        
        To get there, several things must happen. First, electronic-health-record adoption and data standardization must continue. By applying CBO data, we estimate that 55 percent of hospitals and 85 percent of physician practices will reach the basic stages of meaningful use by 2014.
        
        Second, the health information exchange (HIE) infrastructure must expand to provide connectivity. There are already 150 to 200 HIEs across the country, supported by a broad ecosystem of technology players, ranging from large-scale providers, such as Cerner, IBM, and Misys; to smaller ones, such as dbMotion and Medicity; to publicly backed approaches, such as the open-source software advanced by the Nationwide Health Information Network (NHIN). Larger hospital systems are also increasingly building out private HIEs that can help them better integrate care and manage their referral network, in some cases in collaboration with payers.
        
        Finally, relevant clinical data will need to be integrated with claims information, charge data, and remittance information in a way that enables analytics on issues such as cost management, physician management, reimbursement optimization, and service line profitability. As with other transaction-processing and analytic capabilities in health care, developing these solutions will likely require innovative cross-industry collaborations involving some combination of intermediaries and infrastructure providers (payers, IT vendors, and financial institutions) and analytics service providers.
        
        Over time, it should be possible to automate the full cycle of information and payment flows in health care, from the submission of claims to the receipt of payments and reconciliation. The development of an automated payment network would reduce bad debt, cut administrative costs, and save billions of dollars. It would also create the infrastructure needed to sustain the health care payments system in a more retail-oriented world. Beyond such a network, the integration of payments with clinical data (such as claims histories and pharmacy records) promises to provide the infrastructure and analytical basis for the next generation of innovation in provider incentives and payments, such as evidence-based “pay for value” reimbursement models. The momentum behind this transition is growing as the rising incidence of diabetes, obesity, and other chronic conditions requiring long-term care changes the underlying nature of health care financing risk. The need for a system that helps players to address these issues—and consumers to manage the financial aspects of their health care—is becoming ever more pressing. The incremental progress we have seen over the past few years is indicative of much greater change ahead.More than 140 million Americans currently have discretion over health insurance purchases, representing a total of $785 billion in premiums or premium equivalents. Yet most are unable to revisit their current insurance status. A combination of economic anxiety, confusing insurance products, and inadequate distribution is leading to consumer paralysis. Moreover, our research suggests that millions would fail to make rational economic choices even if they understood their options better. Unlike employers that purchase health insurance for their workers, consumers approach this issue by factoring in much more than expense management. More specifically, consumers’ purchasing decisions are often emotionally based, as they are seeking peace of mind in their choices.
        
        Sidebar About the research The 2009 McKinsey retail health care consumer online survey was completed by 4,100 participants. In addition, over 26,000 consumers provided basic demographic and health insurance information. The demographics of the participants mirrored those of the US adult population; however, we oversampled “retail health care consumer” populations for greater statistical insight. For participants under 65 years of age, we surveyed the individually insured, the uninsured, and people with group-sponsored health insurance but with a choice among insurer options provided by their employers or those of their spouses’. For participants 65 years and older, we surveyed people with group-sponsored health insurance who had a choice among insurers, with Medicare Advantage or private fee-for-service coverage, and those with individual-retiree supplemental products.
        
        Despite rising concern and stated willingness to purchase health insurance products (up 11 percentage points since the survey we conducted in 2007), sales have not kept pace with demand—in fact they have remained steady while the amount of shopping has increased (see sidebar, “About the research”). Forty-one percent of retail health consumers report actively shopping, a proportion that has been growing rapidly. And no wonder: the number of people with health-related financial concerns has increased by more than ten percentage points (Exhibit 1), making consumers more willing to consider new trade-offs to protect themselves against health care financing exposure. For Americans, health coverage is the second-highest concern, after day-to-day living expenses (Exhibit 2).
        
        Exhibit 1 Growing concern Consumers have grown even more concerned about health-related expenses. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Exhibit 2 Health insurance worries Health coverage ranks second, after day-to-day living expenses, as a top financial concern for US consumers. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        What is preventing health insurers from effectively addressing this pent-up demand? Our research suggests that a primary barrier is their belief that consumers make economically rational decisions about health benefits. It’s a misguided view. Faced with more choice, complexity, and financial exposure for their health care in an increasingly uncertain world, what consumers are really seeking is peace of mind (Exhibit 3).
        
        Exhibit 3 The value of peace of mind Consumers value the peace of mind provided by health insurance over more practical concerns such as coverage or networks. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        If industry participants are to convert concern into increased sales—and win in the market—they must address these emotional concerns directly. That, however, will require a flexible approach and a new mind-set, which must be embraced across the entire organization. But the benefits are clear. We estimate that closing the gap between consumer needs and product purchase could increase health insurance revenues by as much as $200 billion. What’s more, our research shows that high-performing health insurers convert three times more consumers into purchasers than lower-performing ones do. Insurers that focus on the following five areas will have a better chance of meeting the true needs of health care consumers.
        
        Although consumers are anxious, few understand the severity, or probability, of the risks they face. Health insurers can benefit from educating consumers by helping them translate concern into action, thereby expanding the market. For example, 73 percent of consumers who purchased individual insurance felt that they understood their health insurance options, compared with 51 percent of the uninsured.
        
        Most consumers are unaware of how much major medical procedures actually cost: three out of four consumers report having “no idea” how much they cost. Additionally, few consumers accurately assess the probability of certain health-related risks. Most overestimate the risks of salient but relatively low-probability events (fatal accidents, for example) and underestimate the risk of more common health events (such as heart attacks).
        
        Educating consumers is an essential part of motivating them to address their concerns. Messaging, for example, really matters. Cost–benefit information alone fails to address emotional anxieties. Consumers often respond much more effectively to anecdotes than to a recitation of facts. For example, concern over health care financing increases significantly when a friend or family member experiences a financial crisis—often more than it would given a similar personal experience.
        
        To educate, insurers should make better use of their traditional channels, whether it’s a captive sales force or independent agents. They will also need to explore emerging social-networking and Web 2.0 approaches. For example, sites such as PatientsLikeMe.com are showing how insurers can use these new technologies to get closer to consumers. It is possible to imagine such a site, or similar ones, extending beyond disease-specific networking to consumer suggestions on what health insurance to buy.
        
        When buying insurance, consumers typically frame their approach around effective shopping experiences in other markets, yet health insurers continue to create barriers to purchase by offering a confusing and overwhelming experience. Once consumers decide to consider insurance options, a quick, simple, and personalized experience increases their propensity to purchase. Our research shows that three elements are critical in delivering this kind of purchase experience.
        
        Consumers tend to make decisions when confronted with a change to the status quo. A job change or loss, retirement, and eligibility for Medicare are the most common motivators for consumers to purchase or switch health insurance. Similarly, consumers who have recently experienced a medical event are 13 percent more likely to shop for insurance and 27 percent more likely to make a purchase—and much more likely to purchase supplemental products. In fact, consumers highly concerned about their health-related financial exposure purchase 60 percent more products than the average consumer; nearly two-thirds of concerned consumers own five or more products, compared with less than half that for the average consumer. Insurers should tailor the purchase experience around these life events and address consumers’ emotional needs directly.
        
        Consumers are also more open to addressing multiple concerns at the moment that they decide to purchase. In retail banking, for example, more than 70 percent of sales happen the day a customer opens an account. In a similar fashion, health insurers should be ready to deliver a full suite of products at the moment the consumer is looking to buy. Today, only 15 percent of individually insured people purchase an additional product (for example, pharmacy, dental, or vision coverage) when they first purchase health insurance. Offering consumers multiproduct, multiline sales—a seamless, integrated sale that involves a single application and multiproduct underwriting—at the moment when they decide to address a risk can increase the chance of success. Multiline sales also minimize redundant coverage across disparate products.
        
        Consumers want their needs addressed during the sales experience. Seventy-seven percent of those who purchased health insurance felt that the distributor provided options to meet their needs, compared with only 46 percent for those who ended up not making a purchase decision. Similarly, 81 percent of consumers who purchased health insurance felt that the distributor had a good understanding of their situation during the purchasing process, compared with 44 percent for those who did not make a purchase. Customers who have their questions answered and who feel that the distributor was less self-interested are more inclined to purchase. When approaching the purchasing experience, insurers should make sure they address consumers’ emotional needs directly and provide solutions that are specific to them.
        
        When faced with complexity or too many choices, consumers often fail to act. For example, 74 percent of those who purchased health insurance were satisfied with the ease of the process, compared with 47 percent of those who shopped but didn’t purchase. Few consumers can comprehend the detailed information presented to them—in fact, a third of those who found the purchase process confusing cited “too much information” as the main barrier to purchasing. Health insurers should make it easier for consumers to make choices by offering simple, understandable information. Unfortunately, there are few examples of best practice in the health insurance industry in this realm, although many exist in other industries (such as auto insurance).
        
        Comprehensively addressing the consumers’ health-related financial exposures (health, disability, long-term care) can involve bringing together multiple companies or distributors, further increasing complexity and confusion. To simplify the decision-making process, insurers should present consumers with multiple product options—for example, a recommendation together with one or two comparable alternatives.
        
        Consumers who do purchase or switch health insurers report that the purchase process was fast. The likelihood of individual purchase correlates strongly with the turnaround time for the application. For example, 55 percent made a purchase when the process lasted only a few minutes, compared with 21 percent for a Web transaction that took place over a 24-hour period. Most consumers are also unlikely to shop multiple channels—more than half only use one source of information, although it varies by segment—so rapid access and ability to transact quickly across a range of channels is important. End-to-end improvements of the acquisition process based on lean improvements and front-end automation can help make the process more customer-centric.
        
        Overwhelmingly, consumers cite trustworthiness as the most important factor in considering or staying with a health insurer. Insurers must develop a reputation that offers peace of mind by consistently delivering on their brand promises. Brand awareness is important, particularly since the average consumer considers only two companies when shopping. But trust is critical to driving sales: consumers are two to three times more likely to consider insurers with a strong brand reputation.
        
        Delivering peace of mind is similarly important in retention and capturing the cross-sell opportunity. In the face of inertia and complexity, 71 percent of consumers prefer to stay with their current insurer. Yet this depends on whether their health insurer delivers on its promise: consumers who leave insurers are 13 percent less satisfied with them than consumers who chose to stay.
        
        Although insurers can buy brand awareness, brand reputation must be earned. Whether they offer great service or low cost, insurers should clearly articulate what they are promising and then consistently deliver it. This requires building a flexible service operations model that delivers consistent service at a reasonable price. Since consumers are ultimately seeking peace of mind, the key is to avoid “negative moments of truth.” Our survey shows the level of risk resulting from negative service events is much greater than the benefits of positive ones.
        
        Consistently delivering on a brand promise can quickly radiate and engender loyalty. Indeed, friends and relatives are the third most important source of information for shoppers, and highly satisfied consumers recommended their insurers three times more often than dissatisfied ones do. Similarly, 90 percent of those who trust their insurer would stay with it even if faced with a 10 percent price increase at renewal.
        
        Few consumers recognize how important recommendations are when selecting a product, but they can be powerful determinants of choice. When shopping for health insurance, consumers accept recommendations 58 to 88 percent of the time, depending on the channel.
        
        It’s not surprising that both the frequency with which a recommendation is offered in each channel and the impact varies widely. Recommendations generated by a Web site, for example, are delivered only 53 percent of the time and accepted by consumers only 58 percent of the time when they are delivered. Agents, on the other hand, make recommendations 78 percent of the time, and their recommendations are accepted 88 percent of the time. Given the complicated and emotional nature of the decision and the consumers’ desire for personalized advice, agents’ recommendations continue to be the most persuasive ones.
        
        Although the Web continues to grow in importance as a source of information for consumers—more than half of consumers search online before making a purchase—it does not fare well as a stand-alone channel. Indeed, the Web delivers immediate information, including quotes, but it does not offer a purchase experience that meets the consumers’ emotional needs. Consumers who are considering purchasing health insurance actually make a purchase only 38 percent of the time on the Web, while 65 percent do so when going through in-person channels. Hence, online sales remain low—just 14 percent of consumers report purchasing on the Web. Further, only 1 to 2 percent of consumers purchasing through an agent would have preferred to purchase through another channel, while 9 to 13 percent of consumers who purchased online would have preferred to purchase through another channel—primarily in person.
        
        Improving online sales will require clear recommendations, easier comparisons among options, consumer reviews, and more personalization. Insurers might take cues from Amazon.com, which provides recommendations based on a consumer’s unique searches. The Web provides a wealth of new opportunities for increasing the effectiveness of recommendations, such as endorsements or links to networking sites that discuss health insurance. Nearly a quarter of consumers say they would feel more secure with their purchase if they knew others in their situation had also purchased that plan, and 19 percent reported using social-networking sites to discuss their health-related needs.
        
        The consumers’ health care financing needs vary widely by life stage, but there are distinct segments within a life stage as well. Insurers must gain a more nuanced understanding of consumers, the risks they face, and how their concerns change over time—and then align their products accordingly.
        
        Consumers aged 18 to 29 typically care more about dental and maternity coverage, while those aged 50 to 64 are often more concerned about coverage for major diseases and prescription drugs. A base product across these segments could be a stripped-down plan with salient features, such as catastrophic and preventive coverage, and could then be personalized to accommodate each customer’s unique needs.
        
        A custom-built product design system—where services are added onto a basic plan based on the consumer’s specific needs—would allow insurers to bundle distinct product and service offerings in ways that meet the consumers’ real and perceived risks. Understanding and catering to those needs can enable companies to offer the right additional components to the base product at the moment when consumers are prepared to make a purchase.
        
        Product design should also reflect consumers’ sensitivities to choice and cost trade-offs. For example, 57 percent of people with individual health insurance would be willing to give up some coverage choice in return for a premium discount of $40 a month. But fewer than 42 percent would be willing to accept a narrowing of retail pharmacy choices for a similar discount.
        
        On the other hand, consumers are willing to pay for add-on features they perceive to be of value. For example, 35 percent of individually insured consumers would be willing to pay $10 a month for more time with their physicians. High-income consumers are willing to pay for disease management and remote support. These kinds of segment-tailored packages, which speak directly to consumers’ real and perceived concerns, should be sold as add-ons rather than as the standard features they are on many health plans. Since consumers are less sensitive about the prices of add-ons than with the price of the base product, this approach could have important pricing and margin implications.
        
        More than half of consumers are concerned about routine medical expenses today, and more than 60 percent are concerned about what those expenses will be when they are retired. Yet while health care premiums rank high among household expenses that must be paid, out-of-pocket health care expenses do not. This translates into poor collection rates and rising bad debt within the system—exacerbated by the current economic crisis—and a backlash in the retail market. Health insurers should address this pain point for consumers (and providers) by embedding simple payment solutions within product design.
        
        Our research shows that most consumers are willing and able to pay their health care bills. On a per-visit basis, 83 percent of insured consumers are willing and able to pay less than $500, while 75 percent of insured consumers can absorb an annual expense of less than $1,000—enough to cover the bulk of routine health care. Even 40 percent of the uninsured, when presented with simple financing options, are willing and financially able to absorb annual liabilities of less than $1,000.
        
        Health insurers should address concerns about paying health care bills with creative new products. Half of individually insured consumers, for example, would be willing to prepay their medical bills with credit or debit cards if they got a good-faith estimate in advance, and about half are willing to prepay without restrictions on the amount if they know the cost upfront. By integrating financing and credit options within individual product design—and simplifying the process with unambiguous patient statements—health insurers can capture new opportunities while addressing a major source of anxiety for consumers.
        
        Consumers worry about their health care and the consequences of poor health. Yet concern does not translate into action: only 30 percent of people significantly worried about their health exercise regularly, abstain from smoking, and get routine physicals, and only 35 percent report having improved their health in the previous year.
        
        It is difficult for people to close this gap. For example, consumers tend to favor immediate gratification (eating a doughnut) over future gains (staying fit)—whose benefits can sometimes be uncertain. But in a retail market, inspiring consumers to protect their own health is essential to delivering more affordable products, expanding margins, broadening the pool of attractive market segments, and improving the lifetime value of members.
        
        To transform consumer concern into action, health insurers can take several steps. First, they should intervene at the right time. Consumers tend to assess their situations in relative terms. Nearly two-thirds of consumers who are overweight or have other chronic or high-risk conditions, for example, rank their health as “good” or “excellent.” When their frame of reference changes, however, people become more willing to do something about their health. Immediately after a health event, consumers are significantly more open to addressing future risks and paying for services that address their health concerns. Consumers who have recently experienced a health event are 30 percent more likely to pay for advice and guidance on how to treat their condition, for instance. Behavioral science reveals that this window closes, however, as people become accustomed to their new situations.
        
        Health insurers must engage the consumer while this window is open. By tracking the real-time occurrence of health events, for example, health insurers can better target members for enrollment in health-management programs. Retention is also important: more than three out of four consumers shop for new products upon experiencing a health event, and more than half purchase a new plan. Addressing the consumers’ health concerns at the time of an event could prevent this kind of membership loss.
        
        Second, the way options are presented to consumers can have dramatic impact on their responses. We found that obese consumers, for example, are almost 30 percent more likely to be “extremely concerned” when told that they are “about twice as likely to die next year” (which focuses on the here-and-now implications of their behaviors) than when told that they are “considered obese by national health guidelines.” By appropriately framing risks—and solutions—to consumers, payors can help motivate behavior changes.
        
        Third, in the face of difficult decisions, consumers typically turn to familiar sources of advice. Although consumers seek information from a wider variety of health information sources today, they are still most likely to take their physicians’ advice. For example, 92 percent of respondents directly followed the advice of their primary care physician at the point of care, citing trust as the main reason for not seeking alternative opinions. Yet there may be an untapped demand for additional guidance. Indeed, many consumers are open to other points of influence: 48 percent would be willing to get a second opinion if they received a call from a nurse at their insurance company. Insurers need to recognize the complicated and emotional nature of the choices consumers must make and actively work with trusted points of influence—physicians in particular—to drive healthy behavior.
        
        Health insurers typically provide information and purely financial incentives, such as cash rewards, to help sustain behavior change. But the right incentives targeted at customer biases could motivate consumers to adopt and maintain healthier lifestyles.
        
        Insurers who recognize the motivators of consumer activity can use even low-cost incentives to encourage interest in healthier behavior and leverage them to drive better outcomes. For example, consumers are attracted to probabilistic rewards such as lotteries. When well designed, such incentives can change behavior more effectively than direct cash payments: consumers tend to be overoptimistic about winning, and lotteries offer the right level of variable reinforcement that helps consumers sustain motivation over time.
        
        We recently tested these kinds of behaviorally based incentives using a “regret lottery” design. The goal was to get employees to complete a health risk assessment. Employees were divided into small teams and then enrolled in a weekly lottery. Members of the winning team received a large prize, but only if they had completed the assessment. Winners were widely publicized, playing off of individuals’ anticipated regret at missing their chance of winning the big prize the week their team was selected.
        
        The result was 69 percent completion of health risk assessments in the regret lottery pilot, compared with 43 percent for those that received direct-cash incentives. And, because the expected payout of the lottery condition was equivalent to the cash incentive, the regret lottery was a more effective use of incentive dollars—it would have been significantly more expensive to achieve the same penetration rate using the direct-cash incentive since in the lottery condition only a handful of individuals won each week.
        
        Health insurers should use such insights to change behaviors. Six out of ten high-health-risk consumers, for example, express interest in lotteries—significantly more than direct-cash incentives—as an incentive for engaging in healthy behavior. Similarly, most consumers want to be rewarded for achieving results: about 60 percent of individually insured consumers are interested in discounts for achieving health goals. Many are also willing to accept penalties for failure: 40 percent are interested in products that penalize failure to meet those goals. This suggests that incentive programs such as deposit contracts—where consumers commit themselves to changing their behavior by paying money up front, earn rewards for sticking with the program, and agree to penalties if they don’t comply—can help inspire consumers to change behavior.
        
        For leading health insurers accustomed to competing mainly in an employer environment, gaining deeper insights into the needs and biases embedded in the typical consumer’s behavior will be essential for creating and distributing effective products, earning the consumers’ trust, providing a more satisfying shopping experience, and, ultimately, helping consumers better manage their health. In short, the successful insurers will be those whose business systems can deliver effectively the peace of mind health care consumers desire.The McKinsey Quarterly originally published this article in June 2009. The article explores ways to refine healthcare financing and reimbursement mechanisms in the United States to make them more appropriate for different categories of medical spending—from preventive care to catastrophic and end­-of­-life care. As the United States embarks on further changes to its health system, our original article has gained new resonance.
        
        Since its original publication, several dynamics of our healthcare ecosystem have changed. First, medical expenditures have risen further, and the proportion of expenditures going to various categories of medical spending has shifted (Exhibit 1).1 For example, expenses related to chronic conditions have increased, a result of growth in spending on such disorders as diabetes, heart disease, arthritis, some cancers, and asthma.2 In 2007, care for chronic conditions (both routine care and catastrophic care required because of disease progression) accounted for 32% of US healthcare expenditures ($594 billion); by 2012, that number had grown to 34% ($802 billion). The proportion of total ex­penditures related to elective procedures rose to 15%, from 13%. Although spending for catastrophic care not related to chronic conditions increased in absolute terms, the proportion of total expenditures related to this category fell to 28%, from 31%.
        
        Exhibit 1 Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        These shifts make it increasingly important that we develop financing and reimburse­ment mechanisms that incentivize appro­priate care for chronic conditions, as well as healthy behaviors and value-­conscious use of care among consumers.
        
        Second, since the original article was published, the healthcare system has attempted to better align incentives in provider reimbursement. Both public and private sector actors have made important innovations in this area. For example, in 2009, we recommended that reimbursements should be tied to long­-term health management rather than the volume of services provided. In the past several years, there has been meaningful movement toward value-­based payment models, such as accountable care organizations (ACOs) and patient­-centered medical homes, that aim to restructure provider reimbursements to incentivize care coordination and reward providers for overall management of patients’ health. In 2015, nearly 500 patient­-centered medical home programs were operating in the United States.3 At the end of the same year, over 23.2 million people were receiving care through ACOs.4 Furthermore, most of the ACOs bear at least some financial risk for patient out­comes and cost of care, though provider performance under these programs has been mixed.5 There has also been significant innovation in the use of episode­-based payments. Under these arrangements, providers are evaluated and rewarded based on the quality and cost of care that they provide for an entire episode of care (e.g., an upper respiratory infection or a joint replacement). The Bundled Payments for Care Improvement program, launched in 2013, now includes 1,361 provider partici­pants and 48 distinct clinical episodes.6 In addition, several state Medicaid programs have implemented episodes of care or bundled payments. For example, by spring of 2017, the State of Tennessee will have launched 34 episodes of care, and additional episodes are under design.7
        
        Third, we have seen continued growth in the use of high­-deductible health plans and health savings accounts, which seek to align demand­-side incentives around healthcare use. Cost sharing has been shown to be effective in curbing healthcare utilization (see our recent white paper, “The next imperatives for US healthcare”1).
        
        Despite these developments, available fund­ing mechanisms continue to be inadequate in addressing the true nature of medical risk in many areas. There has not been much progress in ensuring that consumers have appropriate incentives to encourage self­-care and appropriate use of resources—for example, through value-­based insurance design, wellness incentives, and smart design of essential health benefits. High deductibles and copayments are blunt instruments that have the potential to dampen needed as well as unnecessary utilization, and thus could inadvertently increase long­-term expenditures—for example, if these tools discourage patients from using appropriate healthcare services to manage a chronic condition, costly complications could ensue. Finally, a true consumer/retail market for healthcare has been relatively slow to develop, given the pervasive intermediation for routine, purely elective, and discretionary services.
        
        The fundamental nature of medical risk in the United States has changed over the past 20 to 30 years—shifting away from random, infrequent, and catastrophic events driven by accidents, genetic predisposition, or contagious disease and toward behavior­- and lifestyle-­induced chronic conditions. Treating them, and the serious medical events they commonly induce, now costs more than treating the more random, catastrophic events that health insurance was originally designed to cover (see Exhibit 1 above). What’s more, the number of people afflicted by chronic conditions continues to grow at an alarming rate.8
        
        As the nature of medical risk has evolved, neither the funding mechanisms nor the forms of reimbursement for healthcare have adapted adequately, and so the system’s supply and demand sides are both hugely distorted. Consumers are over­-insured against some risks and under­-insured against others; woefully short of the savings required to pay predictable, controllable expenses; and all too likely to be dealing with doctors who have financial incentives to treat isolated problems rather than prevent illness and manage chronic conditions effectively.
        
        These are important—yet frequently over­looked—points in the current debate about the future of healthcare in the United States. With the US government poised to spend billions of dollars to support universal ac­cess, reformers must understand this shift in the nature of risk and move to align financing mechanisms and reim­bursement with it. Pouring more money into the system without modernizing it will probably worsen the healthcare challenges facing the country.
        
        Ideally, consumers should be able to buy enough coverage to feel financially secure but also share in the cost of care. In addition, coverage should be structured to give consumers incentives to manage the risks under their personal control in a value­-conscious way. Just as important, the United States needs to have the reimbursement and care delivery models that best control each type of risk.
        
        To better inform the debate on the healthcare system, we offer a new way to look at the distribution of costs within it. We break down the country’s healthcare spending into separate risk categories, map them to specific medical conditions by their unique characteristics, and identify who pays for what (see the sidebar “About the re­search”).
        
        Because insurance is the dominant financing mechanism and fee for service is the primary way of reimbursing providers, the US healthcare system is misaligned in two respects. First, with consumers over­-insured for some risks and lacking adequate protection for others, the system does not offer incentives for healthy behavior, promote value-­conscious consumption, or provide adequate financial security. Second, in a fee-­for­-service world, providers have a financial incentive to undertake as many procedures as possible—a model especially ill-­equipped to manage increasingly prevalent chronic conditions.
        
        This misalignment is a relatively recent phenomenon. Insurance is effective if it pools random, infrequent, and unpredictable risks. When health insurance was introduced, in the 1930s, it did precisely this. Over the decades, however, it expanded to cover an increasing array of services, largely because employers wanted to attract workers by providing a tax­-advantaged benefit.
        
        In the 1980s and early 1990s, managed care promoted this trend by offering consumers “first­-dollar coverage,” reimbursing for routine services and expenses related to conditions that weren’t random, infrequent, and catastro­phic in exchange for the patients’ willingness to cede decision rights on treatment choices to primary care physicians. When managed care lost popularity, consumers regained choice but largely retained first­-dollar coverage.
        
        The more recent shift requiring consumers to share more of the cost has sought to correct this imbalance through products such as high-­deductible health plans combined with health savings accounts. Some of the cost shifting, though, has not been sufficiently nuanced and left many consumers under­-insured and financially exposed in certain risk categories. Requiring consumers to bear over 10% of the cost of treating a catastrophic event, for example, exposes many people to financial hardships, given that the expense involved could be tens of thousands of dollars. The current approach also does little to promote value-­conscious consumption—after all, people have only a limited ability to avoid accidents and can hardly shop for medical care when they happen (Exhibit 2). Furthermore, the fact that consumers cover almost 30% of the cost of preventive care conflicts with the goal of maximizing its use.
        
        ​ Exhibit 2 Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        As we have seen, the system also suffers from misaligned supply­-side incentives, given the predominance of fee-­for-­service reimbursements to providers. Prices are set through long­-term contracts between providers and government agencies or private insurers, so their primary financial incentive is to increase the volume of profit­able services, such as imaging. Current incentives, moreover, fail to encourage the desired outcomes across categories of risk; for example, insurers are mainly responsible for financing delivery risk—the cost and quality outcomes of care. This approach leads to the overuse of healthcare services, since consumers have little incen­tive to curtail their use of the system, while providers have a strong incentive to increase their volume of services.
        
        These issues are particularly vexing for chronic conditions because the fee­-for­-service reimbursement model is fundamen­tally misaligned with the need to manage long­-term health outcomes. That kind of management is essential to reduce the incidence of expensive catastrophic events arising from the complications of chronic diseases (amputations, for example, as a result of unmanaged diabetes), but the reimbursement system does little to encour­age it. In fact, under the current system, with few exceptions, providers earn more revenue when catastrophic events occur. More troubling still, the fee-­for-­service model tends to fragment the provision of care into scores of unrelated interventions. Yet the effective management of chronic disease calls for integrated, coordinated care among many different types of physicians and between them and medical institutions.
        
        The underlying goal of reform should be to align risks—both risk exposure (lifestyle choices inducing chronic conditions) and expenses incurred (treatment choices affecting costs and outcomes)—with the parties best equipped to control them. To achieve this goal, it will be necessary to determine the most appropriate financing mechanisms and provider reimbursement models for each healthcare risk category; one­-size-­fits-­all approaches are counterproductive in an increasingly complex healthcare world. For some risks, it will be appropriate to use sophis­ticated reimbursement methods: bundled payments for episodes of care, capitation (a fixed payment per year per member), or risk-­sharing arrangements. In many cases, however, relatively simple fee­-for­-service payments will remain the model of choice.
        
        Most US households can afford relatively frequent fee-­for-­service medical episodes such as a visit to a physician to treat a fever or to a pediatrician to treat a toddler’s ear infection. The most efficient way to pay for such services is not insurance but rather savings. (The indigent ought to receive subsidies.) The reimbursement model for these services should resemble that of any other consumer service—providers make value-­based sales to consumers who pay them directly. As in the case of other services, each consumer segment will value features such as convenience, speed, and quality differently, so providers have oppor­tunities to differentiate themselves. One such innovation, consumer­-oriented retail clinics, provides a clear value proposition by offering convenient locations, limited waiting times, and transparent, fixed, and relatively low prices.
        
        There is also little financial need for insurance to cover preventive-­care services, such as vaccinations and screenings (like mammograms) to detect high-­risk conditions early, since they too offer substantial benefits at relatively affordable prices. These services, however, are essential to maintain the medical health of society and to control the cost of treating illnesses in the future. As a matter of good public policy, this type of care should therefore be available as widely as possible, at little or no charge, to ensure the greatest possible access. General public health spending by the government could finance such services, or they could be a required part of the coverage of every health insurance product. Fee-­for-­service reimbursement is simple and effective here.
        
        The largest, fastest-­growing healthcare risks are chronic conditions and catastrophic events attributable to them, such as angioplasty or bypass operations for heart disease and below-­the­-knee amputations for peripheral vascular disease. Addressing this type of medical risk arguably requires the biggest changes in the current system. New financing mechanisms are needed to manage such conditions cost­-effectively over long periods of time by financing investments in wellness and care management today so that costs fall tomorrow. These mechanisms must give consumers incentives based on behavioral-economic principles that promote healthy behavior and value­-conscious consumption of care. Finally, it will be important to give the providers incentives compatible with the need to manage health outcomes across the whole population of chronic patients and to provide multidisciplinary, coordinated care throughout the delivery system.
        
        Devise longer-­duration, portable financing mechanisms. Once you have a chronic condition, the cost of managing it is fairly predictable—this isn’t an insurable expense, which ought to be random, infrequent, and unpredictable. Further, in effective treatments for chronic conditions, true value accrues over time by precluding their progression and, especially, the catastrophic events related to them.
        
        To encourage investments in wellness, pre­vention, and disease management, health insurers or integrated healthcare providers must embrace long­term “ownership” of the patient—something akin to life insurance, which offers coverage that often stretches over many years or even an entire lifetime. Three broad types of financing mechanisms could be effective: multi­-year term policies, annuities (pay a lump sum today for a contract covering chronic-­care expenses permanently or for a fixed period), or self-­insurance (pay out of savings or income).
        
        Private payors in other countries have intro­duced health insurance products based on actuarial concepts similar to those used in life insurance. Some German payors, for example, offer lifetime coverage products. Under these arrangements, younger customers pay premi­ums higher than their risk level would typically command; at older ages, the accumulated surplus is used to reduce premiums.9
        
        Since the consumer controls much of the risk associated with chronic conditions through behavioral choices, the financing mechanisms should include incentives to address the emotional and behavioral biases that stand in the way of rational lifestyle and healthcare choices. Just shifting costsis ineffective, since it often fails to differentiate between unnecessary and sensible (preven­tive services) utilization. But rewards and penalties based on insights from behavioral economics and other behavioral sciences can work well.10
        
        About the research The fundamental nature of medical risk in the United States has changed over the past few decades. In most cases, medical risk no longer results from random, infrequent events driven by accidents, genetic predisposition, or con­tagious disease but from chronic conditions related to behavioral, environmental, or other factors. Treating chronic conditions, and the serious medical events they commonly induce, now costs more than treating the random, catastrophic events that health insurance was originally designed to cover. Although our country’s approach to health insurance—and to paying for healthcare more generally—is changing, it has still not sufficiently adapted to the change in medical risk. As a consequence, consumers still have little incentive to forego unnecessary, inex­pensive services yet are ill protected from the cost of very expensive care. The incentives for providers are only starting to change to encourage them to deliver preventive services and discourage them from offering unneces­sary or poor­-quality care. Medical risk is not uniform, however. We analyzed US healthcare spending and broke it down into separate risk categories, each of which has unique characteristics.1 We then matched the incentives offered to con­sumers and providers to the characteristics of each category. How we did the analysis
        
        Our analysis looked at total annual US health­care spending (excluding government admin­istrative expenses, private insurers’ profits, research expenses, and the cost of equipment, software, and public health activities). We eval­uated expenditures using four major factors: Severity. The magnitude of the medical expense to treat a specific condition. Frequency. How often the condition occurs. Level of consumer discretion. The degree to which consumers can control costs. Temporal dependency. The amount of time a patient is likely to be afflicted with the condition. We then considered a number of other issues. For example, we reviewed evidence-­based guidelines and evaluated the inherent value of preventive medicine. In addition, we investi­gated the primary mechanisms used to pay for services delivered: Out-of-pocket. Expenses paid by consu­mers other than insurance premium payments (e.g., copays, coinsurance, and deductibles). Insurance. Expenses covered by individual insurance, government insurance, and em­ployer­-sponsored insurance (including the employee portion of premiums). Subsidies. Expenses covered by federal and state subsidy programs (e.g., Medicaid and the State Children’s Health Insurance Program), as well as charity care. What we found
        
        The analysis yielded the eight categories of medical risk shown in Exhibit 1. When we looked at how each of these categories was primarily paid for, we discovered there was often a disconnect between the value the services provided and where the funding came from. For example, insurance often covered a greater proportion of the costs of discretionary care than of preventive care. Similarly, we found a disconnect between the share of costs consumers were expected to pay and their ability to influence the need for that care. (Consumers were often respon­sible for more of the cost of uncontrollable catastrophic events than of catastrophic events related to chronic disease.) And we saw little or no relationship between the amount con­sumers were expected to pay in each category and their ability to absorb those costs. Our findings led us to believe that a one­-size­-fits-­all approach to either consumer cost sharing or payment innovation will not be effec­tive in controlling healthcare costs or improving care quality. Only by matching the extent of cost sharing and the primary reimbursement mechanism to the characteristics of each category of medical risk will it be possible to achieve those goals. Admittedly, the approach outlined here is some­what simplified. Patients are not homogenous, and what is an appropriate treatment for one patient may be discretionary or even inappro­priate for another. Thus, models designed to encourage high­-value care and discourage low­-value care through variable cost sharing must be more nuanced to take these differences into account. Payors should rely on clinical evidence when developing smart cost sharing models to move beyond blunt instruments such as high deductibles and uniform copayments or coinsurance rates. And they should re-­examine the models periodically to minimize the risk that either patients or providers can game the results. 1. Singhal S, Pellathy T, Adigozel O. Why understanding medical risk is the key to US health reform. McKinsey Quarterly. June 2009.
        
        Design reimbursements tied to long­-term health management. Reimbursements to providers should be based on long­-term health-management outcomes rather than the fee-­for-­service model. A sensible system could involve capitation or risk sharing, with outcome-­oriented payments reflecting how well a provider manages a condition. The effective management of chronic dis­ease and multiple disorders often requires collaboration among specialists from many medical disciplines, so the reimbursement structure should reinforce coordination of care. Experiments with patient­-centered medical homes—a form of integrated care management—may well show how to manage the risks of chronic conditions.
        
        Today, insurance rarely covers truly elective spending (such as cosmetic surgery, alter­native medicine, or LASIK eye surgery), which the consumer pays for out-­of-­pocket, often using credit. This part of the healthcare marketplace actually works well: elective treatments, as a classic consumer retail item, are available to those willing to assume the full burden of paying for them. In addi­tion, all services not medically justified by evidence-­based standards—for example, certain types of joint surgery if studies show that a lower-cost drug treatment is equally or more effective—should be paid for out­-of­-pocket by the consumer. Some evidence suggests that a robust consumer market for elective procedures, coupled with transparent pricing, has driven down prices for elective procedures in the United States.1
        
        Unpredictable, random, and infrequent risks (heart attacks in previously healthy patients, for example, and interventions for accidents) should be financed through traditional insurance. In such cases, consumers have limited discretion and little ability to exert downward pressure on prices—few victims of auto accidents, for example, can shop for a cost-­effective ambulance service and make well­-informed cost–benefit calculations about treatments. Deductibles on this type of insurance should therefore be kept low; costs are best managed by redesigning reimbursements for providers.
        
        A provider should be compensated in one bundled payment based on the total episode of treatment, from the moment the health crisis starts until full recovery, rather than on a fee-­for-­service basis. Such bundled reimburse­ments would give providers an incentive to improve their efficiency. They would also find it in their interest to restrain costs in a rea­sonable way—for example, by providing cost­-effective services (the correct type of hip joint, say) and high-­quality treatment the first time around rather than having to readmit patients for costly corrections after botched initial interventions. Including specialists and hospitals in this total­-episode­-based reim­bursement system will be essential.
        
        Riders on life insurance policies might be the best way to finance end-­of-­life care—say, for an elderly patient with a known terminal illness—which is generally quite expensive. The insured could decide how much of their benefits to draw down at this stage rather than bequeath them to the benefi­ciaries. Fee-­for-­service reimbursement for providers would probably be appropriate, since it is hard to apply outcome measure­ments or evidence-­based standards to many of these treatments (for instance, experimental ones).
        
        As reform efforts move forward, the guiding principle should be to redesign the demand side (financing mechanisms for consumers) and the supply side (reimbursements and the delivery system) to align medical risks—and the attendant financial incentives—with those who can most effectively control and manage them. Continuing reform initiatives provide a great opportunity to restrain costs, deliver more cost-­effective care, and ease the financial and psychological burden on hard­-pressed US consumers. It can be undertaken fairly, we believe, if the govern­ment helps people in difficult financial straits pay for their care.What are the prospects for an overhaul of the US health care payments system? The recent passage of comprehensive health insurance legislation only adds to the pressure for transforming the system that manages medical bills, claims, and payments. We foresee big changes in coming years, with billions of dollars of value at stake.
        
        The June 2007 McKinsey Quarterly article “Overhauling the US health care payment system” argued that the greater “electronification” of health care transactions, the growing adoption of standards, and increasing innovation by cross-industry entrants would lead to a major restructuring of the US health care payments value chain. Two and half years later, we are still waiting for that massive overhaul. But we believe that major change in the payments landscape is inevitable because of fundamental industry dynamics, such as the proliferation and increasing complexity of health care transactions, the increasingly prominent role of the consumer in payments, and the rising importance of medical and financial risk management for providers. And the pace of change will only accelerate with the rolling out of the new health care law, as more individuals become insured and begin to generate more health-related transactions and industry participants face greater pressure to reduce administrative costs.
        
        We acknowledge that there has been progress in improving the health care payment system. There has been a steady conversion to electronic-data formats, thanks to the adoption of standards across different transaction types compliant with the HIPAA (for example, claims submissions, eligibility checks, and remittance advice), along with the more widespread use of electronic formats and transaction-processing clearinghouses. We estimate that by 2012, about 80 percent of the projected eight billion core US health care transactions will be in electronic formats, excluding lab and pharmacy, which are already largely electronic.
        
        Also, we have observed movement toward developing technical solutions to health care payments problems. There are numerous innovative approaches on the market aimed at improving the transparency and efficiency of payments, with companies offering products such as online bill-paying solutions, patient liability–estimation tools, point-of-sale consumer payments processing, and structured financing solutions. In addition, large health care IT players (for example, GE and McKesson) and a range of financial institutions (JPMorgan Chase and PNC Bank, for instance) continue to make significant investments in health care payments processing, while large payers and providers explore ways to partner to solve payments issues.
        
        The transition to electronic formats, as well as technological innovation, has laid the groundwork for the more fundamental restructuring of health care payments outlined in the 2007 article. But there is still much work to be done: the system remains highly fragmented and inefficient, consuming a disproportionate share of dollars compared with payments systems in other industries. Unlike scale utility approaches that have emerged in financial services or telecommunications, innovative solutions in health care have failed to take hold at scale—either because of misaligned incentives among stakeholders or because few players have the local-market position to drive adoption across a fragmented provider community. And consumer bad debt continues to rise, resulting in more than $65 billion in uncollected revenues in 2010, according to our latest estimates, putting enormous strain on provider economics. We will see more progress in coming years as industry participants address three major challenges.
        
        The volume and complexity of health care transactions is rapidly expanding. We estimate that the number of HIPAA-compliant transaction sets will grow at a compound annual growth rate (CAGR) of about 8 percent in coming years. In addition, new regulations, such as 5010 and ICD-10 (aimed at creating better-defined data standards), are adding more complexity and forcing the expenditure of hundreds of millions of dollars in investments.
        
        We also foresee an explosion of digitized, stored, and transferable clinical data. Today, less than 20 percent of clinical data is electronic, with little standardization across data fields. We see a rapid shift toward the greater use of electronic formats and standardization, in large part spurred by the electronic-health-record requirements in the American Recovery and Reinvestment Act of 2009.
        
        The complexity of clinical data should not be underestimated—a typical patient-level clinical data set can include more than 800 discrete fields, compared with only about 20 to 30 for a financial transaction. Digitizing, standardizing, and normalizing this data so that they can be used for operational and clinical decision making will require large capital investments and create ongoing operating costs. Few health care industry players have the scale or sophistication to manage these issues on their own.
        
        Resulting in part from this systemwide complexity, industry administrative costs will grow by about 10 percent annually over the coming years—higher than the rate of growth of medical inflation. Tackling these costs will require private-sector action. The US Congressional Budget Office (CBO) has only scored about $3 billion in administrative cost savings by 2014 from the Senate and House versions of the recently passed health care law. This amount will cover only a fraction of total industry administrative costs.
        
        The pressure to manage costs should increase the willingness of industry participants to work together and to try new approaches. Cross-industry collaboration could finally spur the creation of payment utilities such as full-cycle-payment automation (described in our 2007 article). As noted there, we believe in the potential for cross-industry collaboration to create an at-scale payment-settlement utility that knits together health care transaction processing through clearinghouses, the automated clearinghouse payment network, and card network payments for retail payments.
        
        According to our modeling of the 2010 flow of health care funds, consumers now pay more in health care costs than do employers. This cost is split among direct payment of noncovered services, out-of-pocket expenses after insurance, and the consumers’ share of premium expenses. Uncompensated care from 46 million uninsured Americans continues to be an issue, but one that will be mitigated by reform. In fact, the fastest-growing portion of bad debt stems from what insured patients fail to pay after insurers have paid their portion of medical bills. This category will likely grow as more insured patients enter the market following passage of the new health care law. For example, at one multifacility hospital system, we found that, for insured patients, “balance after insurance” is growing at 30 percent a year; for patients without insurance (those who pay for services from their own pockets), that figure is only 19 percent.
        
        This trend will place balance-after-insurance collection issues front and center in the health care payments landscape. Physician practices and hospitals alike could do better through innovative patient financing, better front-end retail-revenue-cycle approaches, and consumer-friendly collections. In particular, there will be greater pressure on physicians regarding balance-after-insurance collections as reform accelerates the shift from in-patient to out-patient and from hospitals to physician- or clinic-based settings. Many of these smaller practices are poorly equipped to deal with the resulting retail-revenue-cycle challenges given current mind-sets, workflow practices, and capabilities.
        
        The need to deal with these issues opens the door for greater penetration of point-of-service, retail-oriented payments approaches. For the physician segment, how these solutions are provided, in particular, is what matters—solutions with bundled financing options or ASP models that are integrated into the physician’s workflow and require little capital investment will probably see the most uptake.
        
        Sidebar Modifying consumer behavior As consumers take on more of the risk associated with health care, the traditional relationship among consumers, providers, and payers is changing. With persistent medical inflation, employers continue to promote greater employee cost sharing to reduce their health care spending. Between direct payments to providers from both self-pay consumers and the insured (and their share of insurance premiums), individuals play a major role in the flow of health care funds. And the recent health care insurance law will only increase the role of individuals, as many previously uninsured people enroll in insurance plans, actively utilize health care services, and become responsible for balance-after-insurance payments. The shift to a more consumer-oriented system poses challenges for providers as well. Few of them (with notable exceptions, such as dental practices) are able to estimate a patient’s out-of-pocket expenses, present a bill at the point of service, and collect payment then and there. Instead, providers send a bill, often weeks after the event, and hope the patient pays. To complicate matters, patients typically receive an “explanation of benefits” statement from their insurer—not a bill, but an estimate of their liability after adjudication, which can be more confusing than helpful, since its timing and content are seldom coordinated with the provider’s bill. Moreover, there are few deterrents to nonpayment. Providers are typically reluctant to pursue patients aggressively for fear of reputational risk. Although medical expenses sent to collections do show up on credit reports, many lenders discount them. As a result, provider collection rates run at 50 to 70 percent for small-dollar liabilities for insured patients and fall to about 10 percent for self-pay patients. Uncollected revenues represent 4 to 6 percent of hospital gross revenues. We estimate that in 2010, bad debt will reach some $65 billion. The prevailing assumption is that consumers are unable or unwilling to pay their health care bills. Our consumer research suggests otherwise. It reveals that the lack of financing options, inefficiencies in billing practices, and consumer confusion are all major drivers of nonpayment (Exhibit 1). In fact, our analysis suggests that the vast majority (more than 74 percent) of insured consumers are both able and willing to pay their out-of-pocket medical expenses for annual liabilities of less than $1,000 a year (Exhibit 2). Indeed, more than 90 percent are willing and able to pay if these liabilities are less than $500. Yet collection rates lag well behind these levels, even for relatively low-ticket payments. Exhibit 1 Why don’t they pay? A lack of options and confusion over what’s owed underlie nonpayment of health care bills. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com Exhibit 2 Willing and able Most consumers are willing and able to pay their medical expenses. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com For annual member liabilities greater than $1,000 a year, the “willing and able to pay” segment drops to 62 percent. The pressure around health care expenses that consumers face will only worsen as rising deductibles and out-of-pocket expenses—as well as the explosion in chronic conditions requiring life-long health care—place heavier burdens on household finances. The industry thus faces a stark choice: improve retail-payment capabilities to help consumers manage their health care financing better or risk having rising costs overwhelm consumers’ current willingness and ability to pay. Our research suggests that the industry can address a significant portion of the bad debt in the system through approaches that not only make payments more convenient, less confusing, and easier to distribute over time through financing but also reposition medical bill payments in the household payment hierarchy. New payment solutions must tackle consumers’ confusion and concerns head on. Automated payments, patient statements (instead of bills and explanations of benefits), structured payments plans or lines of credit, and even incentives and reward points based on principles from behavioral economics should all be considered in building a value proposition that consumers will readily adopt. Such innovative payments approaches could create nearly $60 billion a year in value as well as achieve substantial savings in the administrative costs associated with inefficient processing and collections. A model will create value only if it succeeds in inducing more consumers to pay more of their medical bills than they currently do. It must therefore garner adoption beyond those who already pay their bills and who might self-select into a more convenient payment mechanism. It must also reposition health care expenses at the top of the household payment hierarchy, since these expenses typically fall at the bottom. For example, health insurance premiums follow mortgage or rent and utilities as the third most important expense for most households. Medical expenses, on the other hand, rank seventh (after cell phone and Internet bills), with only 7 percent of households rating medical expenses as a top priority within their household budgets. The payments hierarchy reveals that partnerships between insurers and providers could help improve collections, given the much higher importance of paying premiums. Such collaboration could take the form of integrating balance-after-insurance billing with the explanation of benefits or health statement (online or paper), with or without credit risk-taking by the insurer. Further, we estimate that if currently insured consumers had access to more convenient payment mechanisms and structured payment or financing options to help them smooth spiky medical expenses into tight household budgets, only 10 percent of their bad debt would remain uncollectable. This approach could include card-based solutions. Our research suggests that 52 percent of consumers would be willing to use a credit or debit card in a health care transaction if a good-faith estimate of up-front costs were provided, at payment levels that cover the out-of-pocket costs of a typical physician visit or out-patient procedure (Exhibit 3). Exhibit 3 Charge it, please More than half of the respondents were willing to pay health care bills by using credit or debit cards. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        In addition to transactional efficiency, industry dynamics will likely drive the need for innovative financing methods that better meet the requirements of health care consumers. There is little doubt that millions of households will continue to struggle with managing health care costs, despite consumers’ willingness and ability to pay their bills (see sidebar, “Modifying consumer behavior”). Coming up with compelling consumer payment solutions will require financial intermediaries to step in and provide financing and structured-payment options.
        
        As the health care industry moves toward outcome-based, bundled payments, providers must increasingly leverage payment and clinical insights to better understand and manage medical risk. Most providers remain woefully ignorant of the economics relating to patient segments, service lines, geographies, and payers. To understand profitability at a detailed level in an outcome-focused world, providers must have access to and analyze normalized clinical, claims, and payments data.
        
        To get there, several things must happen. First, electronic-health-record adoption and data standardization must continue. By applying CBO data, we estimate that 55 percent of hospitals and 85 percent of physician practices will reach the basic stages of meaningful use by 2014.
        
        Second, the health information exchange (HIE) infrastructure must expand to provide connectivity. There are already 150 to 200 HIEs across the country, supported by a broad ecosystem of technology players, ranging from large-scale providers, such as Cerner, IBM, and Misys; to smaller ones, such as dbMotion and Medicity; to publicly backed approaches, such as the open-source software advanced by the Nationwide Health Information Network (NHIN). Larger hospital systems are also increasingly building out private HIEs that can help them better integrate care and manage their referral network, in some cases in collaboration with payers.
        
        Finally, relevant clinical data will need to be integrated with claims information, charge data, and remittance information in a way that enables analytics on issues such as cost management, physician management, reimbursement optimization, and service line profitability. As with other transaction-processing and analytic capabilities in health care, developing these solutions will likely require innovative cross-industry collaborations involving some combination of intermediaries and infrastructure providers (payers, IT vendors, and financial institutions) and analytics service providers.
        
        Over time, it should be possible to automate the full cycle of information and payment flows in health care, from the submission of claims to the receipt of payments and reconciliation. The development of an automated payment network would reduce bad debt, cut administrative costs, and save billions of dollars. It would also create the infrastructure needed to sustain the health care payments system in a more retail-oriented world. Beyond such a network, the integration of payments with clinical data (such as claims histories and pharmacy records) promises to provide the infrastructure and analytical basis for the next generation of innovation in provider incentives and payments, such as evidence-based “pay for value” reimbursement models. The momentum behind this transition is growing as the rising incidence of diabetes, obesity, and other chronic conditions requiring long-term care changes the underlying nature of health care financing risk. The need for a system that helps players to address these issues—and consumers to manage the financial aspects of their health care—is becoming ever more pressing. The incremental progress we have seen over the past few years is indicative of much greater change ahead.As the insurance industry emerges from recession, it faces a sea of challenges. The economic slowdown has intensified price competition, hitting margins at a time when market turmoil has depressed revenue streams from many insurance holdings. Similar difficulties in adjacent financial sectors have brought new competitors—for instance, joint ventures between banks and financial advisers—into the insurers’ traditional terrain. Structural changes continue to shift global revenue pools to emerging markets, while customer behavior is shifting as more transactions move online.
        
        In this environment, the industry must not only focus its strategic attention on areas from better financial and risk-pool management to M&A but also develop innovative, growth-oriented products that can secure the loyalty of existing customers and attract new ones. Yet in the face of this challenge, insurers may be neglecting an important tool: technology. Across a number of industries, rapidly changing technologies have been changing the postrecession competitive dynamic. Web and communications technologies are spurring ways of creating products and reaching customers, as well as opening doors to more efficient and effective ways of delivering products and services. They are also giving rise to entirely new business models.
        
        For two reasons, some insurers may find crafting a new approach to technology difficult. One is that they often see IT primarily as a cost center prone to overruns and a megaproject mentality; the industry spends 25 percent of its operating budget on IT, and executives often lament poor returns on the investment. Second, the industry is built on high levels of trust in product offerings and often on personal relationships between company representatives and customers. As a result, insurers fear to experiment with new technologies that could damage these fundamentals.
        
        With costs and competition rising and growth facing limits, this is a good time for insurers to reexamine their IT options. Rapidly evolving technologies will probably change the industry’s competitive patterns. Forward-looking companies are already taking steps to gain first-mover advantages through the intensive use of technology, which could facilitate new types of interactions with customers and company agents. In a growing number of areas, for example, IT may allow insurers to automate processes and cut costs without damaging service delivery. Customized products that rely on data and better risk analysis could provide new avenues for growth-boosting innovations. Finally, the combined effects of technology could change the nature of insured risk, leading to market disruptions that insurers will need to understand.
        
        None of this is to say that the industry should pursue technology at the expense of core strengths, like empathy, service, and trust. Technology cannot replace the trust-based relationship between insurers and their clients but it can enhance it—offering better ways to understand and satisfy customer needs. We believe that business and IT leaders together should begin examining technology at a level that matches their strategic goals and appetite for change.
        
        In four areas we have identified, technology could improve the operational performance of insurers, bolster their growth prospects, and perhaps even change current business models. The industry players in the following examples are technology leaders. Most insurers still have a sizable window of time to assess which technology path makes sense for them.
        
        Interactions with customers often rely on personal contacts and face-to-face meetings. But most consumers—indeed, most participants along the industry’s value chain—are experienced users of the Web, “smart” devices, and social networks. Other industries, notably airlines, retailing, and telecommunications, have swiftly adopted such technologies to multiply internal and external interactions. Today, some insurers are exploring ways to link customers, agents, brokers, and back offices through these new channels.
        
        Channel choice. For several years, insurers have been experimenting with new delivery channels for insurance purchases. South African Metropolitan Life, for example, has introduced a pilot life insurance plan named Cover2Go, which allows new customers to contract for short-term life insurance simply by sending an SMS. Similar delivery modes are being explored in Asia, where the clients of AXA Thailand, for instance, can renew their motor policies via text messaging—and are encouraged to do so by a small gift as an incentive. Although these channel choices might not be applicable to all situations in all regions or policy types, they demonstrate the possibilities opened up by new technologies.
        
        Convenient, cheap, ad hoc insurance. We know of one insurer that is drafting plans for low-cost, one-day ski insurance policies. The purchase process would be extremely simple: via cell phones, customers would send an instant message with a photo of an ad to a number displayed on it. Policy charges would then be added to their phone bills.
        
        Information sharing. In a Facebook training initiative, 17,000 State Farm agents have created groups of “friends” to discuss new products or trade experiences about customer service and ways to handle claims more effectively. German insurer Generali Deutschland is using a social network to share information among its brokers.
        
        Partner networks. The vehicle repair company Motorcare has created a wide-ranging digital network that connects insurers to its 800 garages in Germany. It has processed some 140,000 claims and reduced repair costs by up to 20 percent.
        
        Insurance products most often come in standard packages with only a limited range of consumer options, such as the dollar amount of coverage and a few other basics—for instance, auto insurance that covers theft and ancillary property damage as well as liability. Customers, meanwhile, are slotted into fixed risk categories.
        
        Some insurers are now using their huge volumes of data on the needs, preferences, incomes, and lifestyles of consumers to roll out “mass-customized” policies, which combine many predefined options that adapt products to the needs of individuals. (Automobile manufacturers have done so successfully for some time.) The advantages are clear: customers get a bespoke product, while insurers apply the logic of mass production to lower their costs. One large insurer we know of plans to build continental-scale back-office factories with IT systems based on algorithms (also known as “rules engines”) that can adapt policies both to customers’ preferences and to specific market regulations. The idea is to combine the cost benefits of extreme scale with the flexibility needed to accommodate a wide range of products and services.
        
        On another front, auto insurers are implementing dynamic coverage based on driving patterns and behavior. One leading carrier in the United States, for example, uses a tracking device to monitor drivers and applies discounts to policies as a reward for safe driving. Ultimately, sensors installed in customers’ vehicles could track other kinds of higher-risk activities (for instance, parking in high-theft areas). As technologies evolve, some of these features, delivered by GPS in mobile devices, may soon be commonplace. Similarly, health insurers are studying ways to track their customers’ activities, such as diet and exercise, to fashion insurance packages that reward healthy behavior with lower rates.
        
        Elsewhere, insurers are encouraging customers to help cocreate new products. Allstate Insurance, for instance, has set up social-network forums to facilitate interactions among motorcycle customers and enthusiasts. It solicits suggestions and uses them to inspire new products and services.
        
        The industry relies on labor-intensive, often paper-based processes for issuing and administering policies and for managing claims. IT can improve productivity by reducing the number of handoffs among agents, adjustors, and payment staff and by automating straightforward payment decisions through the use of expert software. New claims-management systems, for example, can automatically divide claims into clusters based on their complexity, their estimated value, and the risk of fraud. Some claims are paid automatically, others assigned to the appropriate adjustor. This approach improves cycle times, increases customer satisfaction, and reduces revenue leakage from fraud.
        
        One large auto insurer has launched a prototype insurance project that combines a wide range of technologies. Deceleration sensors and GPS devices in vehicles can detect a collision, identify its location, and automatically notify the insurer. A tow truck is dispatched to the accident scene while additional sensor data help evaluate the damage remotely. Software programs determine the payments and automatically dispatch them, bypassing a range of adjustors and clerical staff. The insurer forecasts a 30 to 40 percent reduction in process costs thanks to this automated process. An increasing number of players (including AXA, the UK-based company More Than, and Zurich) have already taken some initial steps along this path: iPhone applications that clients can use in car accidents or other emergencies. Policy-holders get in touch with the insurer’s emergency assistance center, which uses GPS to note the accident’s location. Customers can use their phones not only to highlight damaged areas by tapping images of their cars on their iPhone’s screen but also to send the center photos of the accident to expedite claims handling.
        
        Leaders should know how technology could lead, over the longer term, to more disruptive change. Risk sharing, commonly known as mutualization, lies at the heart of the insurance business model. A growing ability to crunch data on customers will allow companies to segment them and risk pools much more finely. Competitors with such strengths could offer policies at favorable rates to consumers with more profitable risk profiles.
        
        Social networks play a new role too. Beyond their value as tools, they could spawn novel, self-defining markets. Insurers should monitor the way affinity groups on Facebook or other social networks could, for example, coalesce into “minimutuals” that contract for coverage directly with reinsurers, bypassing traditional insurers. While still a novelty for the insurance industry, the idea of forming spontaneous, ad hoc groups of Internet users to negotiate preferential rates with providers of goods and services isn’t new: Groupon, which specializes in precisely this type of deal cutting, has offered online discounts since 2008.
        
        Technology is not a panacea for the industry’s current challenges. Insurers face broadly differing competitive environments, and individual companies have different capacities for absorbing and deploying new technology. Nonetheless, insurers must develop a greater awareness of how to apply technologies meaningfully. Essentially, they must set a technology-adoption target aligned with their strategic goals, forge closer ties between IT and sometimes skeptical business leaders, and develop a cadre of IT leaders who can meet the new challenges. Our experience suggests that insurers could usefully consider the following approach.
        
        A key first step is to determine which strategic IT posture best fits the organization’s business strategy. In our experience, most companies fit into one of three categories.
        
        Opportunistic companies generally adopt IT innovations for defensive reasons, and only when technologies are mature and well established in the industry. This strategy best suits insurers with highly specialized businesses (for instance, maritime insurance), where success depends more on expert knowledge than on technology. Companies competing in markets or countries less open to competition also fit the opportunistic profile. Since such organizations see technological innovation as something of a necessary evil, their chief concern for IT investments is efficiency—adopting the lowest-cost solution that delivers acceptable reliability rather than paying out large sums for something more ambitious.
        
        Fast followers pick up selected technologies at a relatively early stage to gain competitive advantage—but only once it’s clear they add value. Such companies are rarely early adopters; they wait for others to prove that a concept works and then rely on execution muscle to adopt it quickly and, where possible, to use it more successfully than first movers do. The key for fast followers is to create an “observation deck” so that the organization can spot potentially disruptive technologies early on.
        
        Consider the UK wealth-management operations of a major insurer. In this market segment, an advanced advisory platform for high-net-worth clients is a differentiating feature. The insurer wasn’t the first company to develop such a platform but was very alert to those launched by the competition and to the features that made them more (or less) popular with customers, and thus affected market share. The organization learned from others quite effectively, and its advisory platform, once launched, rapidly became one of the best on the market.
        
        Digital insurers are aggressive, trendsetting, visionary companies—the “Apples” of the insurance industry—that have a well-developed capacity for IT innovation and believe that technology is the key to gaining and sustaining competitive advantage. Companies such as Admiral in the United Kingdom tend to be “greenfield” attackers, actively leveraging technology to access new markets and offer new services. Another leading carrier, in the United States, allows prospective customers to tailor their auto coverage through an interactive application on the insurer’s Web site.
        
        Increasingly, we find that units within established major players are adopting this posture. The UK-based pay-as-you-drive car insurer Coverbox, for instance, offers discounted motor insurance to drivers who install a tracking system in their vehicles.
        
        No matter which of these strategic postures an insurer chooses, successfully deploying innovative technologies requires the IT organization and business unit leaders to align closely. While this may seem self-evident, the interactions of business executives with IT are often patchy, tending to focus narrowly on efficiency or localized investment, not on defining a coherent, company-wide IT portfolio. It’s not surprising that the industry’s failed product launches often result from inadequate or even nonexistent collaboration between IT and business. Both sides should work to develop forums and decision-making bodies where fruitful interactions can take place.
        
        In this vein, one global insurer with multiple business lines has overhauled its process for defining and investing in new IT projects. Divisional CEOs and senior executives of businesses must now meet regularly with the CIO and his direct reports in workshops that examine the relationship between key business initiatives and IT. The program, rolled out to more than 50 countries in the group, has dramatically improved collaboration and the satisfaction of internal customers. Funding for “change the company” IT projects (as opposed to the usual “run the company” ones) has grown to 60 percent of all IT funding, from 40 percent, while IT’s total cost has remained constant in the short term and is expected to decrease in the long term.
        
        A strategic road map and collaborative leadership are essentials, of course. But they will be meaningless if an organization can’t execute very well on both the IT and the business sides. That means efficient processes and strong talent. We see encouraging signs, within the industry, of recent efforts to improve IT management and of the more and more frequent use of “lean” methods. Many insurers are therefore benefiting from faster times to market, while productivity improvements in the 15 to 30 percent range are common. The business and IT skills needed to implement IT innovations are in short supply, however, so insurers must invest wisely in internal talent. And if a company is to apply technology successfully, the entire leadership team should have a thorough grounding in the latest trends in customer needs.
        
        One global insurer has instilled more dynamism in its ranks by emphasizing meritocracy. The senior leadership has adopted a forced ranking of performance and created clear career paths that let high-potential staff members advance quickly while requiring them to rotate periodically through various business lines. The goal: to develop well-trained, highly motivated IT executives who possess a broad understanding of the company’s business and have created networks beyond IT that will give them an outsider’s view of IT’s performance.
        
        For many insurers, IT innovation will require a major rethink of how the business is run and a transformation of IT management. To kick off the thought process, it might be helpful to look beyond the insurance industry to spot promising opportunities offered by new technologies and by new business models that embrace those opportunities.
        
        Insurance executives who set clear targets for a transformation, align the business with IT, and develop a strong talent base before beginning the journey can improve the chances that technology will become an important strategic lever in challenging times.Despite decades of increasingly intensive use of information across industries, IT has remained a black box for many executives. Too often, the link between spending and performance has been unclear, if not problematic. As a result, leaders felt that their only course of action was to hire a competent CIO, throw increasing amounts of money at IT, and hope for the best. The economic disruptions of recent years, however, have tightened budgets and placed a premium on action, forcing companies to rethink IT’s fundamental role.
        
        In most organizations, IT began as a support function, leading to a one-dimensional management approach. However, technology-enabled products, interactive communications, and an “always on” information environment have thrust IT to the forefront, with critical implications for business growth and customer engagement. In addition, established practices, such as lean-management techniques, have highlighted the value of IT in reducing waste and increasing productivity.
        
        This deeper recognition of IT’s potential has given rise to a new management model consisting of two categories: “Factory IT” and “Enabling IT.” Factory IT encompasses the bulk of an organization’s IT activities, applying lessons from the production floor—scale, standardization, and simplification—to drive efficiency, optimize delivery, and lower unit costs. Enabling IT is focused on helping organizations respond more effectively to changing business needs and gain a competitive advantage by spurring innovation and growth.
        
        This approach goes beyond simply relabeling functions to include broader leadership, governance, and organizational changes, and IT leaders will need very different skills to manage each model. Business leaders will have to engage with IT in new ways. For instance, while IT standardization and consolidation increase responsiveness, speed to market, and cost effectiveness, managers may have fewer options to customize solutions. Likewise, more transparency and better metrics may come at the expense of unrestricted choice for configuration and architecture. In return, business leaders would get a new type of IT partner to support innovation, with skills to deliver IT-enabled capabilities quickly that drive both top- and bottom-line growth. But they’ll need to treat such IT staff as full members of their group, offering incentives and rewards for exceptional performance.
        
        The core elements of the typical IT function have changed radically over the past decade, and this evolution has enabled the Factory model. Ten years ago, a company might have felt compelled to create its own software to manage the supply chain and other activities; today, many configurable products can meet those needs. Similarly, expensive, single-purpose servers and the dedicated support staff they require can be replaced by commodity servers, often managed from half a world away.
        
        Moreover, these standardized platforms can now be coupled with mature processes for managing broad swaths of IT, including basic infrastructure and many of the business applications. Under this configuration, IT activities can be restructured and continually improved much as any other business process, using a combination of lean techniques, automation, and outsourcing or cloud computing to drive down costs and improve quality. Results from our latest technology survey demonstrate how companies have already begun to implement some of these practices (Exhibit 1).
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Standardization decreases the resources and specialized development needed to support IT, allowing organizations to apply proven management methods from large industrial settings to reduce IT costs. Lean-management techniques have evolved well beyond manufacturing and are applicable to the types of skilled services found in IT. A company can typically create 20 to 30 percent or more in additional IT capacity by streamlining processes, automating routine functions, and eliminating redundancy. Major sources of IT waste include unnecessary functionality (for example, gold-plated systems with extra, noncore functions), work flow bottlenecks caused by inadequate cross-training, and frequent rework from bugs or constantly changing requirements.
        
        Companies can benefit significantly by replacing customized systems with highly standardized offerings (for example, a Web server or e-mail platform that includes common hardware, applications, and support levels) and service catalogs (essentially à la carte menus that specify the cost of each service). These improvements increase cost transparency and highlight clear opportunities for further efficiency while also giving individual business units the freedom to customize certain features and functions.
        
        Organizations should also recognize that not all processes have equal value and should set service and support levels accordingly. For instance, one bank applied the same exacting levels of support and performance for its critical core banking applications to an in-house employee service portal. By increasing the service portal’s allowable downtime to five days a year, from just five hours, the bank saved hundreds of thousands of dollars in hardware, software, and staff expense.
        
        In our experience, these measures often double workforce productivity by redeploying or reducing staff. A standardized IT environment also allows companies to select from a wider array of vendors whose scale and skills can further reduce costs and improve delivery. In addition, by avoiding customized hardware and operating systems, companies can more readily take advantage of a technology cost curve that has been dropping by 4 to 5 percent annually.
        
        IT tends to operate on a very long-term investment cycle consisting of large, multiyear projects, extended outsourcing deals, and durable infrastructure assets. As the pace of business change accelerates and organizations respond to shifting market conditions or more frequent M&A, IT leaders are often constrained by these investments. IT departments are starting to adapt in two ways:
        
        The cloud: Cloud computing offers access to information, processing, and storage through the network or an external service provider. This mode of delivery allows companies to purchase computer processing as a service, rather than making up-front investments in IT capacity and in-house support staff. The New York Times, for example, digitized and catalogued more than 100 years of archived articles for its Web site in a 24-hour period by using Amazon.com’s cloud offering, avoiding the need to configure and operate a set of servers for a onetime effort.
        
        Agile software development. IT programmers are flocking to approaches that emphasize the very fast, iterative development of systems through close interactions with users, allowing continual feedback and programming refinement. This agility can deliver new systems and capabilities in a matter of weeks or months instead of years. A frequent iteration cycle also keeps IT developers and business users in sync on requirements and priorities. Agile development may not be right for every project. However, since this approach is most effective when business needs are shifting, it is gaining favor among many IT departments.
        
        Together, the cloud and agility can make the IT factory more nimble, with lower costs and faster delivery.
        
        For most companies, IT complexity increases gradually, as systems slowly evolve beyond their initial purpose, or through acquisitions, when new, sometimes duplicative systems are built for individual business units. Performance suffers over time, as ineffective IT slows product introductions, hampers customer interactions, and often makes postmerger integration more difficult.
        
        IT leaders recognize the adverse effects of complexity, but replacing these systems involves a substantial commitment of resources: hardware, new applications, and staff and vendor time. The economics are difficult to justify given the short time horizons of many tough-minded CFOs, particularly since tangible benefits are often realized only in the longer term.
        
        To manage complexity, companies are starting to employ a more holistic business case model that goes beyond the traditional, IT-centric versions. This model includes realistic, verifiable cost–benefit analysis to assess the impact of new systems on the entire organization. Critically, such plans also require a road map for how future projects might build on the investment. At one company, for instance, the business case to deliver a unified view of all customer data showed how better information management could enable follow-on projects for marketing systems, enhancing cross-selling opportunities.
        
        Sidebar Implementing Factory IT While a Factory IT model does not require radical restructuring, it does involve targeted changes in talent, organization, accountability, and governance. IT leaders should use this opportunity to streamline the IT function and create a new baseline that balances the measurable improvement of both cost effectiveness and delivery. To succeed, the model must achieve these goals: Ensure that external customers don’t see a material degradation of their experience and that business users begin to see tangible results in return.
        
        Verify that planning for the IT information environment accounts for a range of business needs and is aligned with larger market trends.
        
        Engage IT executives who have experience in applying lean techniques and driving continuous improvement and the right temperament and skills to form effective partnerships across the business.
        
        The benefits of these business cases are twofold. First, they ensure that the simplification of processes and systems is a starting point for new project investments. Second, better visibility can show how seemingly small IT projects can deliver a high return on investment by improving standardization and service levels across the entire business (see sidebar “Implementing Factory IT”).
        
        As the market-facing complement to the Factory model, Enabling IT focuses on creating new sources of value. This emphasis on innovation requires three things: ready access to relevant information, a willingness to test and learn, and close collaboration. Enabling IT supports these activities by developing the processes and technologies to launch a new sales channel, design a tech-enabled product feature, or increase customer retention through online offerings.
        
        Where Factory IT is housed centrally, Enabling IT’s employees function as an IT SWAT team for new initiatives or business imperatives and are often closely integrated with—or even embedded in—business units. The performance of these employees is rated on metrics such as responsiveness and flexibility rather than on their ability to deliver low unit costs. And where Factory IT focuses on long-term projects and on-time delivery, Enabling IT is typified by rapid prototyping and iterative development. Our survey results show that companies deploy Enabling IT to support innovation and growth in three ways (Exhibit 2):
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The increasing volume of data is taxing the ability of companies to track, filter, and analyze information and turn it into useful, actionable insights. Organizations that build effective information systems can take advantage of emerging opportunities and respond more quickly to unseen market changes. With the rise of electronic health records and prescription data, for example, pharmaceutical companies need systems to structure and mine this information for trends on patient compliance or drug efficacy.
        
        Resolving these issues requires a cross-functional group of IT experts, statistical analysts, and business leaders. IT must develop new technical capabilities to manage the massive amount of information, and IT leaders will need to collaborate more closely with management teams to extract its full value.
        
        Where lean manufacturing and Factory IT seek to avoid errors, Enabling IT’s mind-set tolerates (and even encourages) the mistakes that result from experimentation and iteration as long as they happen quickly, the outcomes are measured, and the lessons are incorporated into the team’s thinking. More companies are embracing rapid experimentation as a way to develop, refine, and upgrade their services or products. Capital One and Google, for example, have been at the forefront of this trend with their credit cards and online services, respectively. That wave is spreading to traditional players: P&G’s Vocalpoint, a network of mothers, provides feedback on new product ideas. Similarly, a leading fast-food company is using IT systems and analytics at test sites to gauge the impact of new menu choices on store-level revenue, operations, and customer experience.
        
        Such experimentation requires the right set of technical capabilities and a flexible IT environment. Managers must employ tools to define, build, test, and improve new products quickly, integrating feedback from both internal stakeholders and a set of users or customers. Responsive IT support is a vital component of this effort. By assembling a team to work hand in hand with the managers on these new business offerings, IT provides essential support to help build and modify business processes and systems rapidly.
        
        IT has historically focused on automating high-velocity transactions for enterprise resource planning (ERP), supply chains, and customer relationship management (CRM). That focus is now shifting to lightweight Web 2.0 tools to support the more diverse transactions and more complex interactions that shape, review, and inform innovation. Tools range in complexity from simple executive blogs to more robust portals where users can collaboratively access and analyze data sets.
        
        Often, these newer forms of knowledge sharing require little additional investment, since the content is largely user generated and the tools rely mostly on existing applications and infrastructure. In fact, these tools usually augment existing transactional systems rather than replace them, thus increasing the value of IT investments while holding down costs. Since users often drive the development of these collaborative tools, strategies that complement users’ work practices and styles are most effective. Although such an endeavor is challenging, the payoff can be substantial.
        
        Sidebar Enabling IT: Bringing it together Enabling IT can help support an organization’s innovation culture. To work well, the model requires three components: Nimble, flexible, and focused work groups embedded in the business and responsible and accountable to both the business unit leader and CIO (in contrast to a highly standardized and structured organization).
        
        A more qualitative approach to measuring performance—focused on the IT team’s contribution to the business unit and its overall results—rather than enterprise-wide goals for costs or efficiency.
        
        Credible, deeply knowledgeable IT leaders and team members who are seen as an integral part of the functions and businesses they enable.
        
        Innovation is seldom neat, and these collaborative systems are no exception. Successful initiatives usually start as small side projects or tests that gain critical mass rapidly, often with little regard for corporate technology or security standards. IT leaders can play a critical role by selecting and validating platforms, setting policies, and promoting capabilities to potential participants. Because Enabling IT staff members already work within business units, they can help guide these projects, giving executives some degree of control and assurance on compliance with critical security and data standards. The result is a smoother transition to the Factory IT environment when the systems reach maturity (see sidebar “Enabling IT: Bringing it together”).
        
        While leading companies may implement these principles differently based on their business needs and culture, we believe there is no turning back. Factory IT’s potential to increase efficiency and reduce costs can finance the next wave of Enabling IT’s innovation. The combination of functional productivity and business value creation will likely be a major competitive differentiator; the first step in delivering this value is to ensure companies have the right leaders in place for each effort.The ability to transform data into insights to help manage a company is the domain of corporate business intelligence, which consists of the processes, applications, and practices that support executive decision making. With such knowledge at a premium, chief information officers have moved to center stage. By connecting the right parties across their companies, CIOs are making their role—helping organizations to mediate between business requirements and IT capabilities—more critical than ever.
        
        It’s a challenging mission because for all the data flowing through companies, executives often struggle to find the information they need to make sound decisions. Potentially valuable content is frequently trapped in organizational silos, lost in transit from one system to another, bypassed by inadequately tuned data collection systems, or presented in user-unfriendly formats. Although wired with layers of information-gathering technology, organizations still find it difficult to deliver the right data to the right people.
        
        At the heart of these difficulties are inadequate executive information systems, supposedly designed to help top management easily access pertinent internal and external data for managing a company. Our research suggests that a set of common problems plagues these systems, which have existed for some time. Some forward-looking companies have therefore given CIOs a mandate to redesign them and to restore their importance in corporate decision making.
        
        When information systems are dysfunctional, performance suffers. The executives of a large chemical company, for example, found that only about half of the data generated from its executive information system was relevant to corporate decision making. Executives needed precise numbers for each strategic business unit, product, and operating business, but nonuniform data made apples-to-apples revenue and cost comparisons difficult.
        
        A rigid design architecture, based solely on financial-accounting rules, restricted the system’s output to a limited number of reporting formats. Custom analyses, such as inventory turnover by product and region, were nearly impossible to generate. A cluttered front-end interface compounded the problem. Executives intent on reviewing key performance indicators (KPIs) had to sort through a jumble of onscreen data, so the CIO needed to take several IT analysts offline every month to comb through the figures and create the desired analyses. Frustrated, the company’s board pressed the CIO to explain why group reporting costs were climbing upward and so much IT support was necessary.
        
        As the chief information officer, the CIO should play a more central role in designing next-generation executive information systems that can help a company’s top managers extract value from the data that surrounds them. Three major factors often hinder success.
        
        Different semantics and inconsistencies in the way information is structured from one unit to another hobble many executive information systems. Data, gathered through a multitude of sources, often with different labels, tags, and uses, can be hard to aggregate accurately for decision-making purposes. Group management accounting may roll up figures one way, operational management another—inconsistencies that can make executives question the reliability of the underlying data. At times, data sets lack contextual links that could provide perspective needed for executive analysis. Even if the system’s interface seems to be convenient, when executives doubt that the numbers are vetted, current, and accurate, they may be disinclined to use it.
        
        Too often, disjointed communication between businesses and IT can lead to flaws in an executive information system’s design. Creating reports may be complex. Sometimes IT logic rather than business analysis drives the navigation system. Tensions may arise as divisions, accustomed to seeing their numbers presented a certain way, vie to retain control over preferred reporting formats. Clear ownership is central to governance, but fiefdom issues are often a problem. As one executive told us, “Data ownership can get personal. The notion ‘I want my data my way’ can be pervasive.”
        
        Because business needs are dynamic, corporate business intelligence must be as well. Yet many executive information systems have static design architectures that limit the capture, organization, and accessibility of data. New demands—say, regulatory changes, the adoption of International Financial Reporting Standards, or requests from the field for performance data—often require time-consuming adjustments. Older systems are largely ill-equipped to handle these updates, so the IT staff must create manual links to Excel spreadsheets and other data tables, and this can cause confusion. Since design limitations prevent the new data from being integrated into the system, parallel data structures crop up across the IT landscape. A well-functioning executive information system should deliver varying levels of detail, yet many dashboards offer only a top-line view of the business; navigation facilities to pierce through layers of reporting data overcharge current IT capabilities.
        
        One global corporation decided that the best way to tackle these problems was a wholly redesigned IT blueprint to support top management. The company, a multibillion-dollar global logistics organization prized for its ability to transport goods from one corner of the globe to another, was having a tough time getting its internal executive information system in order. While it could track cargo along any given point of its delivery network, it had little visibility into its own data streams. Years of rapid growth and decentralized, somewhat laissez-faire information management had created an untidy patchwork of reporting processes across its divisions. Management lacked a single viewpoint into the company’s core performance data and, as a result, couldn’t know for sure which products made money.
        
        Sidebar One company’s blueprint for a next-generation executive information system The CIO at a global company needed to take an end-to-end view in designing his company’s business intelligence system. At the front end, reports needed to cascade in a logical hierarchy. The top managers’ interface required an intuitive design offering a wide range of analyses. To support this robust interface, the underlying information structure would have to be reconceived. Data were standardized and information pathways reworked for the most important key-performance indicators (KPIs) needed to manage the company. The corporate navigator is its framework for designing a next-generation executive information system with the four-layer business/IT model shown in the exhibit. The entry screen, which gives executives a graphical top-line summary of the most important KPIs (Layer 1), includes comment fields to highlight findings or explain deviations. Using the one-page reporting format, Layer 2 offers an at-a-glance assessment of corporate performance in four core areas: financial accounting, management accounting, compliance, and program management. With a few mouse clicks, anyone interested in the underlying details can access pre-defined analyses, such as contribution margins and P&L calculations, by product or business unit. A hierarchical reporting structure connects these views, allowing users to toggle through different levels of performance data. As a result of the economic crisis, cash and liquidity management ranked among the top priorities of this company’s CFO, so the CIO outfitted the next-generation executive information system with a ‘flexible periphery’ linked to financial markets and other data sources. Executives can now model the way market movements influence the cost of capital and cash flows. The remaining tiers house the supporting software and data structures (Layer 4a) and the IT infrastructure, with hardware and networks (Layer 4b). A service-oriented Layer 3, structured by the most important business domains, aligns business requirements with IT capabilities to ensure that the executive information system tags, synthesizes, and applies data efficiently.
        
        Knowing that something had to be done, the CIO formed a task force, with members from both the business side and IT, which quickly found that relations between them were in some ways dysfunctional. Executives from headquarters, the business units, and the divisional and central IT functions all documented performance in their own way, tapping into different data sources to tally their results. These figures were rolled up into a series of group reports, but variances in the underlying data and the lack of a uniform taxonomy made it difficult for managers to know which set of numbers to trust. The management interface, designed to present key performance data, was jammed with so many different and, in some cases, conflicting KPIs as to be largely unusable. A non-user-friendly front end compounded the problem. Executives therefore asked for a new executive information system to gauge the company’s performance at varying levels of detail (exhibit). This new corporate navigator would have to incorporate major improvements in design and functionality (see sidebar, “One company’s blueprint for a next-generation executive information system”).
        
        Exhibit Corporate navigator The benefits of a next-generation executive information system include a streamlined reporting hierarchy and standardized data. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        To bring order, the CIO had to deal with a number of core issues. Standardizing the underlying data to tackle inconsistencies was the first of them. Under the CIO’s leadership, the task force agreed to a set of groupwide KPI definitions, factoring in what the business side saw as the most important KPIs—such as inventory turns, cycle times, and margins—on a product-by-product basis. Then he created a company-wide taxonomy table to smooth the translation from one unit to another. The relationships between key pieces of information were redefined for the most important KPIs needed to manage the company. New IT bridges resolved the problem of manual data transitions from one system to another.
        
        To improve the system’s responsiveness, the CIO redefined the technical blueprint, creating centralized business intelligence data storage to house, tag, and order different data streams. A new architecture based on business domains or groupings rather than IT capabilities allowed data to flow more fluidly. The design included flexible tools for accessing and transforming data—tools that connected the group’s system to various upstream databases, such as those for financial and management accounting. Revised business applications created a common set of monthly and quarterly reporting formats, as well as analyses that executives without deep IT knowledge and experience could use to delve deeper into such things as share-holder value, budgeting, and capital planning. Last, a redesigned and improved interface allowed managers to move easily between these views to get the information they desired in a format they could handle. Features such as color coding, for example, created more intuitive groupings, guiding users through different types of reporting data.
        
        As the CIO resolved these IT issues, he worked in parallel to help business leaders gain a greater degree of visibility into the company’s core operations. In a series of meetings, he asked the leaders to identify their most important KPIs and reports in order to streamline and prioritize them. Stronger logic and better-informed, more disciplined reports and metrics help leaders set better targets, raise questions when performance diverges from them, and monitor progress more efficiently. The dynamic business-based architecture makes it easier to change reports—eliminating the costly ad hoc analyses that had bogged down the process—so fewer IT resources are needed to maintain the system. Because the underlying data are now far more reliable, the CFO, for example, can use the redesigned interface as a presentation device during analyst calls and when he speaks before the board.
        
        Organizational flux, rising competitive pressures, and the expanding global reach of many organizations now place a premium on information that helps executives manage a company. New demands for transparency from stakeholders and regulators magnify the need for better (and often more timely) information. Transforming data into useful insights is critical to creating value and, ultimately, to a company’s competitive advantage. As the leading conduits between business requirements and IT capabilities, CIOs have an opportunity to expand their influence while supporting the goals of their companies.Two-and-a-half years ago, we described eight technology-enabled business trends that were profoundly reshaping strategy across a wide swath of industries. We showed how the combined effects of emerging Internet technologies, increased computing power, and fast, pervasive digital communications were spawning new ways to manage talent and assets as well as new thinking about organizational structures.
        
        Since then, the technology landscape has continued to evolve rapidly. Facebook, in just over two short years, has quintupled in size to a network that touches more than 500 million users. More than 4 billion people around the world now use cell phones, and for 450 million of those people the Web is a fully mobile experience. The ways information technologies are deployed are changing too, as new developments such as virtualization and cloud computing reallocate technology costs and usage patterns while creating new ways for individuals to consume goods and services and for entrepreneurs and enterprises to dream up viable business models. The dizzying pace of change has affected our original eight trends, which have continued to spread (though often at a more rapid pace than we anticipated), morph in unexpected ways, and grow in number to an even ten.
        
        The rapidly shifting technology environment raises serious questions for executives about how to help their companies capitalize on the transformation under way. Exploiting these trends typically doesn’t fall to any one executive—and as change accelerates, the odds of missing a beat rise significantly. For senior executives, therefore, merely understanding the ten trends outlined here isn’t enough. They also need to think strategically about how to adapt management and organizational structures to meet these new demands.
        
        For the first six trends, which can be applied across an enterprise, it will be important to assign the responsibility for identifying the specific implications of each issue to functional groups and business units. The impact of these six trends—distributed cocreation, networks as organizations, deeper collaboration, the Internet of Things, experimentation with big data, and wiring for a sustainable world—often will vary considerably in different parts of the organization and should be managed accordingly. But local accountability won’t be sufficient. Because some of the most powerful applications of these trends will cut across traditional organizational boundaries, senior leaders should catalyze regular collisions among teams in different corners of the company that are wrestling with similar issues.
        
        Three of the trends—anything-as-a-service, multisided business models, and innovation from the bottom of the pyramid—augur far-reaching changes in the business environment that could require radical shifts in strategy. CEOs and their immediate senior teams need to grapple with these issues; otherwise it will be too difficult to generate the interdisciplinary, enterprise-wide insights needed to exploit these trends fully. Once opportunities start emerging, senior executives also need to turn their organizations into laboratories capable of quickly testing and learning on a small scale and then expand successes quickly. And finally the tenth trend, using technology to improve communities and generate societal benefits by linking citizens, requires action by not just senior business executives but also leaders in government, nongovernmental organizations, and citizens.
        
        Across the board, the stakes are high. Consider the results of a recent McKinsey Quarterly survey of global executives on the impact of participatory Web 2.0 technologies (such as social networks, wikis, and microblogs) on management and performance. The survey found that deploying these technologies to create networked organizations that foster innovative collaboration among employees, customers, and business partners is highly correlated with market share gains. That’s just one example of how these trends transcend technology and provide a map of the terrain for creating value and competing effectively in these challenging and uncertain times.
        
        In the past few years, the ability to organize communities of Web participants to develop, market, and support products and services has moved from the margins of business practice to the mainstream. Wikipedia and a handful of open-source software developers were the pioneers. But in signs of the steady march forward, 70 percent of the executives we recently surveyed said that their companies regularly created value through Web communities. Similarly, more than 68 million bloggers post reviews and recommendations about products and services.
        
        Intuit is among the companies that use the Web to extend their reach and lower the cost of serving customers. For example, it hosts customer support communities for its financial and tax return products, where more experienced customers give advice and support to those who need help. The most significant contributors become visible to the community by showing the number of questions they have answered and the number of “thanks” they have received from other users. By our estimates, when customer communities handle an issue, the per-contact cost can be as low as 10 percent of the cost to resolve the issue through traditional call centers.
        
        Other companies are extending their reach by using the Web for word-of-mouth marketing. P&G’s Vocalpoint network of influential mothers is a leading example. Mothers share their experiences using P&G’s new products with members of their social circle, typically 20 to 25 moms. In markets where Vocalpoint influencers are active, product revenues have reached twice those without a Vocalpoint network.
        
        Facebook has marshaled its community for product development. The leading social network recently recruited 300,000 users to translate its site into 70 languages—the translation for its French-language site took just one day. The community continues to translate updates and new modules.
        
        Yet for every success in tapping communities to create value, there are still many failures. Some companies neglect the up-front research needed to identify potential participants who have the right skill sets and will be motivated to participate over the longer term. Since cocreation is a two-way process, companies must also provide feedback to stimulate continuing participation and commitment. Getting incentives right is important as well: cocreators often value reputation more than money. Finally, an organization must gain a high level of trust within a Web community to earn the engagement of top participants.
        
        Jacques Bughin, Michael Chui, and Brad Johnson, “The next step in open innovation,” McKinsey Quarterly, 2008 Number 3.
        
        Michael Chui, Andy Miller, and Roger P. Roberts, “Six ways to make Web 2.0 work,” McKinsey Quarterly, 2009 Number 2.
        
        Josh Bernoff and Charlene Li, Groundswell: Winning in a World Transformed by Social Technologies, first edition, Cambridge, MA: Harvard Business School Press, 2008.
        
        Clay Shirky, Here Comes Everybody: The Power of Organizing Without Organizations, reprint edition, New York, NY: Penguin, 2009.
        
        In earlier research, we noted that the Web was starting to force open the boundaries of organizations, allowing nonemployees to offer their expertise in novel ways. We called this phenomenon “tapping into a world of talent.” Now many companies are pushing substantially beyond that starting point, building and managing flexible networks that extend across internal and often even external borders. The recession underscored the value of such flexibility in managing volatility. We believe that the more porous, networked organizations of the future will need to organize work around critical tasks rather than molding it to constraints imposed by corporate structures.
        
        At one global energy services company, geographic and business unit boundaries prevented managers from accessing the best talent across the organization to solve clients’ technical problems. Help desks supported engineers, for example, but rarely provided creative solutions for the most difficult issues. Using social-network analysis, the company mapped information flows and knowledge resources among its worldwide staff. The analysis identified several bottlenecks but also pointed to a set of solutions. Using Web technologies to expand access to experts around the world, the company set up new innovation communities across siloed business units. These networks have helped speed up service delivery while improving quality by 48 percent, according to company surveys.
        
        Dow Chemical set up its own social network to help managers identify the talent they need to execute projects across different business units and functions. To broaden the pool of talent, Dow has even extended the network to include former employees, such as retirees. Other companies are using networks to tap external talent pools. These networks include online labor markets (such as Amazon.com’s Mechanical Turk) and contest services (such as Innocentive and Zooppa) that help solve business problems.
        
        Management orthodoxies still prevent most companies from leveraging talent beyond full-time employees who are tied to existing organizational structures. But adhering to these orthodoxies limits a company’s ability to tackle increasingly complex challenges. Pilot programs that connect individuals across organizational boundaries are a good way to experiment with new models, but incentive structures must be overhauled and role models established to make these programs succeed. In the longer term, networked organizations will focus on the orchestration of tasks rather than the “ownership” of workers.
        
        Thomas W. Malone, The Future of Work: How the New Order of Business Will Shape Your Organization, Your Management Style, and Your Life, illustrated edition, Cambridge, MA: Harvard Business Press, 2004.
        
        Lowell L. Bryan and Claudia I. Joyce, Mobilizing Minds: Creating Wealth from Talent in the 21st-Century Organization, New York, NY: McGraw-Hill, 2007.
        
        Albert-Laszlo Barabasi, Linked: How Everything is Connected to Everything Else and What It Means for Business, Science, and Everyday Life, New York, NY: Plume, 2009.
        
        Across many economies, the number of people who undertake knowledge work has grown much more quickly than the number of production or transactions workers. Knowledge workers typically are paid more than others, so increasing their productivity is critical. As a result, there is broad interest in collaboration technologies that promise to improve these workers’ efficiency and effectiveness. While the body of knowledge around the best use of such technologies is still developing, a number of companies have conducted experiments, as we see in the rapid growth rates of video and Web conferencing, expected to top 20 percent annually during the next few years.
        
        At one high-tech enterprise, the sales force became a crucible for testing collaboration tools. The company’s sales model relied on extensive travel, which had led to high costs, burned-out employees, and difficulty in scaling operations. The leadership therefore decided to deploy collaboration tools (including video conferencing and shared electronic workspaces, which allow people in different locations to work with the same document simultaneously), and it reinforced the changes with a sharp reduction in travel budgets. The savings on travel were four times the company’s technology investment. Customer contacts per salesperson rose by 45 percent, while 80 percent of the sales staff reported higher productivity and a better lifestyle.
        
        In another instance, the US intelligence community made wikis, documents, and blogs available to analysts across agencies (with appropriate security controls, of course). The result was a greater exchange of information within and among agencies and faster access to expertise in the intelligence community. Engineering company Bechtel established a centralized, open-collaboration database of design and engineering information to support global projects. Engineers starting new ones found that the database, which contained up to 25 percent of the material they needed, lowered launch costs and sped up times to completion.
        
        Despite such successes, many companies err in the belief that technology by itself will foster increased collaboration. For technology to be effective, organizations first need a better understanding of how knowledge work actually takes place. A good starting point is to map the informal pathways through which information travels, how employees interact, and where wasteful bottlenecks lie.
        
        In the longer term, collaboration will be a vital component of what has been termed “organizational capital.” The next leap forward in the productivity of knowledge workers will come from interactive technologies combined with complementary investments in process innovations and training. Strategic choices, such as whether to extend collaboration networks to customers and suppliers, will be important.
        
        Podcast: William Dutton, director of the Oxford Internet Institute at the University of Oxford, says collaboration technologies will revolutionize organizations, vastly expanding their reach and empowering their employees.
        
        Andrew McAfee, Enterprise 2.0: New Collaborative Tools for Your Organization’s Toughest Challenges, first edition, Cambridge, MA: Harvard Business School Press, 2009.
        
        Erik Brynjolfsson and Adam Saunders, Wired for Innovation: How Information Technology is Reshaping the Economy, Cambridge, MA: The MIT Press, 2009.
        
        Wolf Richter, David Bray, and William Dutton, “Cultivating the value of networked individuals,” in Jonathan Foster, Collaborative Information Behavior: User Engagement and Communication Sharing, Hershey, PA: IGI Global.
        
        The adoption of RFID (radio-frequency identification) and related technologies was the basis of a trend we first recognized as “expanding the frontiers of automation.” But these methods are rudimentary compared with what emerges when assets themselves become elements of an information system, with the ability to capture, compute, communicate, and collaborate around information—something that has come to be known as the “Internet of Things.” Embedded with sensors, actuators, and communications capabilities, such objects will soon be able to absorb and transmit information on a massive scale and, in some cases, to adapt and react to changes in the environment automatically. These “smart” assets can make processes more efficient, give products new capabilities, and spark novel business models.
        
        Auto insurers in Europe and the United States are testing these waters with offers to install sensors in customers’ vehicles. The result is new pricing models that base charges for risk on driving behavior rather than on a driver’s demographic characteristics. Luxury-auto manufacturers are equipping vehicles with networked sensors that can automatically take evasive action when accidents are about to happen. In medicine, sensors embedded in or worn by patients continuously report changes in health conditions to physicians, who can adjust treatments when necessary. Sensors in manufacturing lines for products as diverse as computer chips and pulp and paper take detailed readings on process conditions and automatically make adjustments to reduce waste, downtime, and costly human interventions.
        
        As standards for safety and interoperability begin to emerge, some core technologies for the Internet of Things are becoming more widely available. The range of possible applications and their business impact have yet to be fully explored, however. Applications that improve process and energy efficiency (see trend number six, “Wiring for a sustainable world,” later in this article) may be good starting points for trials, since the number of successful installations in these areas is growing. For more complex applications, however, laboratory experiments, small-scale pilots, and partnerships with early technology adopters may be more fruitful, less risky approaches.
        
        Podcast: Kristopher Pister, professor of electrical engineering and computer sciences at the University of California, Berkeley, tells why a new generation of sensors and location technologies will endow the Internet of Things with much greater intelligence.
        
        Michael Chui, Markus Löffler, and Roger Roberts, “The Internet of Things,” McKinsey Quarterly, 2010 Number 2.
        
        Hal R. Varian, Computer Mediated Transactions, Ely Lecture to the American Economics Association, Atlanta, GA, January 3, 2010.
        
        Bernhard Boser, Joe Kahn, and Kris Pister, “Smart dust: Wireless networks of millimeter-scale sensor nodes,” Electronics Research Laboratory Research Summary, 1999.
        
        Could the enterprise become a full-time laboratory? What if you could analyze every transaction, capture insights from every customer interaction, and didn’t have to wait for months to get data from the field? What if . . . ? Data are flooding in at rates never seen before—doubling every 18 months—as a result of greater access to customer data from public, proprietary, and purchased sources, as well as new information gathered from Web communities and newly deployed smart assets. These trends are broadly known as “big data.” Technology for capturing and analyzing information is widely available at ever-lower price points. But many companies are taking data use to new levels, using IT to support rigorous, constant business experimentation that guides decisions and to test new products, business models, and innovations in customer experience. In some cases, the new approaches help companies make decisions in real time. This trend has the potential to drive a radical transformation in research, innovation, and marketing.
        
        Web-based companies, such as Amazon.com, eBay, and Google, have been early leaders, testing factors that drive performance—from where to place buttons on a Web page to the sequence of content displayed—to determine what will increase sales and user engagement. Financial institutions are active experimenters as well. Capital One, which was early to the game, continues to refine its methods for segmenting credit card customers and for tailoring products to individual risk profiles. According to Nigel Morris, one of Capital One’s cofounders, the company’s multifunctional teams of financial analysts, IT specialists, and marketers conduct more than 65,000 tests each year, experimenting with combinations of market segments and new products.
        
        Companies selling physical products are also using big data for rigorous experimentation. The ability to marshal customer data has kept Tesco, for example, in the ranks of leading UK grocers. This brick-and-mortar retailer gathers transaction data on its ten million customers through a loyalty card program. It then uses the information to analyze new business opportunities—for example, how to create the most effective promotions for specific customer segments—and to inform decisions on pricing, promotions, and shelf allocation. The online grocer Fresh Direct shrinks reaction times even further: it adjusts prices and promotions daily or even more frequently, based on data feeds from online transactions, visits by consumers to its Web site, and customer service interactions. Other companies too are mining data from social networks in real time. Ford Motor, PepsiCo, and Southwest Airlines, for instance, analyze consumer postings about them on social-media sites such as Facebook and Twitter to gauge the immediate impact of their marketing campaigns and to understand how consumer sentiment about their brands is changing.
        
        Using experimentation and big data as essential components of management decision making requires new capabilities, as well as organizational and cultural change. Most companies are far from accessing all the available data. Some haven’t even mastered the technologies needed to capture and analyze the valuable information they can access. More commonly, they don’t have the right talent and processes to design experiments and extract business value from big data, which require changes in the way many executives now make decisions: trusting instincts and experience over experimentation and rigorous analysis. To get managers at all echelons to accept the value of experimentation, senior leaders must buy into a “test and learn” mind-set and then serve as role models for their teams.
        
        Podcast: According to Hal Varian, Google’s chief economist, companies that take advantage of “big data” and the new opportunities for experimentation that technology affords will gain a significant competitive edge.
        
        Stefan Thomke, “Enlightened experimentation: The new imperative for innovation,” Harvard Business Review, February 2001, Volume 79, Number 2, pp. 66–75.
        
        Thomas H. Davenport, Jeanne G. Harris, and Robert Morison, Analytics at Work: Smarter Decisions, Better Results, Cambridge, MA: Harvard Business Press, 2010.
        
        Janaki Akella, Timo Kubach, Markus Löffler, and Uwe Schmid, “Data-driven management: Bringing more science into management,” McKinsey Technology Initiative white paper.
        
        Even as regulatory frameworks continue to evolve, environmental stewardship and sustainability clearly are C-level agenda topics. What’s more, sustainability is fast becoming an important corporate-performance metric—one that stakeholders, outside influencers, and even financial markets have begun to track. Information technology plays a dual role in this debate: it is both a significant source of environmental emissions and a key enabler of many strategies to mitigate environmental damage. At present, information technology’s share of the world’s environmental footprint is growing because of the ever-increasing demand for IT capacity and services. Electricity produced to power the world’s data centers generates greenhouse gases on the scale of countries such as Argentina or the Netherlands, and these emissions could increase fourfold by 2020. McKinsey research has shown, however, that the use of IT in areas such as smart power grids, efficient buildings, and better logistics planning could eliminate five times the carbon emissions that the IT industry produces.
        
        Companies are now taking the first steps to reduce the environmental impact of their IT. For instance, businesses are adopting “green data center” technologies to reduce sharply the energy demand of the ever-multiplying numbers of servers needed to cope with data generated by trends such as distributed cocreation and the Internet of Things (described earlier in this article). Such technologies include virtualization software (which enables the more efficient allocation of software across servers) to decrease the number of servers needed for operations, the cooling of data centers with ambient air to cut energy consumption, and inexpensive, renewable hydroelectric power (which of course requires locating data centers in places where it is available). Meanwhile, IT manufacturers are organizing programs to collect and recycle hazardous electronics, diverting them from the waste stream.
        
        IT’s bigger role, however, lies in its ability to reduce environmental stress from broader corporate and economic activities. In a significant push, for example, utilities around the world are deploying smart meters that can help customers shift electricity usage away from peak periods and thereby reduce the amount of power generated by inefficient and costly peak-load facilities. Smart grids can also improve the efficiency of the transmission and distribution of energy and, when coupled with energy storage facilities, could store electricity generated by renewable-energy technologies, such as solar and wind. Likewise, smart buildings embedded with IT that monitors and optimizes energy use could be one of the most important ways of reducing energy consumption in developed economies. And powerful analytic software that improves logistics and routing for planes, trains, and trucks is already reducing the transportation industry’s environmental footprint.
        
        Within the enterprise, both leaders and key functional players must understand sustainability’s growing importance to broader goals. Management systems that build the constant improvement of resource use into an organization’s processes and strategies will raise its standing with external stakeholders while also helping the bottom line.
        
        Podcast: Microsoft’s chief environmental strategist, Rob Bernard, says that existing technologies hold enormous, latent potential to boost energy efficiency—but not without substantial changes in human behavior.
        
        Podcast: Collaboration across industry boundaries, says McKinsey’s Markus Löffler, is critical to forging the technology innovations needed for sustainable growth.
        
        Giulio Boccaletti, Markus Löffler, and Jeremy M. Oppenheim, “How IT can cut carbon emissions,” October 2008.
        
        William Forrest, James M. Kaplan, and Noah Kindler, “Data centers: How to cut carbon emissions and costs,” mckinseyquarterly.com, November 2008.
        
        Technology now enables companies to monitor, measure, customize, and bill for asset use at a much more fine-grained level than ever before. Asset owners can therefore create services around what have traditionally been sold as products. Business-to-business (B2B) customers like these service offerings because they allow companies to purchase units of a service and to account for them as a variable cost rather than undertake large capital investments. Consumers also like this “paying only for what you use” model, which helps them avoid large expenditures, as well as the hassles of buying and maintaining a product.
        
        In the IT industry, the growth of “cloud computing” (accessing computer resources provided through networks rather than running software or storing data on a local computer) exemplifies this shift. Consumer acceptance of Web-based cloud services for everything from e-mail to video is of course becoming universal, and companies are following suit. Software as a service (SaaS), which enables organizations to access services such as customer relationship management, is growing at a 17 percent annual rate. The biotechnology company Genentech, for example, uses Google Apps for e-mail and to create documents and spreadsheets, bypassing capital investments in servers and software licenses. This development has created a wave of computing capabilities delivered as a service, including infrastructure, platform, applications, and content. And vendors are competing, with innovation and new business models, to match the needs of different customers.
        
        Beyond the IT industry, many urban consumers are drawn to the idea of buying transportation services by the hour rather than purchasing autos. City CarShare and ZipCar were first movers in this market, but established car rental companies, spurred by annual growth rates of 25 percent, are also entering it. Similarly, jet engine manufacturers have made physical assets a platform for delivering units of thrust billed as a service.
        
        A number of companies are employing technology to market salable services from business capabilities they first developed for their own purposes. That’s a trend we previously described as “unbundled production.” More deals are unfolding as companies move to disaggregate and make money from corporate value chains. British Airways and GE, for instance, have spun off their successful business-process-outsourcing businesses, based in India, as separate corporations.
        
        Business leaders should be alert to opportunities for transforming product offerings into services, because their competitors will undoubtedly be exploring these avenues. In this disruptive view of assets, physical and intellectual capital combine to create platforms for a new array of service offerings. But innovating in services, where the end user is an integral part of the system, requires a mind-set fundamentally different from the one involved in designing products.
        
        Nicholas Carr, The Big Switch: Rewiring the World, from Edison to Google, reprint edition, New York, NY: W. W. Norton & Company, 2009.
        
        IBM and University of Cambridge, “Succeeding through service innovation: A service perspective for education, research, business and government,” Cambridge Service Science, Management, and Engineering Symposium, Cambridge, July 14–15, 2007.
        
        Multisided business models create value through interactions among multiple players rather than traditional one-on-one transactions or information exchanges. In the media industry, advertising is a classic example of how these models work. Newspapers, magazines, and television stations offer content to their audiences while generating a significant portion of their revenues from third parties: advertisers. Other revenue, often through subscriptions, comes directly from consumers. More recently, this advertising-supported model has proliferated on the Internet, underwriting Web content sites, as well as services such as search and e-mail (see trend number seven, “Imagining anything as a service,” earlier in this article). It is now spreading to new markets, such as enterprise software: Spiceworks offers IT-management applications to 950,000 users at no cost, while it collects advertising from B2B companies that want access to IT professionals.
        
        Technology is propagating new, equally powerful forms of multisided business models. In some information businesses, for example, data gathered from one set of users generate revenue when the business charges a separate set of customers for information services based on that data. Take Sermo, an online community of physicians who join (free of charge) to pose questions to other members, participate in discussion groups, and read medical articles. Third parties such as pharmaceutical companies, health care organizations, financial institutions, and government bodies pay for access to the anonymous interactions and polls of Sermo’s members.
        
        As more people migrate to online activities, network effects can magnify the value of multisided business models. The “freemium” model is a case in point: a group of customers gets free services supported by those who pay a premium for special use. Flickr (online storage of photos), Pandora (online music), and Skype (online communication) not only use this kind of cross-subsidization but also demonstrate the leveraging effect of networks—the greater the number of free users, the more valuable the service becomes for all customers. Pandora harnesses the massive amounts of data from its free users to refine its music recommendations. All Flickr users benefit from a larger photo-posting community, all Skype members from an expanded universe of people with whom to connect.
        
        Other companies find that when their core business is part of a network, valuable data (sometimes called “exhaust data”) are generated as a by-product. MasterCard, for instance, has built an advisory unit based on data the company gathers from its core credit card business: it analyzes consumer purchasing patterns and sells aggregated findings to merchants and others that want a better reading on buying trends. CHEP, a logistics-services provider, captures data on a significant portion of the transportation volume of the fastest-moving consumer goods and is now building a transportation-management business to take advantage of this visibility.
        
        Not all companies, of course, could benefit from multisided models. But for those that can, a good starting point for testing them is to take inventory of all the data in a company’s businesses (including data flowing from customer interactions) and then ask, “Who might find this information valuable?” Another provocative thought: “What would happen if we provided our product or service free of charge?” or—more important, perhaps—“What if a competitor did so?” The responses should provide indications of the opportunities for disruption, as well as of vulnerabilities.
        
        Podcast: New Web technologies are expanding the scope and power of “free” business models, argues McKinsey’s Michael Chui.
        
        Chris Anderson, Free: How Today’s Smartest Businesses Profit by Giving Something for Nothing, New York, NY: Hyperion, 2009.
        
        Annabelle Gawer ed., Platforms, Markets and Innovation, Cheltenham, UK: Edward Elgar Publishing, 2010.
        
        David S. Evans, Andrei Hagiu, and Richard Schmalensee, Invisible Engines: How Software Platforms Drive Innovation and Transform Industries, Cambridge, MA: The MIT Press, 2006.
        
        The adoption of technology is a global phenomenon, and the intensity of its usage is particularly impressive in emerging markets. Our research has shown that disruptive business models arise when technology combines with extreme market conditions, such as customer demand for very low price points, poor infrastructure, hard-to-access suppliers, and low cost curves for talent. With an economic recovery beginning to take hold in some parts of the world, high rates of growth have resumed in many developing nations, and we’re seeing companies built around the new models emerging as global players. Many multinationals, meanwhile, are only starting to think about developing markets as wellsprings of technology-enabled innovation rather than as traditional manufacturing hubs.
        
        In parts of rural Africa, for instance, traditional retail-banking models have difficulty taking root. Consumers have low incomes and often lack the standard documentation (such as ID cards or even addresses) required to open bank accounts. But Safaricom, a telecom provider, offers banking services to eight million Africans through its M-PESA mobile-phone service (M stands for “mobile,” pesa is Swahili for “money”). Safaricom allows a network of shops and gas stations that sell telecommunications airtime to load virtual cash onto cell phones as well.
        
        In China, another technology-based model brings order to the vast, highly dispersed strata of smaller manufacturing facilities. Many small businesses around the world have difficulty finding Chinese manufacturers to meet specific needs. Some of these manufacturers are located in remote areas, and their capabilities can vary widely. Alibaba, China’s leading B2B exchange, with more than 30 million members, helps members share data on their manufacturing services with potential customers and handles online payments and other transactions. Its network, in effect, offers Chinese manufacturing capacity as a service, enabling small businesses anywhere in the world to identify suppliers quickly and scale up rapidly to meet demand.
        
        Hundreds of companies are now appearing on the global scene from emerging markets, with offerings ranging from a low-cost bespoke tutoring service to the remote monitoring of sophisticated air-conditioning systems around the world. For most global incumbents, these represent a new type of competitor: they are not only challenging the dominant players’ growth plans in developing markets but also exporting their extreme models to developed ones. To respond, global players must plug into the local networks of entrepreneurs, fast-growing businesses, suppliers, investors, and influencers spawning such disruptions. Some global companies, such as GE, are locating research centers in these cauldrons of creativity to spur their own innovations there. Others, such as Philips and SAP, are now investing in local companies to nurture new, innovative products for export that complement their core businesses.
        
        Podcast: Vijay Govindarajan, the Earl C. Daum 1924 Professor of International Business at Dartmouth’s Tuck School of Business, explains why innovative business models arising in emerging markets present both opportunities and perils for established global players.
        
        Jeffrey R. Immelt, Vijay Govindarajan, and Chris Trimble, “How GE is disrupting itself,” Harvard Business Review, October 2009, Volume 87, Number 10, pp. 56–65.
        
        “Special report on innovation in emerging markets: The world turned upside down,” the Economist, April 15, 2010.
        
        C. K. Prahalad, The Fortune at the Bottom of the Pyramid: Eradicating Poverty Through Profits, fifth edition, Philadelphia, PA: Wharton School Publishing, July 2009.
        
        The role of governments in shaping global economic policy will expand in coming years. Technology will be an important factor in this evolution by facilitating the creation of new types of public goods while helping to manage them more effectively. This last trend is broad in scope and draws upon many of the other trends described above.
        
        Take the challenges of rising urbanization. About half of the world’s people now live in urban areas, and that share is projected to rise to 70 percent by 2050. Creative public policies that incorporate new technologies could help ease the economic and social strains of population density. “Wired” cities might be one approach. London, Singapore, and Stockholm have used smart assets to manage traffic congestion in their urban cores, and many cities throughout the world are deploying these technologies to improve the reliability and predictability of mass-transit systems. Sensors in buses and trains provide transportation planners with real-time status reports to optimize routing and give riders tools to adjust their commuting plans.
        
        Similarly, networked smart water grids will be critical to address the need for clean water. Embedded sensors can not only ensure that the water flowing through systems is uncontaminated and safe to drink but also sense leaks. And effective metering and billing for water ensures that the appropriate incentives are in place for efficient usage.
        
        Technology can also improve the delivery and effectiveness of many public services. Law-enforcement agencies are using smart assets—video cameras and data analytics—to create maps that define high-crime zones and direct additional police resources to them. Cloud computing and collaboration technologies can improve educational services, giving young and adult students alike access to low-cost content, online instructors, and communities of fellow learners. Through the Web, governments are improving access to many other services, such as tax filing, vehicle registration, benefits administration, and employment services. Public policy also stands to become more transparent and effective thanks to a number of new open-data initiatives. At the UK Web site FixMyStreet.com, for example, citizens report, view, and discuss local problems, such as graffiti and the illegal dumping of waste, and interact with local officials who provide updates on actions to solve them.
        
        Exploiting technology’s full potential in the public sphere means reimagining the way public goods are created, delivered, and managed. Setting out a bold vision for what a wired, smart community could accomplish is a starting point for setting strategy. Putting that vision in place requires forward-thinking yet prudent leadership that sets milestones, adopts flexible test-and-learn methods, and measures success. Inertia hobbles many public organizations, so leaders must craft incentives tailored to public projects and embrace novel, unfamiliar collaborations among governments, technology providers, other businesses, nongovernmental organizations, and citizens.
        
        Jason Baumgarten and Michael Chui, “E-government 2.0,” McKinsey on Government, Number 4, Summer 2009.
        
        Bas Boorsma and Wolfgang Wagner, “Connected urban development: Innovation for sustainability,” NATOA Journal, Winter 2007, Volume 15, Number 4, pp. 5–9.
        
        The pace of technology and business change will only accelerate, and the impact of the trends above will broaden and deepen. For some organizations, they will unlock significant competitive advantages; for others, dealing with the disruption they bring will be a major challenge. Our broad message is that organizations should incorporate an understanding of the trends into their strategic thinking to help identify new market opportunities, invent new ways of doing business, and compete with an ever-growing number of innovative rivals.In most organizations, information travels along familiar routes. Proprietary information is lodged in databases and analyzed in reports and then rises up the management chain. Information also originates externally—gathered from public sources, harvested from the Internet, or purchased from information suppliers.
        
        But the predictable pathways of information are changing: the physical world itself is becoming a type of information system. In what’s called the Internet of Things, sensors and actuators embedded in physical objects—from roadways to pacemakers—are linked through wired and wireless networks, often using the same Internet Protocol (IP) that connects the Internet. These networks churn out huge volumes of data that flow to computers for analysis. When objects can both sense the environment and communicate, they become tools for understanding complexity and responding to it swiftly. What’s revolutionary in all this is that these physical information systems are now beginning to be deployed, and some of them even work largely without human intervention.
        
        Pill-shaped microcameras already traverse the human digestive tract and send back thousands of images to pinpoint sources of illness. Precision farming equipment with wireless links to data collected from remote satellites and ground sensors can take into account crop conditions and adjust the way each individual part of a field is farmed—for instance, by spreading extra fertilizer on areas that need more nutrients. Billboards in Japan peer back at passersby, assessing how they fit consumer profiles, and instantly change displayed messages based on those assessments.
        
        00:00 Podcast When virtual-world capabilities meet real-world businesses McKinsey’s Michael Chui discusses how an Internet of Things, such as sensors and network technology, is changing company processes and consumer interactions—and even entire business models.
        
        Yes, there are traces of futurism in some of this and early warnings for companies too. Business models based on today’s largely static information architectures face challenges as new ways of creating value arise. When a customer’s buying preferences are sensed in real time at a specific location, dynamic pricing may increase the odds of a purchase. Knowing how often or intensively a product is used can create additional options—usage fees rather than outright sale, for example. Manufacturing processes studded with a multitude of sensors can be controlled more precisely, raising efficiency. And when operating environments are monitored continuously for hazards or when objects can take corrective action to avoid damage, risks and costs diminish. Companies that take advantage of these capabilities stand to gain against competitors that don’t.
        
        The widespread adoption of the Internet of Things will take time, but the time line is advancing thanks to improvements in underlying technologies. Advances in wireless networking technology and the greater standardization of communications protocols make it possible to collect data from these sensors almost anywhere at any time. Ever-smaller silicon chips for this purpose are gaining new capabilities, while costs, following the pattern of Moore’s Law, are falling. Massive increases in storage and computing power, some of it available via cloud computing, make number crunching possible at very large scale and at declining cost.
        
        None of this is news to technology companies and those on the frontier of adoption. But as these technologies mature, the range of corporate deployments will increase. Now is the time for executives across all industries to structure their thoughts about the potential impact and opportunities likely to emerge from the Internet of Things. We see six distinct types of emerging applications, which fall in two broad categories: first, information and analysis and, second, automation and control (exhibit).
        
        Exhibit Six distinct types of applications are emerging in two broad categories: information and analysis and automation and control. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        As the new networks link data from products, company assets, or the operating environment, they will generate better information and analysis, which can enhance decision making significantly. Some organizations are starting to deploy these applications in targeted areas, while more radical and demanding uses are still in the conceptual or experimental stages.
        
        When products are embedded with sensors, companies can track the movements of these products and even monitor interactions with them. Business models can be fine-tuned to take advantage of this behavioral data. Some insurance companies, for example, are offering to install location sensors in customers’ cars. That allows these companies to base the price of policies on how a car is driven as well as where it travels. Pricing can be customized to the actual risks of operating a vehicle rather than based on proxies such as a driver’s age, gender, or place of residence.
        
        Or consider the possibilities when sensors and network connections are embedded in a rental car: it can be leased for short time spans to registered members of a car service, rental centers become unnecessary, and each car’s use can be optimized for higher revenues. Zipcar has pioneered this model, and more established car rental companies are starting to follow. In retailing, sensors that note shoppers’ profile data (stored in their membership cards) can help close purchases by providing additional information or offering discounts at the point of sale. Market leaders such as Tesco are at the forefront of these uses.
        
        In the business-to-business marketplace, one well-known application of the Internet of Things involves using sensors to track RFID (radio-frequency identification) tags placed on products moving through supply chains, thus improving inventory management while reducing working capital and logistics costs. The range of possible uses for tracking is expanding. In the aviation industry, sensor technologies are spurring new business models. Manufacturers of jet engines retain ownership of their products while charging airlines for the amount of thrust used. Airplane manufacturers are building airframes with networked sensors that send continuous data on product wear and tear to their computers, allowing for proactive maintenance and reducing unplanned downtime.
        
        Data from large numbers of sensors, deployed in infrastructure (such as roads and buildings) or to report on environmental conditions (including soil moisture, ocean currents, or weather), can give decision makers a heightened awareness of real-time events, particularly when the sensors are used with advanced display or visualization technologies.
        
        Security personnel, for instance, can use sensor networks that combine video, audio, and vibration detectors to spot unauthorized individuals who enter restricted areas. Some advanced security systems already use elements of these technologies, but more far-reaching applications are in the works as sensors become smaller and more powerful, and software systems more adept at analyzing and displaying captured information. Logistics managers for airlines and trucking lines already are tapping some early capabilities to get up-to-the-second knowledge of weather conditions, traffic patterns, and vehicle locations. In this way, these managers are increasing their ability to make constant routing adjustments that reduce congestion costs and increase a network’s effective capacity. In another application, law-enforcement officers can get instantaneous data from sonic sensors that are able to pinpoint the location of gunfire.
        
        The Internet of Things also can support longer-range, more complex human planning and decision making. The technology requirements—tremendous storage and computing resources linked with advanced software systems that generate a variety of graphical displays for analyzing data—rise accordingly.
        
        In the oil and gas industry, for instance, the next phase of exploration and development could rely on extensive sensor networks placed in the earth’s crust to produce more accurate readings of the location, structure, and dimensions of potential fields than current data-driven methods allow. The payoff: lower development costs and improved oil flows.
        
        As for retailing, some companies are studying ways to gather and process data from thousands of shoppers as they journey through stores. Sensor readings and videos note how long they linger at individual displays and record what they ultimately buy. Simulations based on this data will help to increase revenues by optimizing retail layouts.
        
        In health care, sensors and data links offer possibilities for monitoring a patient’s behavior and symptoms in real time and at relatively low cost, allowing physicians to better diagnose disease and prescribe tailored treatment regimens. Patients with chronic illnesses, for example, have been outfitted with sensors in a small number of health care trials currently under way, so that their conditions can be monitored continuously as they go about their daily activities. One such trial has enrolled patients with congestive heart failure. These patients are typically monitored only during periodic physician office visits for weight, blood pressure, and heart rate and rhythm. Sensors placed on the patient can now monitor many of these signs remotely and continuously, giving practitioners early warning of conditions that would otherwise lead to unplanned hospitalizations and expensive emergency care. Better management of congestive heart failure alone could reduce hospitalization and treatment costs by a billion dollars annually in the United States.
        
        Making data the basis for automation and control means converting the data and analysis collected through the Internet of Things into instructions that feed back through the network to actuators that in turn modify processes. Closing the loop from data to automated applications can raise productivity, as systems that adjust automatically to complex situations make many human interventions unnecessary. Early adopters are ushering in relatively basic applications that provide a fairly immediate payoff. Advanced automated systems will be adopted by organizations as these technologies develop further.
        
        The Internet of Things is opening new frontiers for improving processes. Some industries, such as chemical production, are installing legions of sensors to bring much greater granularity to monitoring. These sensors feed data to computers, which in turn analyze them and then send signals to actuators that adjust processes—for example, by modifying ingredient mixtures, temperatures, or pressures. Sensors and actuators can also be used to change the position of a physical object as it moves down an assembly line, ensuring that it arrives at machine tools in an optimum position (small deviations in the position of work in process can jam or even damage machine tools). This improved instrumentation, multiplied hundreds of times during an entire process, allows for major reductions in waste, energy costs, and human intervention.
        
        In the pulp and paper industry, for example, the need for frequent manual temperature adjustments in lime kilns limits productivity gains. One company raised production 5 percent by using embedded temperature sensors whose data is used to automatically adjust a kiln flame’s shape and intensity. Reducing temperature variance to near zero improved product quality and eliminated the need for frequent operator intervention.
        
        Networked sensors and automated feedback mechanisms can change usage patterns for scarce resources, including energy and water, often by enabling more dynamic pricing. Utilities such as Enel in Italy and Pacific Gas and Electric (PG&E) in the United States, for example, are deploying “smart” meters that provide residential and industrial customers with visual displays showing energy usage and the real-time costs of providing it. (The traditional residential fixed-price-per-kilowatt-hour billing masks the fact that the cost of producing energy varies substantially throughout the day.) Based on time-of-use pricing and better information residential consumers could shut down air conditioners or delay running dishwashers during peak times. Commercial customers can shift energy-intensive processes and production away from high-priced periods of peak energy demand to low-priced off-peak hours.
        
        Data centers, which are among the fastest-growing segments of global energy demand, are starting to adopt power-management techniques tied to information feedback. Power consumption is often half of a typical facility’s total lifetime cost, but most managers lack a detailed view of energy consumption patterns. Getting such a view isn’t easy, since the energy usage of servers spikes at various times, depending on workloads. Furthermore, many servers draw some power 24/7 but are used mostly at minimal capacity, since they are tied to specific operations. Manufacturers have developed sensors that monitor each server’s power use, employing software that balances computing loads and eliminates the need for underused servers and storage devices. Greenfield data centers already are adopting such technologies, which could become standard features of data center infrastructure within a few years.
        
        The most demanding use of the Internet of Things involves the rapid, real-time sensing of unpredictable conditions and instantaneous responses guided by automated systems. This kind of machine decision making mimics human reactions, though at vastly enhanced performance levels. The automobile industry, for instance, is stepping up the development of systems that can detect imminent collisions and take evasive action. Certain basic applications, such as automatic braking systems, are available in high-end autos. The potential accident reduction savings flowing from wider deployment could surpass $100 billion annually. Some companies and research organizations are experimenting with a form of automotive autopilot for networked vehicles driven in coordinated patterns at highway speeds. This technology would reduce the number of “phantom jams” caused by small disturbances (such as suddenly illuminated brake lights) that cascade into traffic bottlenecks.
        
        Scientists in other industries are testing swarms of robots that maintain facilities or clean up toxic waste, and systems under study in the defense sector would coordinate the movements of groups of unmanned aircraft. While such autonomous systems will be challenging to develop and perfect, they promise major gains in safety, risk, and costs. These experiments could also spur fresh thinking about how to tackle tasks in inhospitable physical environments (such as deep water, wars, and contaminated areas) that are difficult or dangerous for humans.
        
        The Internet of Things has great promise, yet business, policy, and technical challenges must be tackled before these systems are widely embraced. Early adopters will need to prove that the new sensor-driven business models create superior value. Industry groups and government regulators should study rules on data privacy and data security, particularly for uses that touch on sensitive consumer information. Legal liability frameworks for the bad decisions of automated systems will have to be established by governments, companies, and risk analysts, in consort with insurers. On the technology side, the cost of sensors and actuators must fall to levels that will spark widespread use. Networking technologies and the standards that support them must evolve to the point where data can flow freely among sensors, computers, and actuators. Software to aggregate and analyze data, as well as graphic display techniques, must improve to the point where huge volumes of data can be absorbed by human decision makers or synthesized to guide automated systems more appropriately.
        
        Within companies, big changes in information patterns will have implications for organizational structures, as well as for the way decisions are made, operations are managed, and processes are conceived. Product development, for example, will need to reflect far greater possibilities for capturing and analyzing information.
        
        Companies can begin taking steps now to position themselves for these changes by using the new technologies to optimize business processes in which traditional approaches have not brought satisfactory returns. Energy consumption efficiency and process optimization are good early targets. Experiments with the emerging technologies should be conducted in development labs and in small-scale pilot trials, and established companies can seek partnerships with innovative technology suppliers creating Internet-of-Things capabilities for target industries.Science has permeated marketing for decades. Fans of the television drama Mad Men saw a fictionalized encounter when an IBM System/360 mainframe computer physically displaced the creative department of a late-1960s advertising agency. In reality, though, the 1960s through the early 1990s witnessed a happy marriage of advertising and technology as marketers mastered both the medium of television and the science of Nielsen ratings. These years gave birth to iconic advertising messages in categories ranging from sparkling beverages (“I’d like to buy the world a Coke”) to credit cards (“American Express. Don’t leave home without it”) to air travel (“British Airways: the world’s favourite airline”).
        
        Until recently, marketers could be forgiven for looking back wistfully at this golden age as new forces reshaped their world into something completely different. These new trends include a massive proliferation of television and online channels, the transformation of the home PC into a retail channel, the unrelenting rise of mobile social media and gaming, and—with all these trends—a constant battle for the consumer’s attention.
        
        The resulting expansion of platforms has propelled consistent growth in marketing expenditures, which now total as much as $1 trillion globally. The efficacy of this spending is under deep scrutiny. For example, in a survey of CEOs, close to three out of four agreed with the following statement: marketers “are always asking for more money, but can rarely explain how much incremental business this money will generate.” Chief marketing officers (CMOs), it appears, don’t disagree: in another recent survey, just over one-third said they had quantitatively proved the impact of their marketing outlays. Paradoxically, though, CEOs are looking to their CMOs more than ever, because they need top-line growth and view marketing as a critical lever to help them achieve it. Can marketers deliver amid ongoing performance pressures?
        
        In this article, we’ll explain why we think the answer is yes—and why we are, in fact, on the cusp of a new golden age for marketing. At the core of the new era are five elements that are simultaneously familiar and fast changing. The first two are the science and substance of marketing. Leading marketers are using research and analytics to shed light on who buys what, and why; who influences buyers; and when, in the consumer decision journey, marketing efforts are likely to yield the greatest return. That understanding, in turn, is making it possible for marketers to identify more effectively the functional benefits that customers need, the experiences they want, and the innovations they will value.
        
        But this isn’t just another missive on the power of big data. Organizational simplicity is fueling speed, and story is pulling things together while inspiring both the customer and the organization. Happily, the story just seems to get better as creative minds express themselves through digital means, and it then echoes and expands through social media and user-generated content. As you’ll see, the emerging new rules for marketing extend well beyond data and analysis, crucial though those are, and even transcend the marketing organization itself.
        
        Advances in data, modeling, and automated analysis are creating ever more refined ways of targeting and measuring the returns on marketing investments, while generating powerful new clues about why consumers behave as they do. Long gone is spending guided mostly by intuition and focus groups. Instead, organizations are seeking greater precision by measuring and managing the consumer decision points where well-timed outlays can make the biggest difference.
        
        Big data is a term that’s often used to describe this transition. But it’s not just big data; it’s also big research. A major consumer company investigating the decision journey for its products recently undertook a consumer study, collected through online surveys, on a massive scale and at a speed that would have been unimaginable in the days of mall-intercept interviews. The project, which involved more than 10,000 surveys over the course of a month, uncovered material differences between how the company and consumers were thinking about the category, while also explaining what drives choice at each stage of the journey. These insights are now being used to change brand strategy, product-portfolio design, and marketing campaigns. The potential impact runs into billions of dollars in additional revenue.
        
        While much recent marketing science has played out in the measurement and targeting of advertising and promotion expenditures, many consumer companies are increasing their focus on in-store behavior: how promotions, traffic flows, and physical engagement with products affect sales. Capturing and analyzing data on such issues has become more feasible in recent years thanks to low-cost sensors that can be embedded in products, as well as the ability to capture and analyze huge amounts of unstructured data from store videos—and even to track shoppers’ eye movements.
        
        The impact goes beyond marketing and product teams. Marketing science is boosting the precision of real-time operating decisions. At a major hospitality company, marketing analysts are able to get a read on the performance of a particular property or category over a weekend and then drill down on individual customer segments to assess how to make improvements. If the data show that a profitable segment of weekend travelers are shortening their stays, the company can create special offers (such as late checkouts or room upgrades) to encourage repeat business. Or consider how one industrial-products company revamped its highly fragmented portfolio of more than 500 SKUs sold to customers in a diverse set of industries. Prices varied widely even for the same products, without any clear reasons as to why, hindering efforts to manage margins. An analytical tool that could scan 1.3 million transactions helped the company redraw customer segments, identify products with opportunities for pricing flexibility, and recommend new prices. Ultimately, it reset about 100,000 price points.
        
        More scientific marketing means that CMOs and other senior leaders need enhanced analytical skills to exploit data possibilities more fully and stay ahead of the whirl of developments. One CEO we know believes it’s time to create a position—marketing technology officer (MTO)—that’s rooted both in technology and domain knowledge. Knowing what can be automated, when judgment is required, and where to seek and place new technical talent are becoming increasingly central to effective marketing leadership. That is intensifying the war for specialized talent as traditional marketing powerhouses bid against high-tech companies for needed skills.
        
        As more advanced marketing science and analytics take hold, they are making it increasingly natural for marketing to go beyond messaging and to shape the substance of the business, particularly the experiences of customers, the delivery of functional benefits, and the drive to develop new products and services. Armed with information about customers and a company’s relationships with them, the CMO is well-positioned to help differentiate its products, services, and experiences.
        
        That’s good, because digital innovation, transparency, and customer-centricity have raised expectations across the board. In automobiles, as sensor technologies proliferate and onboard computing power increases, consumers are now starting to expect that collision-avoidance and digitally-enabled safety systems will become part of manufacturers’ offerings. (Luxury carmakers already are making sophisticated safety options part of their marketing story.) In retail, brands like H&M, Topshop, Uniqlo, and Zara have harnessed the consumer’s desire to have it all by bringing mass-market prices to the colors, fabrics, and designs of high fashion. Simultaneously, Amazon and other digital players are pressuring brick-and-mortar retailers, which are responding both by retooling their supply chains to enable faster restocking and one-day delivery and by creating new advertising messages around the in-store pickup of online orders.
        
        Marketers are well placed to help their organizations meet the rising bar by, for example, making the case for customer-care initiatives and for consistency in the customer experience. A better one became the heart of a marketing campaign at European energy supplier Essent, a subsidiary of RWE. To ensure that the company delivered on the promise, the CEO named the chief of marketing to lead the initiative. Among the successes: making customer onboarding less cumbersome by cutting process steps from seven to two. Marketing also took the lead in efforts to create new products that customers wanted. The CMO led a cross-functional team of sales, IT, and product development to produce Essent’s smart, Internet-connected E-thermostat, for instance. Some of its functionality was cocreated with customers.
        
        Similarly, marketing has taken a leadership role in designing and setting standards for Daimler’s highly digital customer-experience brand, “Mercedes me.” The digital platform provides customers with automated appointment booking, personalized financing, a chance to cocreate ideas, access to maintenance data from sensor-enabled automobile diagnostics, and even quick access to Daimler’s car-sharing and taxi services—for use on business trips, for example. (See “Marketing the Mercedes way” for more on the role of marketing at the company.)
        
        These efforts and many more like them are extending marketing into the guts of the business, and most would not have been possible just a few years ago. The power of today’s digital tools and the scientific approaches they make possible are not only enabling a more substantial role for marketing but also giving it opportunities for real-time impact.
        
        Even as marketing reaches new heights with technology-enabled measurement, the importance of the story hasn’t diminished. But ways to tell it are morphing continually as the stuff of storytelling encompasses richer digital interactions, and mobile devices become more powerful communications tools. In this world, creativity is in greater demand than ever.
        
        Google’s “Dear Sophie” advertisement is an example of the modern art form. It tells the story of a father writing to his daughter as she grows up, with the narrative demonstrating how Google search, Gmail, and YouTube can be new channels of human connectivity. (For more on how Google seeks to connect, see “How Google breaks through.”) P&G’s “Pick Them Back Up” spot for the Sochi Olympics (part of the ongoing “Thank You, Mom” campaign) is another moving story. It dramatizes the moms who were there for their kids throughout the years of hard training, who picked them up when they fell, and who deserve celebration as the unsung heroines. It’s hard to watch these commercials and not tear up, at least a little.
        
        Chanel’s recent launch of the new No. 5 perfume offers a good window on how stories are evolving beyond traditional video. Over a decade after their first collaboration, creative chief Karl Lagerfeld has again partnered with film director Baz Luhrmann to produce a short film on a woman whose lifestyle embodies the brand. Their latest effort—“The one that I want”—stars model Gisele Bündchen and features the perfume, along with clothing and other Chanel products. Beyond the film itself, a series of YouTube videos extend the campaign with shorts on the making of the film, interviews with Luhrmann on both projects, behind-the-scenes footage from Chanel’s studio, and more. All of this is designed to amplify the lifestyle message of the fragrance’s launch in a way that traditional TV or print couldn’t accomplish.
        
        New media also dictate that marketers relinquish control of the story as digital interactions with customers become more frequent. Customers want to interact with stories and modify them on social media. Following the kinds of story rules that once made board members and CEOs comfortable is no longer feasible. Social-media programs are consuming a larger share of many marketing budgets. A number of major consumer companies are using interaction centers to monitor and participate in social-media conversations as they develop, sometimes including the promotion of discussions on corporate social-media channels.
        
        Agency-management issues also are an important piece of the puzzle. Talent scarcity, evolving digital storytelling, and perceived institutional rigidities have opened new debates about the best ways to access creativity. Some companies, like Chanel, are enhancing their control over the story with supplemental digital content. Other global marketing leaders are bringing in-house more of their story muscle, particularly when it involves lighter message content for social media. Agencies are responding. Many are acquiring more digital talent and working to break down silos to overcome perceptions that they are actually geared to bigger productions and may lack the digital and story skills to handle new content in an agile, integrated way. All this is very much in flux, suggesting that leaders who aren’t asking fundamental questions about the roles of (and fit between) agencies and internal marketing teams stand the risk of being left behind.
        
        In a digital economy, marketing is no longer a “batch” process but a continuous one. Consumer preferences change with stunning velocity, as do the dynamics of markets and product life cycles. This culture of urgency means that marketers need a new agility, plus the management skills and organizational clout to bring other functions together at a higher clock speed.
        
        How speed is achieved, of course, will vary by company and industry. A number of CMOs we know are setting the terms of how functional units should collaborate and spelling out what the entire organization needs to know to get new products to market at a stepped-up pace. In these cases, marketing becomes the glue across the organization, providing oversight and coordination.
        
        To speed up its digital tempo, Nestlé’s marketing organization launched digital-acceleration teams. These specialists train business units and functions in the skills needed to be effective in digital marketing and social communications. Nestlé’s country units have adopted the approach, as well, allowing them to adapt the digital training to local market conditions, while adhering to core, company-wide standards.
        
        At Google, lead times for new products are continually shrinking. Internal teams are attuned to putting products in front of consumers and then, in real time, to bringing back insights in a cycle of testing, learning, and iterating. Marketers are central to this process: they work to develop close relationships with product-development teams in order to inject their knowledge of user needs into how products are developed. That helps create a vision of the product from the user’s eyes, and one that engineering teams are eager to create. Achieving that shared vision between product developers and marketers is a key element of speed in formulating new products and features. The time-to-market benefits of better information and more fluid collaboration extend to a wide range of companies, sectors, and business functions. Consider, for example, how data and collaboration are increasing the speed and agility of B2B sales teams. (For more, see “Do you really understand how your business customers buy?”)
        
        Complexity is the enemy of speed, which is a big reason why a number of leading marketers are reforming their organizations. Too often, expanding geographic footprints, product proliferation, and new arrays of channels and digital specialties have led to complex hierarchies, silos, communication gaps, and redundancies. But these can be tamed.
        
        For example, one telecommunications company realized that a cumbersome organizational structure was getting in the way of delivering the top-notch customer service that the CEO had designated as a strategic priority. He created a unit combining existing call centers and a newly formed social-media customer-care group. The leader of the unit reports directly to him. Proximity to the top of the company allows the new team to collaborate more smoothly across the organization, while signaling the importance of the customer experience.
        
        Many consumer marketers are using technology to reduce complexity. They are embracing internal social-media platforms to encourage the generation and sharing of ideas, which helps speed up problem solving across the organization. Daimler, meanwhile, reorganized its marketing and sales departments around the idea of the “best customer experience.” It created a new customer-experience function bundling several headquarters functions into one that maps the entire customer journey, with the goal of locking in a consistent brand experience throughout the world.
        
        Simplifying working relationships with advertising and other media agencies is another goal for many marketing organizations. Trade-offs abound: specialist agencies have expertise in new digital-content formats and delivery channels, but they aren’t always full-service shops. Larger agencies offer more services, but the strengths of many still lie in traditional media. Marketers building teams of employees with strong skills in digital content and delivery are bringing more activities in-house, but bulking up can create complexity and slow things down. And of course, simplicity can’t come at the expense of great creative output.
        
        In our work with global marketers, including many leading-edge practitioners, we are seeing significant progress in each of these five dimensions. As you think about the implications of science, substance, story, speed, and simplicity for your organization, we suggest that you ask yourself five questions:
        
        Are we taking advantage of the science of data and research to uncover new insights, or are we working off yesterday’s facts, assertions, and heuristics? Do we fully exploit the power of marketing to enhance the substance—that is, the products, services, and experiences—we offer our customers, or are we just selling hard with a “me-too” mind-set? Do we have a clear brand story that echoes through cyberspace, or do we feel that we aren’t quite capturing hearts and minds? Have we created simplifiers within our organization, or have complex matrices become a logjam? Are we faster or slower to market than our competition?
        
        Although this may seem like a lot to handle, the rapid changes and fast-breaking opportunities facing marketers in the 21st century suggest to us that the best ones will have good answers to all of these questions. In our opinion, those that do will not only enjoy above-market growth, they will define the next golden age of marketing.Lorraine Twohill has made a career of pushing frontiers and forging connections. A 1992 graduate of Dublin City University, she spent a decade building brands for organizations across Europe. In 2003, Google tapped Twohill for its growing EMEA business, and in the process made her their first non-US marketing hire. Twohill advanced steadily from there; in 2009, she was named global head of marketing and then, in 2014, senior vice president of global marketing.
        
        Twohill recently sat down with McKinsey’s Jonathan Gordon to share her views on a new inflection point in marketing. What new technologies are arising, which best practices are emerging, and what fundamentals still hold true since marketing’s first golden age?
        
        Lorraine Twohill: The core assets that were so important in the first golden age are as important today: a great central thought, great writing, great creativity. Back in the ’60s, TV was coming onboard but all the work was in print. And the brilliance of print is that you have to have a really great thought and great writing. The bar isn’t any lower today. You have to have authenticity, a great central thought. Those same skills that were needed back then are as critical today.
        
        The way I think about marketing—and the way I tend to talk to my team about it—is “knowing the user, knowing the magic, and connecting the two.” Knowing the user means understanding who your consumers are, who your customers are. Not just knowing who they are, but what they need, what are their deep insights, and understanding how we can help them. Knowing the magic means knowing what’s in the hearts and minds of your engineers and your product managers, and what they’re building. Connecting the two means bringing the magic built by engineers to the world in a way that is relevant, meaningful, and compelling to the everyday consumer. So we create something that the world will be excited about.
        
        Lorraine Twohill: The beauty of marketing today is that we can really show the return. The data allows us to demonstrate impact in a much more transparent way than in the past. It’s measurable, and we focus a lot on that. We’re very rigorous about the modeling we put in place and the tracking of our campaigns. Impact matters, results matter, tracking matters.
        
        And I think right now we’re at a very interesting inflection point. The tools available to marketers today are extraordinary. They know far more about their consumers than ever before. They are able to have a much more meaningful, two-way conversation. It’s definitely the golden age for marketing in many ways.
        
        We are excited about the automation of media planning and buying through the use of data and algorithms—what’s known as “programmatic.” I’ve challenged my team to hit a target of 60 percent for our display marketing via programmatic. You still have to define your audience but it is now much simpler to deliver the right message to the right person at the right time with precision. There are fewer wasted ad impressions. It’s also better for users because I’m not frustrating them with ads that aren’t relevant. And since it takes a lot of the grunt work out of media planning, it frees my team to focus on creativity.
        
        Lorraine Twohill: Google has a very data-led culture. But we care just as much about the storytelling and the brand, and how we tell the world about our mission. So I have found that getting the storytelling right—and having the substance and the authenticity in the storytelling—is as respected internally as the return and the impact. And with the use of the analytic tools we have, the storytelling becomes more important than ever. If anything, there’s too much talk about the science right now. I have a colleague who is writing a paper on the future of marketing: it’s data, data, science, science. I’m like, “It’s not!” Or rather, it is those things, yes. But if you fall down on the art, if you fail on the messaging and storytelling, all that those tools will get you are a lot of bad impressions.
        
        The Quarterly: How do you approach the messaging and the storytelling, especially given the challenges of proliferation? How do you break through the clutter?
        
        Lorraine Twohill: We start with the user, and we focus on what we call “one real user.” You have to think about the consumer as a human being. What matters in his or her life. And, honestly, you do not wake up in the morning and think, “I need a new browser today,” for example. You wake up in the morning and worry about getting your kids to school and paying your mortgage and saving for the future.
        
        If we are going to interrupt you with something that we think is important to you, we have to find a way to tell you about it so that it resonates with you. There has to be a benefit to you. There has to be substance. So, we tell real-life stories. We say, “Listen, your life will change because our product will do this.” Or “Your life just got better because now you can have this.” We don’t do the storytelling unless we have that. Before we get into storytelling, we’ll sit with the team and say, “Okay, why does the world need this? What is going to change in a person’s life if they have this? What’s unique about this? What’s truly great about it?” There has to be substance there.
        
        The Quarterly: That’s interesting; we’ve also identified “substance” as one of the leading elements of marketing’s new golden age—along with science, storytelling, speed, and simplicity.
        
        Lorraine Twohill: Substance is really important. And I think that’s what gave marketing a lot of its bad name in the ’80s and ’90s. There was an awful lot of hype without substance. And a lot of exaggeration. You know: big hair, big everything.
        
        Our engineers have a real sense of purpose and they care about building products and features that have substance and will make a meaningful difference in people’s lives; for example, look at the impact of search in giving people all over the world access to information. So that makes my job a lot easier and it gives my team something real to talk about. For example, Gmail launched the promotions and social tabs because we realized everybody’s inboxes were getting flooded with promotional emails and social-media emails. So we created two tabs where they all just immediately go: “Job done.” And people just loved that—a little feature for a mature product that people went nuts over because it was a real pain point. We call them “toothbrush problems.”
        
        Lorraine Twohill: Toothbrush problems—small problems, pain points—like you brush your teeth twice a day. But they are recurring problems, and we should just make them go away. And at the same time, we also look at big problems, like the deaths on the road from cars; whereas if you had driverless cars, that problem would go away. We look at what we can solve, from everyday toothbrush problems all the way up to epic problems.
        
        The Quarterly: Would you say, as well, that achieving the best customer experience means not only getting better at telling stories to the customer, but also getting better at listening?
        
        Lorraine Twohill: Certainly. I think that should be marketing’s role in the company—to really be the champion of the consumer, the face of the user internally, and the guardian of the user’s best interests and the user’s needs.
        
        We can put products in front of people and get consumer insights back almost in real-time. We can test and iterate, test and learn. Even more traditional companies can now exist in the digital world, and be smart about how they use the Internet as a great focus group. You can more quickly get user insights, and reach more people. And we can very quickly get that feedback to the teams as they go through their evolutions of a product. Then we bake that feedback into the product as it gets better.
        
        Lorraine Twohill: Storytelling is the ultimate example of creative judgment. And in my view, the one thing you cannot train marketers on is creative judgment. You can train on most other things. But the folks who have great creative judgment—and you really know it when you see it—are few and far between. You can have principles and guidebooks and frameworks and brand guidelines. You can have the whole kit and caboodle. But just innate gut instinct, brilliant, creative judgment—that’s what we look for, and that’s where you see results.
        
        Lorraine Twohill: Well, you have to look at the world around you. You have to leave the building. Not enough folks do that; too many become very internally focused. They’re in management team meetings; they’re working with cross-functional teams. But you have to go out and look at the world around you—see the people, how they use your products, go into homes, walk around the city. No matter where I go in the world, I don’t just go somewhere for meetings. For example, if I go to Tokyo, I won’t sit in the office for two days and fly back. I’ll take a half a day to walk that city. You don’t understand the idea of paying with a phone until you actually pay with a phone. You walk into any store in Tokyo, beep, and it’s done.
        
        I’ll also take the most junior folks on the team and say, “Where do you hang out, where do you shop? Put me around your neighborhood. Take me to where you buy your groceries. Show me what you’re excited about.” They love this! And I learn so much from them—I come back bursting.
        
        Lorraine Twohill: And a faster one. You know, it’s not a two-year lead time for a Google product. It’s much quicker. Being able to bring insights to the table, consumer insights, in real time and get insights back in—being able to test and iterate, test and iterate—is extremely important. We also like to keep a healthy sense of urgency, the feeling of being on a small, multifaceted team up against the odds. Usually, that’s drawn from our own people: engineers, creative, and product managers. It’s very creative because engineers are creative at heart. And when you bring creatives together with engineers, you get phenomenal ideas and phenomenal thinking.
        
        Lorraine Twohill: It means marketing needs to raise its game. Engineers look at the world in a different way than the rest of us. They see things that are broken and want to fix them. They’re big visionaries, big thinkers, because they have huge imaginations. They think of crazy ideas and go build them. You have to be as good as that. You have a seat at the genius table with people that can code, people that are creative, and are extraordinarily talented. To have a seat there, you need to raise your game.Ola Källenius is a self-confessed “car guy” who still harkens back to being “that kid with the dream of driving that Mercedes star.” The Swedish-born Källenius joined the then Daimler-Benz AG in a management associate program in 1993, was named executive director of McLaren Automotive in 2003, and became a member of the Divisional Board of Mercedes-Benz Cars, responsible for marketing and sales, in 2013. In January 2015, Källenius was appointed to the Board of Management of Daimler AG. At age 45, he is the youngest member on that governing body. In a recent conversation with McKinsey’s Jesko Perrey, Källenius shared his views on what’s driving the future of marketing, particularly at the luxury end.
        
        Ola Källenius: It has become a more challenging game, but I would say that some of the basics are still the same. You need an attitude, a story. You have but two buttons to push—emotion and intelligence, heart and mind.
        
        Ola Källenius: One thing I want to stress, because everyone tends to talk just about the digital side, is that in the world of modern luxury it’s not all digital. It’s human touch. That’s equally important as digital—more important, in a way. Think about other luxury brands, like Hermès and Louis Vuitton. Look what they’re doing: they’re building flagship stores that are beautiful, where you actually like to just browse around before you buy. Those are emotional places. So let’s not believe that, even for younger people, this side does not count. It does. At the same time, people want seamless integration between the physical side and the digital side.
        
        Ola Källenius: Here’s one example where big data has actually changed the way we’re doing business: car2go. We know everything that happens to those cars, 24/7, around the year. If you start analyzing that data, you can see patterns. You can see, for instance, that between 8:00 and 10:00 in the morning, in different cities, there is a likelihood that somebody picks up a car, drives somewhere, and is in a certain “neighborhood A.” So we can make sure there are more cars in that neighborhood during those hours.
        
        We can also improve the customer experience so there is a one-to-one relationship with the customer. That’s what we do now with “Mercedes me,” which allows our customers to have a unique Mercedes ID. This allows seamless integration between your smartphone and your car, and between us and our vehicles. We know, for example, how your brake pads are wearing. That data lets us know when a car needs service even before the customer does, so we can prompt a service appointment.
        
        The Quarterly: How important is it for you to connect within the organization—to integrate marketing into product development, for example?
        
        Ola Källenius: We have completely reorganized our marketing and sales department instead of having different things in different areas, and have created what we call “best customer experience.” We bundle the different areas inside our headquarters function, which does the blueprint for the whole customer journey. We now have a steering committee with our telematics people and entertainment people and IT people sitting together because you can’t do marketing well in isolation. You have to have engineering with you, and you have to have IT with you—otherwise it doesn’t work.
        
        Ola Källenius: Another way we’re achieving the human touch is building on Apple’s idea of the “product genius.” It’s a role, in the retail network, that is not a sales role, so customers don’t have the pressure of the transaction when they speak with this person. We call the role our “product concierge.”
        
        The product-concierge role is solely to help our customers understand the product before the sale, after the sale, and after they have left the lot. So if you’ve purchased your new S-Class, the product concierge explains the car to you, and you kind of understand how it works. But now you’ve driven off, and you’re sitting there with the telemetric system, and you forgot how to activate your Mercedes-Benz apps, for instance. You call the product concierge, and he or she will explain it to you.
        
        This is just one example of a role that didn’t exist previously at a car dealer and almost didn’t exist anywhere. We’ve improved the customer experience by eliminating the pressure of the transaction. Hamburg was our pilot for that. Now, we’re training 500 product concierges in China as we speak.
        
        The Quarterly: Is this also a response, in part, to the challenge of proliferation? How do you break through the clutter?
        
        Ola Källenius: If you look at society as a whole, we all know this, the amount of information that you absorb per day now—compared with, maybe, what you did 10, 30, 50 years ago—is much, much higher. So to grab the attention of the relevant people and drive their buyers’ choice, you have to be really smart about this. This has huge importance, as far as Mercedes is concerned, compared with where we were years ago, when marketing was more just about the product.
        
        Now, to digitize within Mercedes, we have a proof point that we push for connectivity: “Mercedes me.” You have to have connectivity, especially for younger people. We offer all kinds of services around the car and beyond, so to speak. The look and feel of our advertising, physical presentation, and stores all need to fit into that world. This is reflected in our “Mercedes me” showroom in Hamburg—well, you could call it a showroom, but it’s really not. It’s a restaurant, it’s a happening place where we cooperate with artists and with musicians. It’s the cool place to be for young, successful professionals. They’re working hard all week, and they deserve a treat on the weekends!
        
        The Quarterly: Which becomes a key facet of this new golden age, does it not? The better you engage with your customer, the stronger your customer’s experience going forward.
        
        Ola Källenius: The founding father of our company called it: “The best or nothing.” What did he mean when he said that? He was not talking about a product description, per se. He was talking about attitude. You don’t rest on your laurels. You move beyond.
        
        We push the emotional button very consciously across touch points in marketing. And the great thing with Mercedes is that you do have emotional brands. When you buy a Mercedes, it’s always been about the dream of the little kid one day driving the star.Few business functions have been as profoundly disrupted by digitization as marketing. The era of expensive campaigns pushing products through mass media has been upended, as consumers, empowered by information, are demanding more and more from the companies they choose to form relationships with. In this interview with McKinsey’s Luke Collins, David Edelman, coleader of McKinsey’s global digital marketing strategy group, explains the state of digital marketing, what companies get wrong and what they should be doing, and the role of senior leaders in pushing their organizations to master the art of digital marketing. An edited transcript of Edelman’s remarks follows.
        
        Most companies think in terms of campaigns. They periodically want to get interest in the marketplace, so they come out with something they want to promote, whether it’s on a quarterly, monthly, or maybe weekly basis. The reality is, though, that at any given time you’re only pushing out something to those customers for whom that one thing is relevant. But most people sell a lot of things and could be a lot of different things to many different people. And what digital allows you to do is flip that model around and say for almost anybody, “There’s something about what we have to offer that should be connecting with them.”
        
        There’s this broad array of ways that people have been taking the virtual phone into the physical world and using that to navigate better and to be dramatically more empowered. They’re learning more about not just price but about where things come from, what reviews are, what really is the difference between this one and that one, maybe seeing whether or not something might be better for them versus something else.
        
        What digital allows you to do is have something for everybody and use the data that you have about an individual in a particular moment—because of the search term that they use, or because of where they’ve been looking on your website, or due to the social engagement they’ve had with your brand—to categorize them differently and have something for each of them. But that means having something for each of them and creating that range of content and offers. So instead of just having 2 or 10 things that you’re pushing out, it could mean having 100 things, 100 content objects, 100,000 different variants of your offers.
        
        That’s a very different model of marketing than saying, “OK, what should our campaign be? What do we need to promote? Let’s work with the agency. Let’s eventually come up with creative. Let’s come up with a target group of customers, and let’s just get this thing out and push it to them.” Digital is faster. It has a lot more complexity. And it’s more like agile development in software, where you’ve got this fast-turn, constant iterative testing. It’s just applying that into the marketing discipline.
        
        There are particular challenges that come up, stumbling blocks that we see repeatedly, that you can actually get around. And one of the first is—just starting with data and discovery—the dream that you’re going to have an integrated customer-data warehouse. That is a dream that, for many organizations, can take years to put together. And it’s going to be very hard. As you’re putting it together, things are going to keep moving. So it’s not something that you should be waiting for. It’s waiting for Godot.
        
        Instead, you should be thinking about—for a limited range of things that we think are going to be the highest value, for a particular range of segments—what is the data we need just for that, and can we create a lightweight way of bringing that together?
        
        The second challenge is organizational impediments to getting people to work together. So much of this now is about the customer’s journey. They’re on a cross-channel journey where they’re going to touch your mobile site, use their laptop, talk to somebody over the phone, go into a store. It’s all one journey. And you’ve got to be able to look at that in its totality and get people to work together and acknowledge the fact that these channels are all going to have different roles.
        
        The last part is working through what it takes to test and learn. We’ve seen a lot of companies saying, “Well, we don’t have the budget to do that many variations of a web landing page.” Or “We don’t really see how we can get approvals that fast through compliance, and through our legal folks.” It starts with just an attitude and leadership, saying, “We are going to work this through and going to work together as a team to try to get these obstacles out of the way.”
        
        One of the best ways to do that is through small-scale pilots. Pick a small geography, a specific segment of customers, a few products—a small-scale, contained base, and start with a test and learn to improve things within that. Understand: What are the challenges? What are the policies you’ll be up against? What are the processes? What data do you, and don’t you, have? Start building that muscle before you scale more broadly.
        
        When we work with clients, it’s interesting how much they tend to focus on big tools and systems and large-scale algorithm development, when a lot of this is about smart decisions, organization issues, and process design.
        
        We find a need to be ruthless about prioritizing: What data do we really need? Let’s focus on getting that together and work on it. Then, from a design perspective, let’s get the right people in the room, with the right incentives so they’ll work together and have shared common goals; and they’ll be in a setting where it’s comfortable to work together, where they’ve got the right project team, with the right leadership behind them that’s supporting the fact that they’re doing this—instead of everybody out for themselves. And, then, working through all the obstacles that hinder rapid-cycle test and learn, and accepting the fact that you’re going to be out there constantly testing things.
        
        Those three things—prioritizing the data; getting the right people from different functions to work together; and working on that rapid-cycle test and learn—are really what you’re trying to drive toward. If you can build those muscles, you can apply that to whatever stuff digital’s going to throw your way. And there’s always going to be new digital stuff.
        
        Now, you still need the right technology backbone, and you need to have the capabilities underneath it to be able to move the data to different channels and to be able to even take your prioritized data and bring that together. I’m not minimizing the technology challenge here. For many companies, they do need to make some pretty high-stakes decisions about their technology stack, whether it’s getting data, analyzing it, building their models, content management, getting it out to market, measurement and optimization on the back end. That whole stack does need to be thought through.
        
        And for many companies, there are a lot of breaks in that: The systems aren’t open. They’re locked into older technologies. And nobody’s really focusing on that whole end-to-end set of decisions. So there is something that you need to do technology-wise, but it needs to emerge from the sense of what it is that you really want to do from a customer-management perspective.
        
        I work with a lot of senior leaders in different industries, helping them steer their way through some kind of digital transformation, whether it’s from a functional perspective as a chief marketing officer, or whether it’s from a general business-manager perspective. And there are definitely certain kinds of patterns that I see in terms of how a lot of senior leaders need to recalibrate their mind-set.
        
        One of the first things is recognizing that digital isn’t just this added thing. It’s not just one more channel. It’s different. It’s about changing the way you’re operating, because it is about using data, faster cycle times, more interactivity with more empowered customers. And that is going to change a lot of what’s going to happen underneath that senior leader.
        
        So one of the first things that I think senior leaders need to do is get out there and actually see what people are doing more often. Too many people are just in their office, very internally focused, with all the complexities of their calendars. And you’ve got to break them out, get them out there, and see how somebody’s using Facebook in a store to ask people about whether their product is good or not.
        
        The second thing is looking across your team, getting the team to work together more in a cross-functional mode, and setting the expectation that it is going to have to be a more team-oriented approach toward problem solving, toward getting stuff out the door, toward hastening cycle times. You’ve got to think, as a senior leader, “What are the things that are preventing my channels, or my products teams, from working together, and what can I do to role model or change the incentives to get people to solve the problems in a more integrated way?”
        
        The third thing that we see is asking more from the data that you have and recognizing how much more the data is going to actually drive a lot of the decision making, a lot of the ways you’re handling customers, and many of the value-added services themselves—recommendation engines, for example. It’s challenging your teams to say, “It’s not just ‘What is our product strategy, what is our customer-experience strategy?’ It’s also, ‘What is our data strategy? How are we getting more information about our customers? How are we going to use that information to drive value? Is that going to lead our customers to do more business with us so that we can then gather more information back?’”
        
        Information and data is going to be a critical source of advantage, and it’s pointing your spotlight on how your organization is going to compete to get the best data—because that’s going to drive a lot of the insights. That’s a different perspective than many senior executives realize they need to take on a day-to-day basis.Customers have been spoiled. Thanks to companies such as Amazon and Apple, they now expect every organization to deliver products and services swiftly, with a seamless user experience.
        
        Customers want to log in to their online electricity account and see a real-time report of their consumption. They expect to buy a phone from their telecommunications provider and have it activated and set up immediately out of the box. They want bank loans to be preapproved or approved in minutes. They expect all service providers to have automated access to all the data they provided earlier and not to ask the same questions over and over again. They wonder why a bank needs their salary slips as proof of income when their money is being deposited directly into the bank every month by their employer.
        
        Many traditional organizations can’t meet these expectations. As a result, attackers born in the digital age can swoop in and disrupt the market through rapid delivery of digital products and services combined with advanced algorithms and full access to information.
        
        Customers wouldn’t phrase it this way, but they are demanding from companies in many industries a radical overhaul of business processes. Intuitive interfaces, around-the-clock availability, real-time fulfillment, personalized treatment, global consistency, and zero errors—this is the world to which customers have become increasingly accustomed. It’s more than a superior user experience, however; when companies get it right, they can also offer more competitive prices because of lower costs, better operational controls, and less risk.
        
        To meet these high customer expectations, companies must accelerate the digitization of their business processes. But they should go beyond simply automating an existing process. They must reinvent the entire business process, including cutting the number of steps required, reducing the number of documents, developing automated decision making, and dealing with regulatory and fraud issues. Operating models, skills, organizational structures, and roles need to be redesigned to match the reinvented processes. Data models should be adjusted and rebuilt to enable better decision making, performance tracking, and customer insights. Digitization often requires that old wisdom be combined with new skills, for example, by training a merchandising manager to program a pricing algorithm. New roles, such as data scientist and user-experience designer, may be needed.
        
        The benefits are huge: by digitizing information-intensive processes, costs can be cut by up to 90 percent and turnaround times improved by several orders of magnitude. Examples span multiple industries: one bank digitized its mortgage-application and decision process, cutting the cost per new mortgage by 70 percent and slashing time to preliminary approval from several days to just one minute. A telecommunications company created a self-serve, prepaid service where customers could order and activate phones without back-office involvement. A shoe retailer built a system to manage its in-store inventory that enabled it to know immediately whether a shoe and size was in stock—saving time for customers and sales staff. An insurance company built a digital process to automatically adjudicate a large share of its simple claims.
        
        In addition, replacing paper and manual processes with software allows businesses to automatically collect data that can be mined to better understand process performance, cost drivers, and causes of risk. Real-time reports and dashboards on digital-process performance permit managers to address problems before they become critical. For example, supply-chain-quality issues can be identified and dealt with more rapidly by monitoring customer buying behavior and feedback in digital channels. Leading organizations (see sidebar, “Scaling digitization processes at a European bank”) have come to recognize that the traditional large-scale projects to migrate all current processes to a digital world often take an extremely long time to deliver impact, and sometimes don’t work at all. Instead, successful companies are reinventing processes, challenging everything related to an existing process and rebuilding it using cutting-edge digital technology. For example, rather than creating technology tools to help back-office employees type customer complaints into their systems, leading organizations create self-serve options for customers to type in their own complaints.
        
        Sidebar Scaling digitization processes at a European bank A European bank is midway through an ambitious program to digitize its top 20 processes. The bank, in a challenging situation after the financial crisis, aspired to achieve improvements for its customers while significantly reducing costs. The bank set a stretch target of being ten times better in efficiency and turnaround times in each of its top processes. The bank started with its account-opening and mortgage applications. For each process, the development team used the first two weeks to define a digital vision for each product and a road map to get there. Then came rapid development of a digital prototype while redesigning the underlying business process, combining lean methodologies and agile software approaches. Within six weeks, a version of the user interface was ready. The team tested the process with customers, iterating and improving it, and gradually picking up the volume of mortgages and new accounts processed by the new digital process. After four months, the digital process was in production. The team scaled it through a website launch, followed by a rollout to the branch network. Then the bank moved to digitize processes such as personal lending and deposits. A new process allows customers to create an account in less than 10 minutes, replacing one that required 30 to 60 minutes of talking to branch staff and then up to a few days before the account was opened. The new digital mortgage-application process uses an online calculator that is connected to the bank’s credit-scoring models and gives customers a preliminary offer in less than a minute. Once an offer is received, customers can log in to an online portal that allows them to submit their application and documents. This approach has cut costs significantly while improving customer satisfaction.
        
        This kind of approach is usually done process by process in a series of short-term releases combining traditional process-reengineering methods like lean with new agile software-development methodologies.
        
        Companies in most industries can learn from the practices employed by firms that have done this successfully.
        
        Digitization often enables a process to be fundamentally reconfigured; for example, combining automated decision making with self-service can eliminate manual processes. Successful digitization efforts start by designing the future state for each process without regard for current constraints—say, shortening a process turnaround time from days to minutes. Once a compelling future state has been described, constraints (for instance, legally required checks) can be reintroduced. Companies should not hesitate to challenge each constraint. Many are corporate myths that can be quickly resolved through discussions with customers or regulators.
        
        Digitizing select stages of the customer experience may increase efficiency in specific areas of the process and address some burning customer issues, but it will never deliver a truly seamless experience, and as a result may leave significant potential on the table. To tackle an end-to-end process such as customer onboarding, process-digitization teams need support from every function involved in the customer experience. The end customer should be heavily involved too, not least to challenge conventional wisdom. To do this, some firms are creating start-up-style, cross-functional units that bring together all colleagues—including IT developers—involved in the end-to-end customer experience. The cross-functional unit has the mandate to challenge the status quo. Members are often collocated to improve lines of communication and ensure a true team effort.
        
        Digitization skills are in short supply, so successful programs emphasize building in-house capabilities. The goal is to create a center of excellence with skilled staff that can be called upon to digitize processes quickly. Still, many times companies must search for talent externally to address the need for new skill sets and roles, such as data scientists or user-experience designers. Given its importance, the first managers selected to lead the transformation should be carefully chosen, well trusted in the organization, and ready to commit for a long period of time. It is also important that the team has the skills needed to build the required technology components in a modular way so that they can be reused across processes, maximizing economies of scale.
        
        Traditional IT-intensive programs deliver a return only at the end of the project, sometimes years after the project’s kickoff. Digitizing end-to-end processes one by one, however, can deliver improved performance in just three to five months. Complex IT challenges such as legacy-systems integration can be harder to move along quickly, but there are ways to mitigate the risks of delay. For example, one industrial company pursuing an IT legacy-systems integration used low-cost offshore resources to rekey data among systems, allowing a new digital customer process to be brought online for use with pilot customers while a robust IT interface was built in parallel. This approach reduced the risk involved with the integration effort and accelerated payback.
        
        Moving quickly isn’t always easy. More often than not, it’s business decision making that’s causing the bottleneck rather than IT development. That’s why digitization programs need strong board-level support to align all the stakeholders, while all other decisions should be delegated to the project team.
        
        In traditional deployment, a new solution is rolled out progressively across sites to existing user teams. However, a different approach may be needed when organizations undertake digitization, because of radical changes to processes and the supporting organization. For example, telecommunications salespeople may prefer customers to apply for services through the existing store system instead of self-serve kiosks. Bank-credit underwriters may not trust automated algorithms and may choose to review automatically approved applications. In these cases, it might be easier to roll in a new organizational unit to handle the new digital process, and then bring employees into this unit while increasing the volumes handled by it in parallel. This ensures a much easier transition to the digital process by not expending extensive energy on changing old habits and behaviors. By the time all process volume has migrated to the new digital process, the new organizational unit will have “swallowed” all the required employees from the legacy units.
        
        Companies that digitize processes can improve their bottom lines and delight customers. The value at stake depends on the business model and starting point but can be estimated by allocating costs to end-to-end processes and benchmarking against peers. To kick-start the approach and build capabilities and momentum, organizations can undertake one or two pilots and then scale rapidly.The age of experimentation with digital is over. In an often bleak landscape of slow economic recovery, digital continues to show healthy growth. E-commerce is growing at double-digit rates in the United States and most European countries, and it is booming across Asia. To take advantage of this momentum, companies need to move beyond experiments with digital and transform themselves into digital businesses. Yet many companies are stumbling as they try to turn their digital agendas into new business and operating models. The reason, we believe, is that digital transformation is uniquely challenging, touching every function and business unit while also demanding the rapid development of new skills and investments that are very different from business as usual. To succeed, management teams need to move beyond vague statements of intent and focus on “hard wiring” digital into their organization’s structures, processes, systems, and incentives.
        
        There is no blueprint for success, but there are plenty of examples that offer insights into the approaches and actions of a successful digital transformation. By studying dozens of these successes—looking beyond the usual suspects—we discovered that effective digital enterprises share these seven traits.
        
        Leadership teams must be prepared to think quite differently about how a digital business operates. Digital leaders set aspirations that, on the surface, seem unreasonable. Being “unreasonable” is a way to jar an organization into seeing digital as a business that creates value, not as a channel that drives activities. Some companies frame their targets by measures such as growth or market share through digital channels. Others set targets for cost reduction based on the cost structures of new digital competitors. Either way, if your targets aren’t making the majority of your company feel nervous, you probably aren’t aiming high enough.
        
        When Angela Ahrendts became CEO of Burberry, in 2006, she took over a stalling business whose brand had become tarnished. But she saw what no one else could: that a high-end fashion retailer could remake itself as a digital brand. Taking personal control of the digital agenda, she oversaw a series of groundbreaking initiatives, including a website (ArtoftheTrench.com) that featured customers as models, a more robust e-commerce catalog that matched the company’s in-store inventory, and the digitization of retail stores through features such as radio-frequency identification tags. During Ahrendts’s tenure, revenues tripled. (Apple hired Ahrendts last October to head its retail business.)
        
        Netflix was another brand with an unreasonably aspirational vision. It had built a successful online DVD rental business, but leadership saw that the future of the industry would be in video streaming, not physical media. The management team saw how quickly broadband technology was evolving and made a strategic bet that placed it at the forefront of a surge in real-time entertainment. As the video-streaming market took off, Netflix quickly captured nearly a third of downstream video traffic. By the end of 2013, Netflix had more than 40 million streaming subscribers.
        
        The skills required for digital transformation probably can’t be groomed entirely from within. Leadership teams must be realistic about the collective ability of their existing workforce. Leading companies frequently look to other industries to attract digital talent, because they understand that emphasizing skills over experience when hiring new talent is vital to success, at least in the early stages of transformation. The best people in digital product management or user-experience design may not work in your industry. Hire them anyway.
        
        Tesco, the UK grocery retailer, made three significant digital acquisitions over a two-year span: blinkbox, a video-streaming service; We7, a digital music store; and Mobcast, an e-book platform. The acquisitions enabled Tesco to quickly build up the skills it needed to move into digital media. In the United States, Verizon followed a similar path with strategic acquisitions that immediately bolstered its expertise in telematics (Hughes Telematics in 2012) and cloud services (CloudSwitch in 2011), two markets that are growing at a rapid pace.
        
        This “acqui-hire” approach is not the only option. But we have observed that significant lateral hiring is required in the early stages of a transformation to create a pool of talent deep enough to execute against an ambitious digital agenda and plant the seeds for a new culture.
        
        A bank or retailer that acquires a five-person mobile-development firm and places it in the middle of its existing web operations is more likely to lose the team than to assimilate it. Digital talent must be nurtured differently, with its own working patterns, sandbox, and tools. After a few false starts, Wal-Mart Stores learned that “ring fencing” its digital talent was the only way to ensure rapid improvements. Four years ago, the retail giant’s online business was lagging. It was late to the e-commerce market as executives protected their booming physical-retail business. When it did step into the digital space, talent was disbursed throughout the business. Its $5 billion in online sales in 2011 paled next to Amazon’s $48 billion.
        
        In 2011, however, Wal-Mart established @WalmartLabs, an “idea incubator,” as part of its growing e-commerce division in Silicon Valley—far removed from the company’s Bentonville, Arkansas, headquarters. The group’s innovations, including a unified company-wide e-commerce platform, helped Wal-Mart increase online revenues by 30 percent in 2013, outpacing Amazon’s rate of growth.
        
        Wal-Mart took ring fencing to the extreme, turning its e-commerce business into a separate vertical with its own profit and loss. This approach won’t work for every incumbent, and even when it does, it is not necessarily a long-term solution. Thus Telefónica this year recombined with the core business Telefónica Digital, a separate business unit created in 2011 to nurture and strengthen its portfolio of digital initiatives. To deliver in an omnichannel world, where customers expect seamless integration of digital and analog channels, seamless internal integration should be the end goal.
        
        The leaders of incumbent companies must aggressively challenge the status quo rather than accepting historical norms. Look at how everything is done, including the products and services you offer and the market segments you address, and ask “Why?” Assume there is an unknown start-up asking the exact same question as it plots to disrupt your business. It is no coincidence that many textbook cases of companies redefining themselves come from Silicon Valley, the epicenter of digital disruption. Think of Apple’s transformation from struggling computer maker into (among other things) the world’s largest music retailer, or eBay’s transition from online bazaar to global e-commerce platform.
        
        Digital leaders examine all aspects of their business—both customer-facing and back-office systems and processes, up and down the supply chain—for digitally driven innovation. In 2007, car-rental company Hertz started to deploy self-service kiosks similar to those used by airlines for flight check-in. In 2011, it leapfrogged airlines by moving to dual-screen kiosks—one screen to select rental options via touch screen, a second screen at eye level to communicate with a customer agent using real-time video.
        
        We see digital leaders thinking expansively about partnerships to deliver new value-added experiences and services. This can mean alliances that span industry sectors, such as the Energy@home partnership among Electrolux, Enel, Indesit, and Telecom Italia to create a communications platform for smart devices and domestic appliances.
        
        Rapid decision making is critical in a dynamic digital environment. Twelve-month product-release cycles are a relic. Organizations need to move to a cycle of continuous delivery and improvement, adopting methods such as agile development and “live beta,” supported by big data analytics, to increase the pace of innovation. Continuous improvement requires continuous experimentation, along with a process for quickly responding to bits of information.
        
        Integrating data sources into a single system that is accessible to everyone in the organization will improve the “clock speed” for innovation. P&G, for example, created a single analytics portal, called the Decision Cockpit, which provides up-to-date sales data across brands, products, and regions to more than 50,000 employees globally. The portal, which emphasizes projections over historical data, lets teams quickly identify issues, such as declining market share, and take steps to address the problems.
        
        U.S. Xpress, a US transportation company, collects data in real time from tens of thousands of sources, including in-vehicle sensors and geospatial systems. Using Apache Hadoop, an open-source tool set for data analysis, and real-time business-intelligence tools, U.S. Xpress has been able to extract game-changing insights about its fleet operations. For example, looking at the fuel consumption of idling vehicles led to changes that saved the company more than $20 million in fuel consumption in the first year alone.
        
        Many organizations focus their digital investments on customer-facing solutions. But they can extract just as much value, if not more, from investing in back-office functions that drive operational efficiencies. A digital transformation is more than just finding new revenue streams; it’s also about creating value by reducing the costs of doing business.
        
        Investments in digital should not be spread haphazardly across the organization under the halo of experimentation. A variety of frequent testing is critical, but teams must quickly zero in on the digital investments that create the most value—and then double down.
        
        Often, great value is found in optimizing back-office functions. At Starbucks, one of the leaders in customer-experience innovation, just 35 of 100 active IT projects in 2013 were focused on customer- or partner-facing initiatives. One-third of these projects were devoted to improving efficiency and productivity away from the retail stores, and one-third focused on improving resilience and security. In manufacturing, P&G collaborated with the Los Alamos National Laboratory to create statistical methods to streamline processes and increase uptime at its factories, saving more than $1 billion a year.
        
        Rising customer expectations continue to push businesses to improve the customer experience across all channels. Excellence in one channel is no longer sufficient; customers expect the same frictionless experience in a retail store as they do when shopping online, and vice versa. Moreover, they are less accepting of bad experiences; one survey found that 89 percent of consumers began doing business with a competitor following a poor customer experience. On the flip side, 86 percent said they were willing to pay more for a better customer experience.
        
        A healthy obsession with improving the customer experience is the foundation of any digital transformation. No enterprise is perfect, but leadership teams should aspire to fix every error or bad experience. Processes that enable companies to capture and learn from every customer interaction—positive or negative—help them to regularly test assumptions about how customers are using digital and constantly fine-tune the experience.
        
        This mind-set is what enables companies to go beyond what’s normal and into the extraordinary. If online retailer Zappos is out of stock on a product, it will help you find the item from a competitor. Little wonder that 75 percent of its orders come from repeat customers.
        
        Leaders of successful digital businesses know that it’s not enough to develop just one or two of these traits. The real innovators will learn to excel at all seven of them. Doing so requires a radically different mind-set and operating approach.Exhibit We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The board of a large European insurer was pressing management for answers. A company known mostly for its online channel had begun to undercut premiums in a number of markets and was doing so without agents, building on its dazzling brand reputation online and using new technologies to engage buyers. Some of the insurer’s senior managers were sure the threat would abate. Others pointed to serious downtrends in policy renewals among younger customers avidly using new web-based price-comparison tools. The board decided that the company needed to quicken its digital pace.
        
        For many leaders, this story may sound familiar, harkening back to the scary days, 15 years ago, when they encountered the first wave of Internet competitors. Many incumbents responded effectively to these threats, some of which in any event dissipated with the dot-com crash. Today’s challenge is different. Robust attackers are scaling up with incredible speed, inserting themselves artfully between you and your customers and zeroing in on lucrative value-chain segments.
        
        The digital technologies underlying these competitive thrusts may not be new, but they are being used to new effect. Staggering amounts of information are accessible as never before—from proprietary big data to new public sources of open data. Analytical and processing capabilities have made similar leaps with algorithms scattering intelligence across digital networks, themselves often lodged in the cloud. Smart mobile devices make that information and computing power accessible to users around the world.
        
        As these technologies gain momentum, they are profoundly changing the strategic context: altering the structure of competition, the conduct of business, and, ultimately, performance across industries. One banking CEO, for instance, says the industry is in the midst of a transition that occurs once every 100 years. To stay ahead of the unfolding trends and disruptions, leaders across industries will need to challenge their assumptions and pressure-test their strategies.
        
        Digitization often lowers entry barriers, causing long-established boundaries between sectors to tumble. At the same time, the “plug and play” nature of digital assets causes value chains to disaggregate, creating openings for focused, fast-moving competitors. New market entrants often scale up rapidly at lower cost than legacy players can, and returns may grow rapidly as more customers join the network.
        
        Digital capabilities increasingly will determine which companies create or lose value. Those shifts take place in the context of industry evolution, which isn’t monolithic but can follow a well-worn path: new trends emerge and disruptive entrants appear, their products and services embraced by early adopters (exhibit). Advanced incumbents then begin to adjust to these changes, accelerating the rate of customer adoption until the industry’s level of digitization—among companies but, perhaps more critically, among consumers as well—reaches a tipping point. Eventually, what was once radical is normal, and unprepared incumbents run the risk of becoming the next Blockbuster. Others, which have successfully built new capabilities (as Burberry did in retailing), become powerful digital players. (See the accompanying article, “The seven traits of effective digital enterprises.”) The opportunities for the leaders include:
        
        Enhancing interactions among customers, suppliers, stakeholders, and employees. For many transactions, consumers and businesses increasingly prefer digital channels, which make content universally accessible by mixing media (graphics and video, for example), tailoring messages for context (providing location or demographic information), and adding social connectivity (allowing communities to build around themes and needs, as well as ideas shared among friends). These channels lower the cost of transactions and record them transparently, which can help in resolving disputes.
        
        Improving management decisions as algorithms crunch big data from social technologies or the Internet of Things. Better decision making helps improve performance across business functions—for example, providing for finer marketing allocations (down to the level of individual consumers) or mitigating operational risks by sensing wear and tear on equipment.
        
        Enabling new business or operating models, such as peer-to-peer product innovation or customer service. China’s Xiaomi crowdsources features of its new mobile phones rather than investing heavily in R&D, and Telstra crowdsources customer service, so that users support each other to resolve problems without charge. New business or operating models can also disintermediate existing customer–supplier relations—for example, when board-game developers or one-person shops manufacture products using 3-D printers and sell directly to Amazon.
        
        The upshot is that digitization will change industry landscapes as it gives life to new sets of competitors. Some players may consider your capabilities a threat even before you have identified them as competitors. Indeed, the forces at work today will bring immediate challenges, opportunities—or both—to literally all digitally connected businesses.
        
        Our research and experience with leading companies point to seven trends that could redefine competition.
        
        Digital technologies create near-perfect transparency, making it easy to compare prices, service levels, and product performance: consumers can switch among digital retailers, brands, and services with just a few clicks or finger swipes. This dynamic can commoditize products and services as consumers demand comparable features and simple interactions. Some banks, for instance, now find that simplifying products for easy purchase on mobile phones inadvertently contributes to a convergence between their offerings and those of competitors that are also pursuing mobile-friendly simplicity.
        
        Third parties have jumped into this fray, disintermediating relationships between companies and their customers. The rise of price-comparison sites that aggregate information across vendors and allow consumers to compare prices and service offerings easily is a testament to this trend. In Europe, chain retailers, which traditionally dominate fast-moving consumer goods, have seen their revenues fall as customers flock to discounters after comparing prices even for staples like milk and bread. In South Korea, online aggregator OK Cashbag has inserted itself into the consumer’s shopping behavior through a mobile app that pools product promotions and loyalty points for easy use across more than 50,000 merchants.
        
        These dynamics create downward pressure on returns across consumer-facing industries, and the disruptive currents are now rippling out to B2B businesses.
        
        Digital dynamics often undermine barriers to entry and long-standing sources of product differentiation. Web-based service providers in telecommunications or insurance, for example, can now tap markets without having to build distribution networks of offices and local agents. They can compete effectively by mining data on risks and on the incomes and preferences of customers.
        
        At the same time, the expense of building brands online and the degree of consumer attention focused on a relatively small number of brands are redrawing battle lines in many markets. Singapore Post is investing in an e-commerce business that benefits from the company’s logistics and warehousing backbone. Japanese web retailer Rakuten is using its network to offer financial services. Web powerhouses like Google and Twitter eagerly test industry boundaries through products such as Google Wallet and Twitter’s retail offerings.
        
        New competitors can often be smaller companies that will never reach scale but still do a lot of damage to incumbents. In the retailing industry, for instance, entrepreneurs are cherry-picking subcategories of products and severely undercutting pricing on small volumes, forcing bigger companies to do the same.
        
        Digital businesses reduce transaction and labor costs, increase returns to scale from aggregated data, and enjoy increases in the quality of digital talent and intellectual property as network effects kick in. The cost advantages can be significant: online retailers may generate three times the level of revenue per employee as even the top-performing discounters. Comparative advantage can materialize rapidly in these information-intensive models—not over the multiyear spans most companies expect.
        
        Scale economies in data and talent often are decisive. In insurance, digital “natives” with large stores of consumer information may navigate risks better than traditional insurers do. Successful start-ups known for digital expertise and engineer-friendly cultures become magnets for the best digital talent, creating a virtuous cycle. These effects will accelerate consolidation in the industries where digital scale weighs most heavily, challenging more capital- and labor-intensive models. In our experience, banking, insurance, media, telecommunications, and travel are particularly vulnerable to these winner-takes-all market dynamics.
        
        In France, for instance, the start-up Free has begun offering mobile service supported by a large and active digital community of “brand fans” and advocates. The company nurtures opinion-leader “alpha fans,” who interact with the rest of the base on the Internet via blogs, social networks, and other channels, building a wave of buzz that quickly spreads across the digital world. Spending only modestly on traditional marketing, Free nonetheless has achieved high levels of customer satisfaction through its social-media efforts—and has gained substantial market share.
        
        As digital forces reduce transaction costs, value chains disaggregate. Third-party products and services—digital Lego blocks, in effect—can be quickly integrated into the gaps. Amazon, for instance, offers businesses logistics, online retail “storefronts,” and IT services. For many businesses, it may not pay to build out those functions at competitive levels of performance, so they simply plug an existing offering into their value chains. In the United States, registered investment advisers have been the fastest-growing segment of the investment-advisory business, for example. They are expanding so fast largely because they “insource” turnkey systems (including record keeping and operating infrastructure) purchased from Charles Schwab, Fidelity, and others that give them all the capabilities they need. With a license, individuals or small groups can be up and running their own firms.
        
        In the travel industry, new portals are assembling entire trips: flights, hotels, and car rentals. The stand-alone offerings of third parties, sometimes from small companies or even individuals, plug into such portals. These packages are put together in real time, with dynamic pricing that depends on supply and demand. As more niche providers gain access to the new platforms, competition is intensifying.
        
        Software replaces labor in digital businesses. We estimate, for instance, that of the 700 end-to-end processes in banks (opening an account or getting a car loan, for example), about half can be fully automated. Computers increasingly are performing complex tasks as well. “Brilliant machines,” like IBM’s Watson, are poised to take on the work of many call-center workers. Even knowledge-intensive areas, such as oncology diagnostics, are susceptible to challenge by machines: thanks to the ability to scan and store massive amounts of medical research and patients’ MRI results, Watson diagnoses cancers with much higher levels of speed and accuracy than skilled physicians do. Digitization will encroach on a growing number of knowledge roles within companies as they automate many frontline and middle-management jobs based upon synthesizing information for C-level executives.
        
        At the same time, companies are struggling to find the right talent in areas that can’t be automated. Such areas include digital skills like those of artificial-intelligence programmers or data scientists and of people who lead digital strategies and think creatively about new business designs. A key challenge for senior managers will be sensitively reallocating the savings from automation to the talent needed to forge digital businesses. One global company, for example, is simultaneously planning to cut more than 10,000 employees (some through digital economies) while adding 3,000 to its digital business. Moves like these, writ large, could have significant social repercussions, elevating the opportunities and challenges associated with digital advances to a public-policy issue, not just a strategic-business one.
        
        Digital technologies know no borders, and the customer’s demand for a unified experience is raising pressure on global companies to standardize offerings. In the B2C domain, for example, many US consumers are accustomed to e-shopping in the United Kingdom for new fashions (see sidebar, “How digitization is reshaping global flows”). They have come to expect payment systems that work across borders, global distribution, and a uniform customer experience.
        
        Sidebar How digitization is reshaping global flows As the spread of the Internet and digital technologies reshapes the competitive landscape of industries, it is also revolutionizing the traditional flows of goods, services, finance, and people. All this is happening at breakneck pace (exhibit), as we showed in a recent report, Global flows in a digital age: How trade, finance, people, and data connect the world economy. The pace will only accelerate as global Internet traffic, which has expanded 18-fold since 2005, surges an additional 8-fold by 2025. Exhibit The digital component of global flows is growing quickly. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com Digitization transforms global flows by vastly reducing marginal production and distribution costs in three ways. The first is the creation of purely digital goods, in both the B2B and B2C realms. The volume of digital consumer goods, from music to movies, transported and reproduced around the globe continues to soar. Apps that allow consumers to purchase virtual goods and digital services on mobile devices have become a significant industry. For businesses, digitization is transforming even physical flows of people into virtual flows, enabling remote work through tools for global collaboration. In some manufacturing sectors, it is now possible to ship a digital design file for 3-D printing and then make the product where it will be consumed instead of producing centrally and shipping the physical goods. Second, digitization enhances the value of physical flows by the use of “digital wrappers” that pack information around goods as they traverse global value chains. Online reviews or customer ratings, for example, help consumers decide whether to purchase products. Increasingly common digital tags and sensors connected by wireless communications can identify objects and collect information about transactions, the location of a product, and when it is used. Such wrappers greatly improve processes ranging from payment systems to supply-chain management. Imagine Apple trying to assemble the iPod, with 451 parts from many different countries, without digital tracking and supply-chain-management tools. Finally, digitization is creating online platforms that bring efficiency and speed to production and cross-border exchanges. Proliferating e-commerce platforms allow greater and faster flows of goods and services to new markets and help smaller players participate in expanding global trade. New online markets in information flows facilitate innovation through crowdsourcing, while other platforms let designers upload product designs, use 3-D printers to create physical items, and manage logistics and payments. About the Author(s) Jacques Bughin is a director in McKinsey’s Brussels office; James Manyika is a director in the San Francisco office and a director of the McKinsey Global Institute; Olivia Nottebohm is a principal in the Silicon Valley office.
        
        In B2B markets from banking to telecommunications, corporate purchasers are raising pressure on their suppliers to offer services that are standardized across borders, integrate with other offerings, and can be plugged into the purchasing companies’ global business processes easily. One global bank has aligned its offerings with the borderless strategies of its major customers by creating a single website, across 20 countries, that integrates what had been an array of separate national or product touch points. A US technology company has given each of its larger customers a customized global portal that allows it to get better insights into their requirements, while giving them an integrated view of global prices and the availability of components.
        
        Digitization isn’t a one-stop journey. A case in point is music, where the model has shifted from selling tapes and CDs (and then MP3s) to subscription models, like Spotify’s. In transportation, digitization (a combination of mobile apps, sensors in cars, and data in the cloud) has propagated a powerful nonownership model best exemplified by Zipcar, whose service members pay to use vehicles by the hour or day. Google’s ongoing tests of autonomous vehicles indicate even more radical possibilities to shift value. As the digital model expands, auto manufacturers will need to adapt to the swelling demand of car buyers for more automated, safer features. Related businesses, such as trucking and insurance, will be affected, too, as automation lowers the cost of transportation (driverless convoys) and “crash-less” cars rewrite the existing risk profiles of drivers.
        
        Rethinking strategy in the face of these forces involves difficult decisions and trade-offs. Here are six of the thorniest.
        
        The growth and profitability of some businesses become less attractive in a digital world, and the capabilities needed to compete change as well. Consequently, the portfolio of businesses within a company may have to be altered if it is to achieve its desired financial profile or to assemble needed talent and systems.
        
        Tesco has made a number of significant digital acquisitions over a two-year span to take on digital competition in consumer electronics. Beauty-product and fragrance retailer Sephora recently acquired Scentsa, a specialist in digital technologies that improve the in-store shopping experience. (Scentsa touch screens access product videos, link to databases on skin care and fragrance types, and make product recommendations.) Sephora officials said they bought the company to keep its technology out of competitors’ reach and to help develop in-store products more rapidly.
        
        Companies that lack sufficient scale or expect a significant digital downside should consider divesting businesses. Some insurers, for instance, may find themselves outmatched by digital players that can fine-tune risks. In media, DMGT doubled down on an investment in their digital consumer businesses, while making tough structural decisions on their legacy print assets, including the divestment of local publications and increases in their national cover price. Home Depot continues to shift its investment strategy away from new stores to massive new warehouses that serve growing online sales. This year it bought Blinds.com, adding to a string of website acquisitions.
        
        Incumbents too have opportunities for launching disruptive strategies. One European real-estate brokerage group, with a large, exclusively controlled share of the listings market, decided to act before digital rivals moved into its space. It set up a web-based platform open to all brokers (many of them competitors) and has now become the leading national marketplace, with a growing share. In other situations, the right decision may be to forego digital moves—particularly in industries with high barriers to entry, regulatory complexities, and patents that protect profit streams.
        
        Between these extremes lies the all-too-common reality that digital efforts risk cannibalizing products and services and could erode margins. Yet inaction is equally risky. In-house data on existing buyers can help incumbents with large customer bases develop insights (for example, in pricing and channel management) that are keener than those of small attackers. Brand advantages too can help traditional players outflank digital newbies.
        
        A large incumbent in an industry that’s undergoing digital disruption can feel like a whale attacked by piranhas. While in the past, there may have been one or two new entrants entering your space, there may be dozens now—each causing pain, with none individually fatal. PayPal, for example, is taking slices of payment businesses, and Amazon is eating into small-business lending. Companies can neutralize attacks by rapidly building copycat propositions or even acquiring attackers. However, it’s not feasible to defend all fronts simultaneously, so cooperation with some attackers can make more sense than competing.
        
        Santander, for instance, recently went into partnership with start-up Funding Circle. The bank recognized that a segment of its customer base wanted access to peer-to-peer lending and in effect acknowledged that it would be costly to build a world-class offering from scratch. A group of UK banks formed a consortium to build a mobile-payment utility (Paym) to defend against technology companies entering their markets. British high-end grocer Waitrose collaborated with start-up Ocado to establish a digital channel and home distribution before eventually creating its own digital offering.
        
        Digital technologies themselves are opening pathways to collaborative forms of innovation. Capital One launched Capital One Labs, opening its software interfaces to multiple third parties, which can defend a range of spaces along their value chains by accessing Capital One’s risk- and credit-assessment capabilities without expending their own capital.
        
        As digital opportunities and challenges proliferate, deciding where to place new bets is a growing headache for leaders. Diversification reduces risks, so many companies are tempted to let a thousand flowers bloom. But often these small initiatives, however innovative, don’t get enough funding to endure or are easily replicated by competitors. One answer is to think like a private-equity fund, seeding multiple initiatives but being disciplined enough to kill off those that don’t quickly gain momentum and to bankroll those with genuinely disruptive potential. Since 2010, Merck’s Global Health Innovation Fund, with $500 million under management, has invested in more than 20 start-ups with positions in health informatics, personalized medicine, and other areas—and it continues to search for new prospects. Other companies, such as BMW and Deutsche Telekom, have set up units to finance digital start-ups.
        
        The alternative is to double down in one area, which may be the right strategy in industries with massive value at stake. A European bank refocused its digital investments on 12 customer decision journeys, such as buying a house, that account for less than 5 percent of its processes but nearly half of its cost base. A leading global pharmaceutical company has made significant investments in digital initiatives, pooling data with health insurers to improve rates of adherence to drug regimes. It is also using data to identify the right patients for clinical trials and thus to develop drugs more quickly, while investing in programs that encourage patients to use monitors and wearable devices to track treatment outcomes. Nordstrom has invested heavily to give its customers multichannel experiences. It focused initially on developing first-class shipping and inventory-management facilities and then extended its investments to mobile-shopping apps, kiosks, and capabilities for managing customer relationships across channels.
        
        Integrating digital operations directly into physical businesses can create additional value—for example, by providing multichannel capabilities for customers or by helping companies share infrastructure, such as supply-chain networks. However, it can be hard to attract and retain digital talent in a traditional culture, and turf wars between the leaders of the digital and the main business are commonplace. Moreover, different businesses may have clashing views on, say, how to design and implement a multichannel strategy.
        
        One global bank addressed such tensions by creating a groupwide center of excellence populated by digital specialists who advise business units and help them build tools. The digital teams will be integrated with the units eventually, but not until the teams reach critical mass and notch a number of successes. The UK department-store chain John Lewis bought additional digital capabilities with its acquisition of the UK division of Buy.com, in 2001, ultimately combining it with the core business. Wal-Mart Stores established its digital business away from corporate headquarters to allow a new culture and new skills to grow. Hybrid approaches involving both stand-alone and well-integrated digital organizations are possible, of course, for companies with diverse business portfolios.
        
        Advancing the digital agenda takes lots of senior-management time and attention. Customer behavior and competitive situations are evolving quickly, and an effective digital strategy calls for extensive cross-functional orchestration that may require CEO involvement. One global company, for example, attempted to digitize its processes to compete with a new entrant. The R&D function responsible for product design had little knowledge of how to create offerings that could be distributed effectively over digital channels. Meanwhile, a business unit under pricing pressure was leaning heavily on functional specialists for an outsize investment to redesign the back office. Eventually, the CEO stepped in and ordered a new approach, which organized the digitization effort around the decision journeys of clients.
        
        Faced with the need to sort through functional and regional issues related to digitization, some companies are creating a new role: chief digital officer (or the equivalent), a common way to introduce outside talent with a digital mind-set to provide a focus for the digital agenda. Walgreens, a well-performing US pharmacy and retail chain, hired its president of digital and chief marketing officer (who reports directly to the CEO) from a top technology company six years ago. Her efforts have included leading the acquisition of drugstore.com, which still operates as a pure play. The acquisition upped Walgreens’ skill set, and drugstore.com increasingly shares its digital infrastructure with the company’s existing site: walgreens.com.
        
        Relying on chief digital officers to drive the digital agenda carries some risk of balkanization. Some of them, lacking a CEO’s strategic breadth and depth, may sacrifice the big picture for a narrower focus—say, on marketing or social media. Others may serve as divisional heads, taking full P&L responsibility for businesses that have embarked on robust digital strategies but lacking the influence or authority to get support for execution from the functional units.
        
        Alternatively, CEOs can choose to “own” and direct the digital agenda personally, top down. That may be necessary if digitization is a top-three agenda item for a company or group, if digital businesses need substantial resources from the organization as a whole, or if pursuing new digital priorities requires navigating political minefields in business units or functions.
        
        Regardless of the organizational or leadership model a CEO and board choose, it’s important to keep in mind that digitization is a moving target. The emergent nature of digital forces means that harnessing them is a journey, not a destination—a relentless leadership experience and a rare opportunity to reposition companies for a new era of competition and growth.Three years ago, we described ten information technology–enabled business trends that were profoundly altering the business landscape. The pace of technology change, innovation, and business adoption since then has been stunning. Consider that the world’s stock of data is now doubling every 20 months; the number of Internet-connected devices has reached 12 billion; and payments by mobile phone are hurtling toward the $1 trillion mark.
        
        This progress both reflects the trends we described three years ago and is influencing their shape. The article that follows updates our 2010 list. (For a more detailed treatment, download the related white paper [PDF–1MB] from the McKinsey Global Institute.) In addition to describing how several trends have grown in importance, we have added a few that are rapidly gathering momentum, while removing those that have entered the mainstream.
        
        The dramatic pace at which two trends have been advancing is transforming them into 21st-century business “antes”: competitive necessities for most if not all companies. Big data and advanced analytics have swiftly moved from the frontier of our trends to a set of capabilities that need to be deeply embedded across functions and operations, enabling managers to have a better basis for understanding markets and making business decisions. Meanwhile, social technologies are becoming a powerful social matrix—a key piece of organizational infrastructure that links and engages employees, customers, and suppliers as never before.
        
        Implicit in our earlier work, and explicit in this update, is a focus on information and communication technologies. Other forms of technology are changing, too, of course, and as we’ve been updating this list, we’ve also been conducting new research on the most disruptive technologies of all types. Four of the trends described here reflect IT disruptions elaborated in that separate but related research, which encompasses fields as wide-ranging as genomics and energy and materials science. The Internet of All Things, the linking of physical objects with embedded sensors, is being exploited at breakneck pace, simultaneously creating massive network effects and opportunities. “The cloud,” with its ability to deliver digital power at low cost and in small increments, is not only changing the profile of corporate IT departments but also helping to spawn a range of new business models by shifting the economics of “rent versus buy” trade-offs for companies and consumers. The result is an acceleration of a trend we identified in 2010: the delivery of anything as a service. The creeping automation of knowledge work, which affects the fastest-growing employee segment worldwide, promises a new phase of corporate productivity.Finally, up to three billion new consumers, mostly in emerging markets, could soon become fully digital players, thanks chiefly to mobile technologies. Our research suggests that the collective economic impact (in the applications that we examined) of information technologies underlying these four trends could range from $10 trillion to $20 trillion annually in 2025.
        
        The next three trends will be most familiar to digital marketers, but their relevance is expanding across the enterprise, starting with customer-experience, product, and channel management. The integration of digital and physical experiences is creating new ways for businesses to interact with customers, by using digital information to augment individual experiences with products and services. Consumer demand is rising for products that are free, intuitive, and radically user oriented. And the rapid evolution of IT-enabled commerce is reducing entry barriers and opening new revenue streams to a range of individuals and companies.
        
        Finally, consider the extent to which government, education, and health care—which often seem outside the purview of business leaders—could benefit from adopting digital technologies at the same level as many industries have. Productivity gains could help address the imperative (created by aging populations) to do more with less, while technological innovation could improve the quality and reach of many services. The embrace of digital technologies by these sectors is thus a trend of immense importance to business, which indirectly finances many services and would benefit greatly from the rising skills and improved health of citizens everywhere.
        
        Social technologies are much more than a consumer phenomenon: they connect many organizations internally and increasingly reach outside their borders. The social matrix also extends beyond the cocreation of products and the organizational networks we examined in our 2010 article. Now it has become the environment in which more and more business is conducted. Many organizations rely on distributed problem solving, tapping the brain power of customers and experts from within and outside the company for breakthrough thinking. Pharmaceutical player Boehringer Ingelheim sponsored a competition on Kaggle (a platform for data-analysis contests) to predict the likelihood that a new drug molecule would cause genetic mutations. The winning team, from among nearly 9,000 competitors, combined experience in insurance, physics, and neuroscience, and its analysis beat existing predictive methods by more than 25 percent.
        
        In other research, we have described how searching for information, reading and responding to e-mails, and collaborating with colleagues take up about 60 percent of typical knowledge workers’ time—and how they could become up to 25 percent more productive through the use of social technologies. Global IT-services supplier Atos has pledged to become a “zero e-mail” company by 2014, aiming to boost employee productivity by replacing internal e-mail with a collaborative social-networking platform.
        
        Companies also are becoming more porous, able to reach across units speedily and to assemble teams with specialized knowledge. Kraft Foods, for example, has invested in a more powerful social-technology platform that supports microblogging, content tagging, and the creation and maintenance of communities of practice (such as pricing experts). Benefits include accelerated knowledge sharing, shorter product-development cycles, and faster competitive response times. Companies still have ample running room, though: just 10 percent of the executives we surveyed last year said their organizations were realizing substantial value from the use of social technologies to connect all stakeholders: customers, employees, and business partners.
        
        Social features, meanwhile, can become part of any digital communication or transaction—embedded in products, markets, and business systems. Users can “like” things and may soon be able to register what they “want,” facilitating new levels of commercial engagement. Department-store chain Macy’s has used Facebook likes to decide on colors for upcoming apparel lines, while Wal-Mart Stores chooses its weekly toy specials through input from user panels. In broadcasting, Europe’s RTL Group is using social media to create viewer feedback loops for popular shows such as the X Factor. A steady stream of reactions from avid fans allows RTL to fine-tune episode plots.
        
        Indeed, our research suggests that when social perceptions and user experiences (both individual and collective) matter in product selection and satisfaction, the potential impact of social technologies on revenue streams can be pronounced. We are starting to see these effects in sectors ranging from automobiles to retailing as innovative companies mine social experiences to shape their products and services.
        
        Three years ago, we described new opportunities to experiment with and segment consumer markets using big data. As with the social matrix, we now see data and analytics as part of a new foundation for competitiveness. Global data volumes—surging from social Web sites, sensors, smartphones, and more—are doubling faster than every two years. The power of analytics is rising while costs are falling. Data visualization, wireless communications, and cloud infrastructure are extending the power and reach of information.
        
        With abundant data from multiple touch points and new analytic tools, companies are getting better and better at customizing products and services through the creation of ever-finer consumer microsegments. US-based Acxiom offers clients, from banks to auto companies, profiles of 500 million customers—each profile enriched by more than 1,500 data points gleaned from the analysis of up to 50 trillion transactions. Companies are learning to test and experiment using this type of data. They are borrowing from the pioneering efforts of companies such as Amazon.com or Google, continuously using what’s known as A/B testing not only to improve Web-site designs and experiences but also to raise real-world corporate performance. Many advanced marketing organizations are assembling data from real-time monitoring of blogs, news reports, and Tweets to detect subtle shifts in sentiment that can affect product and pricing strategy.
        
        Advanced analytic software allows machines to identify patterns hidden in massive data flows or documents. This machine “intelligence” means that a wider range of knowledge tasks may be automated at lower cost (see the fifth trend, below, for details). And as companies collect more data from operations, they may gain additional new revenue streams by selling sanitized information on spending patterns or physical activities to third parties ranging from economic forecasters to health-care companies.
        
        Despite the widespread recognition of big data’s potential, organizational and technological complexities, as well as the desire for perfection, often slow progress. Gaps between leaders and laggards are opening up as the former find new ways to test, learn, organize, and compete. For companies trying to keep pace, developing a big-data plan is becoming a critical new priority—one whose importance our colleagues likened, in a recent article, to the birth of strategic planning 40 years ago.
        
        Planning must extend beyond data strategy to encompass needed changes in organization and culture, the design of analytic and visualization tools frontline managers can use effectively, and the recruitment of scarce data scientists (which may require creative approaches, such as partnering with universities). Decisions about where corporate capabilities should reside, how external data will be merged with propriety information, and how to instill a culture of data-driven experimentation are becoming major leadership issues.
        
        Tiny sensors and actuators, proliferating at astounding rates, are expected to explode in number over the next decade, potentially linking over 50 billion physical entities as costs plummet and networks become more pervasive. What we described as nascent three years ago is fast becoming ubiquitous, which gives managers unimagined possibilities to fine-tune processes and manage operations.
        
        Through FedEx’s SenseAware program, for example, customers place a small device the size of a mobile phone into packages. The device includes a global positioning system, as well as sensors to monitor temperature, light, humidity, barometric pressure, and more—critical to some biological products and sensitive electronics. The customer knows continuously not only where a product is but also whether ambient conditions have changed. These new data-rich renditions of radio-frequency-identification (RFID) tags have major implications for companies managing complex supply chains.
        
        Companies are starting to use such technologies to run—not just monitor—complex operations, so that systems make autonomous decisions based on data the sensors report. Smart networks now use sensors to monitor vehicle flows and reprogram traffic signals accordingly or to confirm whether repairs have been made effectively in electric-power grids.
        
        New technologies are leading to what’s known as the “quantified self” movement, allowing people to become highly involved with their health care by using devices that monitor blood pressure and activity—even sleep patterns. Leading-edge ingestible sensors take this approach further, relaying information via smartphones to physicians, thereby providing new opportunities to manage health and disease.
        
        The buying and selling of services derived from physical products is a business-model shift that’s gaining steam. An attraction for buyers is the opportunity to replace big blocks of capital investment with more flexible and granular operating expenditures. A prominent example of this shift is the embrace of cloud-based IT services. Cosmetics maker Revlon, for example, now operates more than 500 of its IT applications in a private cloud built and operated by its IT team. It saved $70 million over two years, and when an entire factory, including a data center in Venezuela, was destroyed by a fire, the company was able to shift operations to New Jersey in under two hours. Moves like this, which suggest that cloud-delivered IT can be reliable and resilient, create new possibilities for the provision of mission-critical IT through internal or external assets and suppliers.
        
        This model is spreading beyond IT as a range of companies test ways to monetize underused assets by transforming them into services, benefitting corporate buyers that can sidestep owning them. Companies with trucking fleets, for instance, are creating new B2B businesses renting out idle vehicles by the day or the hour. And a growing number of companies with excess office space are finding that they can generate revenue by offering space for short-term uses. The Los Angeles Times has rented space to film crews, for example. Cloud-based online services are feeding the trend both by facilitating remote-work patterns that free up space and by connecting that space with organizations which need it.
        
        Other companies are seizing opportunities in consumer markets. Online services now allow rentals of everything from designer clothing and handbags to college textbooks. Home Depot rents out products from household tools to trucks. IT that can track usage and bill for services is what makes these models possible.
        
        While we and others have written about the importance of cloud-based IT services for some time, the potential impact of this trend is in its early stages. Companies have much to discover about the efficiencies and flexibility possible through reenvisioning their assets, whether that entails shifting from capital ownership to “expensed” services or assembling assets to play in this arena, as Amazon.com has done by offering server capacity to a range of businesses. Moreover, an understanding of what’s most amenable to being delivered as a service is still evolving—as are the attitudes and appetites of buyers. Thus, much of the disruption lies ahead.
        
        Physical labor and transactional tasks have been widely automated over the last three decades. Now advances in data analytics, low-cost computer power, machine learning, and interfaces that “understand” humans are moving the automation frontier rapidly toward the world’s more than 200 million knowledge workers.
        
        Powerful productivity-enhancing technologies already are taking root. Developments in how machines process language and understand context are allowing computers to search for information and find patterns of meaning at superhuman speed. At Clearwell Systems, a Silicon Valley company that analyzes legal documents for pretrial discovery, machines recently scanned more than a half million documents and pinpointed the 0.5 percent of them that were relevant for an upcoming trial. What would have taken a large team of lawyers several weeks took only three days. Machines also are becoming adept at structuring basic content for reports, automatically generating marketing and financial materials by scanning documents and data.
        
        Signaling a new milepost in the quest for artificial intelligence, IBM’s Jeopardy-winning computer Watson has turned its attention to cancer research. Watson “trained” for the work by reading more than 600,000 medical-evidence reports, 1.5 million patient records, and 2.0 million pages of clinical-trial reports and medical-journal articles. Now it is the backbone of a decision-support application for oncologists at Memorial Sloan-Kettering Cancer Center, in New York.
        
        At information-intensive companies, the culture and structure of the organization could change if machines start occupying positions along the knowledge-work value chain. Now is the time to begin planning for an era when the employee base might consist both of low-cost Watsons and of higher-priced workers with the judgment and technical skills to manage the new knowledge “workforce.” At the same time, business and government leaders will be jointly responsible for mitigating the destabilization caused by the displacement of knowledge workers and their reallocation to new roles. Retraining workers, redesigning education, and redefining the nature of work will all be important elements of this effort.
        
        As incomes rise in developing nations, their citizens are becoming wired, connected by mobile computing devices, particularly smartphones that will only increase in power and versatility. Although several emerging markets have experienced double-digit growth in Internet adoption, enormous growth potential remains: India’s digital penetration is only 10 percent and China’s is around 40 percent. Rising levels of connectivity will stimulate financial inclusion, local entrepreneurship, and enormous opportunities for business.
        
        As Internet-enabled smartphones and other mobile devices move rapidly down the cost curve, they will enable vast new applications and sources of value. A harbinger of the value to come is the success of mobile-payment services across a number of developing economies. Dutch–Bangla Bank Limited (DBBL), in Bangladesh, for example, garnered over a million mobile-payment subscribers in ten months. Standard Bank of South Africa reduced its origination costs for new customers by 80 percent using mobile devices.
        
        Another source of value is local matching services that connect supply with demand. Kenya’s Google-backed iHub project uses technology services to identify and finance entrepreneurs. Technology also helps multinationals adapt products and business models to local conditions. In India, Unilever provides mobile devices to rural distributors, including traditional mom-and-pop stores. The devices relay information (such as stock levels and pricing) back to the company, so Unilever can improve its demand forecasts, inventory management, and marketing strategy—raising sales in rural stores by a third.
        
        The borders of the digital and physical world have been blurring for many years as consumers learned to shop in virtual stores and to meet in virtual spaces. In those cases, the online world mirrors experiences of the physical world. Increasingly, we’re seeing an inversion as real-life activities, from shopping to factory work, become rich with digital information and as the mobile Internet and advances in natural user interfaces give the physical world digital characteristics.
        
        Today’s clever apps use smartphone technology to sense our locations and those of our friends or even allow us to point to foreign street signs for quick translations. Augmented reality will go further with next-generation wearable devices such as Google Glass, which deploys cameras and wireless connections to project information, on demand, through eyeglasses. Other wearable technologies are also gathering steam, from “intelligent textiles” to wristwatch computers that can not only display e-mails and texts but also run mobile apps. Technologies pioneered in game consoles allow us to use physical movements and gestures to interact with digital devices.
        
        Companies are applying these technologies to experiences that have remained resolutely physical, creating a new domain of customer interaction. Food retailers Tesco and Delhaize have deployed life-size store displays at South Korean and Belgian subway stations, respectively. The screens allow commuters waiting for trains to use smartphones to order groceries, which are then shipped to their homes or available for pickup at a physical store location. Other retailers are using similar displays in their physical stores so consumers can easily order out-of-stock products. Macy’s has installed “magic mirrors” in store dressing rooms: a 72-inch display that allows shoppers to “try on” clothes virtually to help them make their selection.
        
        Businesses are also integrating the digital world into physical work activities, thereby boosting their productivity and effectiveness. Boeing uses virtual-reality glasses so that factory workers assembling its 747 aircraft need to consult manuals less frequently. Annotated pop-ups point to drilling locations and display proper wire connections.
        
        Executives need to examine their businesses to find areas where immersive experiences or interactive touch points can stimulate engagement with “always on” customers. And they should reflect on the potential for interactive digital platforms to play roles in product design and marketing or in gathering customer feedback. These possibilities will grow in importance as customers and employees come to expect interaction between heightened digital and physical offerings.
        
        After nearly two decades of shopping, reading, watching, seeking information, and interacting on the Internet, customers expect services to be free, personalized, and easy to use without instructions. This ethos presents a challenge for business, since customers expect instant results, as well as superb and transparent customer service, for all interactions—from Web sites to brick-and-mortar stores. Fail to deliver, and competitors’ offerings are only an app download away.
        
        A number of businesses have battled in the free-services arena against tough digital competitors such as Craigslist, peer-to-peer music services, and Wikipedia. In 2012, Electronic Arts lost 400,000 players when it began charging for its online Star Wars game. Players came back when the company designed a “freemium” offer: users paid only after the first 50 levels. Additional challenges to traditional pricing power appear each day with comparative price apps that allow consumers to “showroom” at physical stores and then buy online at lower prices.
        
        Indeed, users will probably never pay for many valuable technology-enabled services, such as search—and the list seems to be growing rapidly. Providers of these “free” services will need to innovate with alternative business models. The most successful are likely to be multisided ones, which tap large profit pools that can be generated from information gathered by an adjacent free activity that’s commercially relevant. A familiar example is Google’s policy of offering its search services free of charge while garnering revenues at the other side of the platform by selling advertising or insights into customer behavior. In a world of free, the hunt is on for such monetization ideas. More and more companies, for example, are exploring opportunities to sell to third parties or to create new services based on sanitized information (“exhaust data”).
        
        Consumers, meanwhile, expect to be valued by companies and treated as individuals. In the online world, Spotify and Netflix analyze their customers’ histories to create “for me” experiences when recommending music and movies. Services are becoming even more hassle free online: new Web and mobile apps are designed to be so easy to use that instructions are no longer needed. The demand for “quick and easy” is compelling companies to modify how they deliver real-world offerings—for example, by allowing customers to photograph checks and deposit them using smartphone apps.
        
        A world of digitized instant gratification and low switching costs could force many businesses to seek innovative business models that provide more products and services free of charge or at lower cost. They’ll also have to think about offering more personalization in their products and services: customization at a mass level. This approach could require changes to back-end systems, which are often designed for mass production. Businesses will need new ways to collect information that furthers personalization, to embed experimentation into product-development efforts, and to ensure that offerings are easy to use—and even fun.
        
        The rise of the mobile Internet and the evolution of core technologies that cut costs and vastly simplify the process of completing transactions online are reducing barriers to entry across a wide swath of economic activity. Amped-up technology platforms are enabling peer-to-peer commerce to replace activities traditionally carried out by companies and giving birth to new kinds of payment systems and monetization models.
        
        Entry costs have fallen to the point where people who knit sweaters, for example, can tap into a global market of customers. Airbnb brokers deals between travelers and people with spare rooms to rent in their homes or apartments. It booked more than ten million overnight stays in 2012 and could soon be selling more room nights than major international hotel chains do. Similar marketplaces are springing up for bicycles, cars, labor, and more.
        
        Mobile-payment networks, sometimes augmented with services that extend beyond pure transactions, are a second area of evolution for e-commerce as costs fall. Starbucks envisions extending its pioneering use of smartphones for payments to include instant photo verification of buyers. New mobile-commerce platforms that manage transactions can offer customers the option of paying with credit credentials they established for other merchants. The mobile-payments provider Square offers customers using its service access to their sales data from any transaction and allows them to set up customer-loyalty programs easily.
        
        This trend will become more striking over the next decade or so: 600 cities, most in emerging markets, will account for roughly two-thirds of the world’s GDP growth. One likely consequence for fast-growing cities will be the rapid development of dense, digitally enabled commerce—new, highly evolved ecosystems combining devices, payment systems, digital and technology infrastructure, and logistics.
        
        The private sector has a big stake in the successful transformation of government, health care, and education, which together account for a third of global GDP. They have lagged behind in productivity growth at least in part because they have been slow to adopt Web-based platforms, big-data analytics, and other IT innovations. Technology-enabled productivity growth could help reduce the cost burden while improving the quality of services and outcomes, as well as boosting long-term global-growth prospects.
        
        Many governments are already using the Web to improve services and reduce waste. India has enrolled 380 million citizens in the world’s largest biometric-identity program, Aadhaar, and plans to use the system to make over $50 billion in cash transfers to poor citizens, saving $6 billion in fraudulent payments. In 2011, the US government introduced a Cloud First policy, which laid out a vision to shift a quarter of the $80 billion in annual federal spending to the cloud from in-house data centers, thus saving 20 to 30 percent on the cost of the shifted work. Governments can also use IT to better engage citizens, as South Korea has done with its e-People site, which helps citizens send online civil petitions for policy changes or reports of corruption.
        
        Technology also is opening new opportunities to contain rising health-care costs and improve access. In rural Bangladesh, 90 percent of births occur outside hospitals. A mobile-notification system alerts clinics to dispatch nurse–midwife teams, who are now present in 89 percent of births. In China, a public–private partnership created a cardiovascular-monitoring system that allows patients to self-administer electrocardiograms and transmit data to specialists in Beijing, who can suggest treatments by phone. At New York’s Mount Sinai Hospital, a venture with General Electric uses smart tags to track the flow of hundreds of patients, treatments, and medical assets in real time. The hospital estimates it could potentially treat 10,000 more patients each year as a result and generate $120 million in savings and revenues over several years.
        
        Finally, there’s education, which represents 4.5 percent of global GDP. Technology is starting to change the equation. Using game technologies and immersive math courseware, DreamBox makes learning more fun, while algorithms adapt the learning experience to each student’s needs. Brilliant.org allows talented mathematicians and physics students around the world to learn at their own pace. Global massive online open courses (MOOCs) offer university-level “classes” using social networks, videos, and community interactions.
        
        Smartphones and tablets are entering classrooms en masse to deliver personalized content. India is running trials of the sub-$50 Aakash tablet to link more than 25,000 colleges in an e-learning program. Other technologies are improving teachers’ skills and performance through online collaboration, access to best-in-class pedagogies, and better tracking of student achievement, which facilitates targeted interventions.
        
        What does all this mean for busy senior executives—beyond the obvious that there’s no escaping these trends, that they will continue to evolve, and that their implications, which will vary for different types of organizations, merit serious attention? We’d suggest that the era of pervasive connectedness underlying these trends also implies a need for more focused attention on issues such as the following:
        
        Transparent and innovative business models. Real-time information, instant price discovery, and quick problem resolution are becoming basic expectations of consumers, citizens, and business customers in the digital realm. Collectively, they will force many companies to rethink elements of their business models. Leaders will need to make their companies more transparent and elevate rapid responsiveness to the level of a core competency. Business models built on transparency and responsiveness will not only satisfy customers but also help companies become more nimble, innovative, and credible with all their stakeholders.
        
        Talent. The rising economic and business impact of information technology means that competition will heat up for graduates in science, technology, engineering, and mathematics—the STEM fields, where job growth is likely to be about 1.7 times faster than it will be in other areas. As the automation of knowledge work gains momentum, and computers start handling a growing number of tasks now performed by knowledge workers, some midlevel ones will probably be displaced and people with higher-level skills will become more important. Providing new forms of training to upgrade knowledge workers’ capabilities and rethinking the nature of public education will be critical priorities for business and government leaders.
        
        Organization. The Internet’s model and values, particularly connectivity and nonhierarchical interactions, have significant organizational implications. The flowering of many of these trends could imply decentralization, along with changing relationships among managers, employees, suppliers, and customers. These shifts aren’t always comfortable for leaders, but they hold the potential for boosting innovation, loyalty, business reach, productivity, and marketing effectiveness, while reducing costs.
        
        Privacy and security. Billions of people soon will be socializing, sharing information, and conducting transactions on the Internet. As businesses and governments use the Web to monitor assets, manage payments, and store data, they will be tracking moves individuals make on the Internet. Navigating the issues associated with generating economic utility while managing privacy will require organizations to examine trade-offs and address tensions in a clear, thoughtful way as rules of the road are established. Meanwhile, the value of the massive stores of digital information will only increase, giving criminals, terrorists, and even rogue states bigger incentives to breach firewalls and making the protection of data an imperative for top management. Keeping up with state-of-the-art encryption standards and security-management practices, for example, is moving from an arcane corner of data management to a core customer expectation, which, if not met, could severely damage a business’s reputation.
        
        In short, as these trends take hold, leaders must prepare for the disruption of long-standing commercial and social relationships, as well as the emergence of unforeseen business priorities. The difficulty of embracing those realities while addressing related risks and concerns may give some leaders pause. But it’s worth keeping in mind that if the future traces past experience, these technology-enabled business trends will not only be a boon for consumers but also stimulate growth, innovation, and a new wave of pace-setting companies.The payoff from joining the big-data and advanced-analytics management revolution is åno longer in doubt. The tally of successful case studies continues to build, reinforcing broader research suggesting that when companies inject data and analytics deep into their operations, they can deliver productivity and profit gains that are 5 to 6 percent higher than those of the competition. The promised land of new data-driven businesses, greater transparency into how operations actually work, better predictions, and faster testing is alluring indeed.
        
        But that doesn’t make it any easier to get from here to there. The required investment, measured both in money and management commitment, can be large. CIOs stress the need to remake data architectures and applications totally. Outside vendors hawk the power of black-box models to crunch through unstructured data in search of cause-and-effect relationships. Business managers scratch their heads—while insisting that they must know, upfront, the payoff from the spending and from the potentially disruptive organizational changes.
        
        The answer, simply put, is to develop a plan. Literally. It may sound obvious, but in our experience, the missing step for most companies is spending the time required to create a simple plan for how data, analytics, frontline tools, and people come together to create business value. The power of a plan is that it provides a common language allowing senior executives, technology professionals, data scientists, and managers to discuss where the greatest returns will come from and, more important, to select the two or three places to get started.
        
        There’s a compelling parallel here with the management history around strategic planning. Forty years ago, only a few companies developed well-thought-out strategic plans. Some of those pioneers achieved impressive results, and before long a wide range of organizations had harnessed the new planning tools and frameworks emerging at that time. Today, hardly any company sets off without some kind of strategic plan. We believe that most executives will soon see developing a data-and-analytics plan as the essential first step on their journey to harnessing big data.
        
        The essence of a good strategic plan is that it highlights the critical decisions, or trade-offs, a company must make and defines the initiatives it must prioritize: for example, which businesses will get the most capital, whether to emphasize higher margins or faster growth, and which capabilities are needed to ensure strong performance. In these early days of big-data and analytics planning, companies should address analogous issues: choosing the internal and external data they will integrate; selecting, from a long list of potential analytic models and tools, the ones that will best support their business goals; and building the organizational capabilities needed to exploit this potential.
        
        Successfully grappling with these planning trade-offs requires a cross-cutting strategic dialogue at the top of a company to establish investment priorities; to balance speed, cost, and acceptance; and to create the conditions for frontline engagement. A plan that addresses these critical issues is more likely to deliver tangible business results and can be a source of confidence for senior executives.
        
        A game plan for assembling and integrating data is essential. Companies are buried in information that’s frequently siloed horizontally across business units or vertically by function. Critical data may reside in legacy IT systems that have taken hold in areas such as customer service, pricing, and supply chains. Complicating matters is a new twist: critical information often resides outside companies, in unstructured forms such as social-network conversations.
        
        Making this information a useful and long-lived asset will often require a large investment in new data capabilities. Plans may highlight a need for the massive reorganization of data architectures over time: sifting through tangled repositories (separating transactions from analytical reports), creating unambiguous golden-source data, and implementing data-governance standards that systematically maintain accuracy. In the short term, a lighter solution may be possible for some companies: outsourcing the problem to data specialists who use cloud-based software to unify enough data to attack initial analytics opportunities.
        
        Integrating data alone does not generate value. Advanced analytic models are needed to enable data-driven optimization (for example, of employee schedules or shipping networks) or predictions (for instance, about flight delays or what customers will want or do given their buying histories or Web-site behavior). A plan must identify where models will create additional business value, who will need to use them, and how to avoid inconsistencies and unnecessary proliferation as models are scaled up across the enterprise.
        
        As with fresh data sources, companies eventually will want to link these models together to solve broader optimization problems across functions and business units. Indeed, the plan may require analytics “factories” to assemble a range of models from the growing list of variables and then to implement systems that keep track of both. And even though models can be dazzlingly robust, it’s important to resist the temptation of analytic perfection: too many variables will create complexity while making the models harder to apply and maintain.
        
        The output of modeling may be strikingly rich, but it’s valuable only if managers and, in many cases, frontline employees understand and use it. Output that’s too complex can be overwhelming or even mistrusted. What’s needed are intuitive tools that integrate data into day-to-day processes and translate modeling outputs into tangible business actions: for instance, a clear interface for scheduling employees, fine-grained cross-selling suggestions for call-center agents, or a way for marketing managers to make real-time decisions on discounts. Many companies fail to complete this step in their thinking and planning—only to find that managers and operational employees do not use the new models, whose effectiveness predictably falls.
        
        There’s also a critical enabler needed to animate the push toward data, models, and tools: organizational capabilities. Much as some strategic plans fail to deliver because organizations lack the skills to implement them, so too big-data plans can disappoint when organizations lack the right people and capabilities. Companies need a road map for assembling a talent pool of the right size and mix. And the best plans will go further, outlining how the organization can nurture data scientists, analytic modelers, and frontline staff who will thrive (and strive for better business outcomes) in the new data- and tool-rich environment.
        
        By assembling these building blocks, companies can formulate an integrated big-data plan similar to what’s summarized in the exhibit. Of course, the details of plans—analytic approaches, decision-support tools, and sources of business value—will vary by industry. However, it’s important to note an important structural similarity across industries: most companies will need to plan for major data-integration campaigns. The reason is that many of the highest-value models and tools (such as those shown on the right of the exhibit) increasingly will be built using an extraordinary range of data sources (such as all or most of those shown on the left). Typically, these sources will include internal data from customers (or patients), transactions, and operations, as well as external information from partners along the value chain and Web sites—plus, going forward, from sensors embedded in physical objects.
        
        Exhibit A successful data plan will focus on three core elements. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        To build a model that optimizes treatment and hospitalization regimes, a company in the health-care industry might need to integrate a wide range of patient and demographic information, data on drug efficacy, input from medical devices, and cost data from hospitals. A transportation company might combine real-time pricing information, GPS and weather data, and measures of employee labor productivity to predict which shipping routes, vessels, and cargo mixes will yield the greatest returns.
        
        Every plan will need to address some common challenges. In our experience, they require attention from the senior corporate leadership and are likely to sound familiar: establishing investment priorities, balancing speed and cost, and ensuring acceptance by the front line. All of these are part and parcel of many strategic plans, too. But there are important differences in plans for big data and advanced analytics.
        
        As companies develop their big-data plans, a common dilemma is how to integrate their “stovepipes” of data across, say, transactions, operations, and customer interactions. Integrating all of this information can provide powerful insights, but the cost of a new data architecture and of developing the many possible models and tools can be immense—and that calls for choices. Planners at one low-cost, high-volume retailer opted for models using store-sales data to predict inventory and labor costs to keep prices low. By contrast, a high-end, high-service retailer selected models requiring bigger investments and aggregated customer data to expand loyalty programs, nudge customers to higher-margin products, and tailor services to them.
        
        That, in a microcosm, is the investment-prioritization challenge: both approaches sound smart and were, in fact, well-suited to the business needs of the companies in question. It’s easy to imagine these alternatives catching the eye of other retailers. In a world of scarce resources, how to choose between these (or other) possibilities?
        
        There’s no substitute for serious engagement by the senior team in establishing such priorities. At one consumer-goods company, the CIO has created heat maps of potential sources of value creation across a range of investments throughout the company’s full business system—in big data, modeling, training, and more. The map gives senior leaders a solid fact base that informs debate and supports smart trade-offs. The result of these discussions isn’t a full plan but is certainly a promising start on one.
        
        Or consider how a large bank formed a team consisting of the CIO, the CMO, and business-unit heads to solve a marketing problem. Bankers were dissatisfied with the results of direct-marketing campaigns—costs were running high, and the uptake of the new offerings was disappointing. The heart of the problem, the bankers discovered, was a siloed marketing approach. Individual business units were sending multiple offers across the bank’s entire base of customers, regardless of their financial profile or preferences. Those more likely to need investment services were getting offers on a range of deposit products, and vice versa.
        
        The senior team decided that solving the problem would require pooling data in a cross-enterprise warehouse with data on income levels, product histories, risk profiles, and more. This central database allows the bank to optimize its marketing campaigns by targeting individuals with products and services they are more likely to want, thus raising the hit rate and profitability of the campaigns. A robust planning process often is needed to highlight investment opportunities like these and to stimulate the top-management engagement they deserve given their magnitude.
        
        A natural impulse for executives who “own” a company’s data and analytics strategy is to shift rapidly into action mode. Once some investment priorities are established, it’s not hard to find software and analytics vendors who have developed applications and algorithmic models to address them. These packages (covering pricing, inventory management, labor scheduling, and more) can be cost-effective and easier and faster to install than internally built, tailored models. But they often lack the qualities of a killer app—one that’s built on real business cases and can energize managers. Sector- and company-specific business factors are powerful enablers (or enemies) of successful data efforts. That’s why it’s crucial to give planning a second dimension, which seeks to balance the need for affordability and speed with business realities (including easy-to-miss risks and organizational sensitivities).
        
        To understand the costs of omitting this step, consider the experience of one bank trying to improve the performance of its small-business underwriting. Hoping to move quickly, the analytics group built a model on the fly, without a planning process involving the key stakeholders who fully understood the business forces at play. This model tested well on paper but didn’t work well in practice, and the company ran up losses using it. The leadership decided to start over, enlisting business-unit heads to help with the second effort. A revamped model, built on a more complete data set and with an architecture reflecting differences among various customer segments, had better predictive abilities and ultimately reduced the losses. The lesson: big-data planning is at least as much a management challenge as a technical one, and there’s no shortcut in the hard work of getting business players and data scientists together to figure things out.
        
        At a shipping company, the critical question was how to balance potential gains from new data and analytic models against business risks. Senior managers were comfortable with existing operations-oriented models, but there was pushback when data strategists proposed a range of new models related to customer behavior, pricing, and scheduling. A particular concern was whether costly new data approaches would interrupt well-oiled scheduling operations. Data managers met these concerns by pursuing a prototype (which used a smaller data set and rudimentary spreadsheet analysis) in one region. Sometimes, “walk before you can run” tactics like these are necessary to achieve the right balance, and they can be an explicit part of the plan.
        
        At a health insurer, a key challenge was assuaging concerns among internal stakeholders. A black-box model designed to identify chronic-disease patients with an above-average risk of hospitalization was highly accurate when tested on historical data. However, the company’s clinical directors questioned the ability of an opaque analytic model to select which patients should receive costly preventative-treatment regimes. In the end, the insurer opted for a simpler, more transparent data and analytic approach that improved on current practices but sacrificed some accuracy, with the likely result that a wider array of patients could qualify for treatment. Airing such tensions and trade-offs early in data planning can save time and avoid costly dead ends.
        
        Finally, some planning efforts require balancing the desire to keep costs down (through uniformity) with the need for a mix of data and modeling approaches that reflect business realities. Consider retailing, where players have unique customer bases, ways of setting prices to optimize sales and margins, and daily sales patterns and inventory requirements. One retailer, for instance, has quickly and inexpensively put in place a standard next-product-to-buy model for its Web site. But to develop a more sophisticated model to predict regional and seasonal buying patterns and optimize supply-chain operations, the retailer has had to gather unstructured consumer data from social media, to choose among internal-operations data, and to customize prediction algorithms by product and store concept. A balanced big-data plan embraces the need for such mixed approaches.
        
        Even after making a considerable investment in a new pricing tool, one airline found that the productivity of its revenue-management analysts was still below expectations. The problem? The tool was too complex to be useful. A different problem arose at a health insurer: doctors rejected a Web application designed to nudge them toward more cost-effective treatments. The doctors said they would use it only if it offered, for certain illnesses, treatment options they considered important for maintaining the trust of patients.
        
        Problems like these arise when companies neglect a third element of big-data planning: engaging the organization. As we said when describing the basic elements of a big-data plan, the process starts with the creation of analytic models that frontline managers can understand. The models should be linked to easy-to-use decision-support tools—call them killer tools—and to processes that let managers apply their own experience and judgment to the outputs of models. While a few analytic approaches (such as basic sales forecasting) are automatic and require limited frontline engagement, the lion’s share will fail without strong managerial support.
        
        The aforementioned airline redesigned the software interface of its pricing tool to include only 10 to 15 rule-driven archetypes covering the competitive and capacity-utilization situations on major routes. Similarly, at a retailer, a red flag alerts merchandise buyers when a competitor’s Internet site prices goods below the retailer’s levels and allows the buyers to decide on a response. At another retailer, managers now have tablet displays predicting the number of store clerks needed each hour of the day given historical sales data, the weather outlook, and planned special promotions.
        
        But planning for the creation of such worker-friendly tools is just the beginning. It’s also important to focus on the new organizational skills needed for effective implementation. Far too many companies believe that 95 percent of their data and analytics investments should be in data and modeling. But unless they develop the skills and training of frontline managers, many of whom don’t have strong analytics backgrounds, those investments won’t deliver. A good rule of thumb for planning purposes is a 50–50 ratio of data and modeling to training.
        
        Part of that investment may go toward installing “bimodal” managers who both understand the business well and have a sufficient knowledge of how to use data and tools to make better, more analytics-infused decisions. Where this skill set exists, managers will of course want to draw on it. Companies may also have to create incentives that pull key business players with analytic strengths into data-leadership roles and then encourage the cross-pollination of ideas among departments. One parcel-freight company found pockets of analytical talent trapped in siloed units and united these employees in a centralized hub that contracts out its services across the organization.
        
        When a plan is in place, execution becomes easier: integrating data, initiating pilot projects, and creating new tools and training efforts occur in the context of a clear vision for driving business value—a vision that’s unlikely to run into funding problems or organizational opposition. Over time, of course, the initial plan will get adjusted. Indeed, one key benefit of big data and analytics is that you can learn things about your business that you simply could not see before.
        
        Here, too, there may be a parallel with strategic planning, which over time has morphed in many organizations from a formal, annual, “by the book” process into a more dynamic one that takes place continually and involves a broader set of constituents. Data and analytics plans are also too important to be left on a shelf. But that’s tomorrow’s problem; right now, such plans aren’t even being created. The sooner executives change that, the more likely they are to make data a real source of competitive advantage for their organizations.Big data and analytics have climbed to the top of the corporate agenda—with ample reason. Together, they promise to transform the way many companies do business, delivering performance improvements not seen since the redesign of core processes in the 1990s. As such, these tools and techniques will open new avenues of competitive advantage.
        
        Many executives, however, remain unsure about how to proceed. They’re not certain their organizations are prepared for the required changes, and a lot of companies have yet to fully exploit the data or analytics capabilities they currently possess.
        
        In this video, McKinsey director David Court describes a way forward, with insights that also appear in a recent Harvard Business Review article written together with McKinsey’s global managing director, Dominic Barton. Court advises companies to focus on the big decisions where better data and models will improve outcomes. Leaders also need to transform their organizations so that frontline managers are comfortable using the potent new tools. This article is also available in a summary format.
        
        The video and edited transcript below are excerpts from Court’s September 2012 conversation with McKinsey Publishing’s Frank Comes.
        
        Big data and analytics actually have been receiving attention for a few years, but the reason is changing. A few years ago, I thought the question was “We have all this data. Surely there’s something we can do with it.” Now the question is “I see my competitors exploiting this and I feel I’m getting behind.” And in fact, the people who say this are right.
        
        If you look at the advantages people get from using data and analytics—in terms of what they can do in pricing, what they can do in customer care, what they can do in segmentation, what they can do in inventory management—it’s not a little bit of a difference anymore. It’s a significant difference. And for that reason, the question being asked is “I’m behind. I don’t like it. Catch me up.”
        
        I get asked, “Who’s big data for?” And my answer is it’s for just about everybody. There are going to be data-based companies: Amazon, Google, Bloomberg. They’re great companies, and they have a lot of opportunity. But just because you’re not going to be a data company doesn’t mean you can’t exploit data analytics. And the key is to focus on the big decisions for which if you had better data, if you had better predictive ability, if you had a better ability to optimize, you’d make more money.
        
        So where have I been seeing data analytics recently? Well, the answer is in many places. Let me focus first on efforts to do better things with your customers. An airline optimizing what price it charges on each flight for any day of the week. A bank figuring out how to best do its customer care across the four or five channels that it has. Allowing customers to be able to ask questions and get better answers and to direct them. All of that is on the customer side of things.
        
        And then in operations, think of an airline or a railway scheduling its crews. Think of a retailer optimizing its supply chain for how much inventory to hold versus “What do I pay for my transportation costs?” All of that lends itself to big data—the need to model—but frontline managers have to be able to use it.
        
        So what’s the formula or what’s the key success factor for exploiting data analytics? From our work—and we’ve probably talked to 100 people—it always comes down to three things: data, models, transformation. Data is the creative use of internal and external data to give you a broader view on what is happening to your operations or your customer. Modeling is all about using that data to get workable models that can either help you predict better or allow you to optimize better in terms of your business.
        
        And the third success factor is about transforming the company to take advantage of that data in models. This is all about simple tools for managers—doubling down on the training for managers so they understand, have confidence in, and can use the tools. Transforming your company to take advantage of data and analytics is the hard part, OK?
        
        I always describe both a short-term problem and a medium-term problem. The short-term problem is that if you’ve developed a new model that predicts or optimizes, how do you get your frontline managers to use it? That’s always a combination of simple tools and training and things like that. Then there’s a medium-term challenge, which is “How do I upscale my company to be able to do this on a broader scale?”
        
        The question then is how to build what I’m going to call the “bimodal athlete.” And what I mean by this is, imagine that we go to a retailer and meet its buyers, or to a technology company or consumer company and meet the people that make the pricing decisions, or to somebody doing scheduling. Here you need people that have a sense of the business, and they need to be comfortable with using the data analytics. If you’re good at data analytics but you don’t have this feel for the business, you’ll make naïve decisions. If you’re comfortable with the feel of the business but you never use analytics, you’re just leaving a lot of money on the table that your competitors are going to be able to exploit. So the challenge is how to build that bimodal athlete and how to get the technical talent.
        
        There are several things you just have to do. The first is you need to focus. And what I mean by focus is, let’s take a pricing manager in a consumer services company or a buyer in a retailer. They have 22 things they do. Don’t try and change 22 things; try and change 2 or 3 things. Focus on part of the decision and focus, therefore, where the greatest economic leverage is in the business.
        
        The second is that you’ve got to make a decision support tool the frontline user understands and has confidence in. The moment you make it simple, understandable, then people start using it and you get better decisions. For a company, if you have 100,000 employees and you’ve got only 14 that actually know this stuff and how to use it, you’re not going to get sustainable change.
        
        You don’t have to have 100,000. But you might have to have 10,000, five years from now, that are comfortable with analytics. So, again, link it to the processes, get the metrics right, and make sure you build the capabilities across the company.By now, most companies recognize that they have opportunities to use data and analytics to raise productivity, improve decision making, and gain competitive advantage. "Analytics will define the difference between the losers and winners going forward," says Tim McGuire, a McKinsey director.
        
        But actually mapping out an analytics plan is complicated. You have to set a strategy; draw a detailed road map for investing in assets such as technology, tools, and data sets; and tackle the intrinsic challenges of securing commitment, reinventing processes, and changing organizational behavior. Our collection of content, which synthesizes key insights drawn from many analytics projects, sets out the key issues, whether you are launching a pilot project or a large-scale transformation.
        
        Video Building a data-driven organization Matt Ariker, COO, McKinsey Consumer Marketing Analytics Center
        
        "Big data: What's your plan?" sets out the imperative task: to develop a plan that brings together data, analytics, frontline tools, and people to create business value. Only by spending the time to craft a plan can executives establish a common language to focus on goals and on ways of getting started.
        
        Above, in the first of three videos, Tim McGuire sets out the triple challenge that companies face: deciding which data to use (and where outside your organization to look), handling analytics (and securing the right capabilities to do so), and using the insights you've gained to transform your operations. Misconceptions around these tasks trip up many companies.
        
        In another video, Matt Ariker, of McKinsey's Consumer Marketing Analytics Center, focuses on the human element: the skills needed; how to organize and integrate new capabilities, people, and roles; and the mind-set and behavioral changes organizations must make to become data driven. Finally, McKinsey expert Matthias Roggendorf outlines the essentials of a business case for implementing a data transformation.A big-data revolution is under way in health care. Start with the vastly increased supply of information. Over the last decade, pharmaceutical companies have been aggregating years of research and development data into medical databases, while payors and providers have digitized their patient records. Meanwhile, the US federal government and other public stakeholders have been opening their vast stores of health-care knowledge, including data from clinical trials and information on patients covered under public insurance programs. In parallel, recent technical advances have made it easier to collect and analyze information from multiple sources—a major benefit in health care, since data for a single patient may come from various payors, hospitals, laboratories, and physician offices.
        
        Fiscal concerns, perhaps more than any other factor, are driving the demand for big-data applications. After more than 20 years of steady increases, health-care expenses now represent 17.6 percent of GDP—nearly $600 billion more than the expected benchmark for a nation of the United States’s size and wealth. To discourage overutilization, many payors have shifted from fee-for-service compensation, which rewards physicians for treatment volume, to risk-sharing arrangements that prioritize outcomes. Under the new schemes, when treatments deliver the desired results, provider compensation may be less than before. Payors are also entering similar agreements with pharmaceutical companies and basing reimbursement on a drug’s ability to improve patient health. In this new environment, health-care stakeholders have greater incentives to compile and exchange information.
        
        While health-care costs may be paramount in big data’s rise, clinical trends also play a role. Physicians have traditionally used their judgment when making treatment decisions, but in the last few years there has been a move toward evidence-based medicine, which involves systematically reviewing clinical data and making treatment decisions based on the best available information. Aggregating individual data sets into big-data algorithms often provides the most robust evidence, since nuances in subpopulations (such as the presence of patients with gluten allergies) may be so rare that they are not readily apparent in small samples.
        
        Although the health-care industry has lagged behind sectors like retail and banking in the use of big data—partly because of concerns about patient confidentiality—it could soon catch up. First movers in the data sphere are already achieving positive results, which is prompting other stakeholders to take action, lest they be left behind. These developments are encouraging, but they also raise an important question: is the health-care industry prepared to capture big data’s full potential, or are there roadblocks that will hamper its use? (In a related video, McKinsey director Nicolaus Henke explains how analytics is transforming the practice of medicine.)
        
        Health-care stakeholders are well versed in capturing value and have developed many levers to assist with this goal. But traditional tools do not always take complete advantage of the insights that big data can provide. Unit-price discounts, for instance, are based primarily on contracting and negotiating leverage. And like most other well-established health-care value levers, they focus solely on reducing costs rather than improving patient outcomes. Although these tools will continue to play an important role, stakeholders will only benefit from big data if they take a more holistic, patient-centered approach to value, one that focuses equally on health-care spending and treatment outcomes. We have created five pathways to assist them in redefining value and identifying tools that are appropriate for the new era. They focus on the following concepts:
        
        Right living. Patients must be encouraged to play an active role in their own health by making the right choices about diet, exercise, preventive care, and other lifestyle factors.
        
        Patients must be encouraged to play an active role in their own health by making the right choices about diet, exercise, preventive care, and other lifestyle factors. Right care. Patients must receive the most timely, appropriate treatment available. In addition to relying heavily on protocols, right care requires a coordinated approach, with all caregivers having access to the same information and working toward the same goal to avoid duplication of effort and suboptimal treatment strategies.
        
        Patients must receive the most timely, appropriate treatment available. In addition to relying heavily on protocols, right care requires a coordinated approach, with all caregivers having access to the same information and working toward the same goal to avoid duplication of effort and suboptimal treatment strategies. Right provider. Any professionals who treat patients must have strong performance records and be capable of achieving the best outcomes. They should also be selected based on their skill sets and abilities rather than their job titles. For instance, nurses or physicians’ assistants may perform many tasks that do not require a doctor.
        
        Any professionals who treat patients must have strong performance records and be capable of achieving the best outcomes. They should also be selected based on their skill sets and abilities rather than their job titles. For instance, nurses or physicians’ assistants may perform many tasks that do not require a doctor. Right value. Providers and payors should continually look for ways to improve value while preserving or improving health-care quality. For example, they could develop a system in which provider reimbursement is tied to patient outcomes or undertake programs designed to eliminate wasteful spending.
        
        Providers and payors should continually look for ways to improve value while preserving or improving health-care quality. For example, they could develop a system in which provider reimbursement is tied to patient outcomes or undertake programs designed to eliminate wasteful spending. Right innovation. Stakeholders must focus on identifying new therapies and approaches to health-care delivery. They should also try to improve the innovation engines themselves—for instance, by advancing medicine and boosting R&D productivity.
        
        The value pathways evolve as new data become available, fostering a feedback loop. The concept of right care, for instance, could change if new data suggest that the standard protocol for a particular disease does not produce optimal results. And a change in one pathway could spur changes in others, since they are interdependent. An investigation into right value, for example, could reveal that patients are most likely to suffer costly complications after an appendectomy if their physician performs few of these operations. This finding could influence opinions not only about value but about the right provider to perform an appendectomy.
        
        Some health-care leaders have already captured value from big data by focusing on the concepts outlined in our pathways or have set the groundwork for doing so. Consider a few examples:
        
        Kaiser Permanente has fully implemented a new computer system, HealthConnect, to ensure data exchange across all medical facilities and promote the use of electronic health records. The integrated system has improved outcomes in cardiovascular disease and achieved an estimated $1 billion in savings from reduced office visits and lab tests.
        
        Blue Shield of California, in partnership with NantHealth, is improving health-care delivery and patient outcomes by developing an integrated technology system that will allow doctors, hospitals, and health plans to deliver evidence-based care that is more coordinated and personalized. This will help improve performance in a number of areas, including prevention and care coordination.
        
        AstraZeneca established a four-year partnership with WellPoint’s data and analytics subsidiary, HealthCore, to conduct real-world studies to determine the most effective and economical treatments for some chronic illnesses and common diseases. AstraZeneca will use HealthCore data, together with its own clinical-trial data, to guide R&D investment decisions. The company is also in talks with payors about providing coverage for drugs already on the market, again using HealthCore data as evidence.
        
        During a recent scan of the industry, we found that interest in big data is not confined to traditional players. Since 2010, more than 200 new businesses have developed innovative health-care applications. About 40 percent of these were aimed at direct health interventions or predictive capabilities. That’s a powerful new frontier for health-data applications, which historically focused more on data management and retrospective data analysis (exhibit).
        
        Exhibit Many innovative US health-care data applications move beyond retroactive reporting to interventions and predictive capabilities. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Some devices take patient monitoring to a new level. For instance, Asthmapolis has created a GPS-enabled tracker that records inhaler usage by asthmatics. The information is ported to a central database and used to identify individual, group, and population-based trends. The data are then merged with Centers for Disease Control and Prevention information about known asthma catalysts (such as high pollen counts in the Northeast or volcanic fog in Hawaii). Together, the information helps physicians develop personalized treatment plans and spot prevention opportunities.
        
        Another company, Ginger.io, offers a mobile application in which patients with select conditions agree, in conjunction with their providers, to be tracked through their mobile phones and assisted with behavioral-health therapies. The app records data about calls, texts, geographic location, and even physical movements. Patients also respond to surveys delivered over their smartphones. The Ginger.io application integrates patient data with public research on behavioral health from the National Institutes of Health and other sources. The insights obtained can be revealing—for instance, a lack of movement or other activity could signal that a patient feels physically unwell, and irregular sleep patterns (revealed through late-night calls or texts) may signal that an anxiety attack is imminent.
        
        To determine the opportunity of the new value pathways, we evaluated a range of health-care initiatives and assessed their potential impact as total annual cost savings, holding outcomes constant, using a 2011 baseline. If these early successes were scaled up to create systemwide impact, we estimate that the pathways could account for $300 billion to $450 billion in reduced health-care spending, or 12 to 17 percent of the $2.6 trillion baseline in US health-care costs.
        
        Even a few simple interventions can have an enormous impact when scaled up. In the “right living” pathway, for instance, we estimate that aspirin use by those at risk for coronary heart disease, combined with early cholesterol screening and smoking cessation, could reduce the total cost of their care by more than $30 billion. While these actions have been encouraged for some time, big data now enables faster identification of high-risk patients, more effective interventions, and closer monitoring.
        
        Our estimate of $300 billion to $450 billion in reduced health-care spending could be conservative, as many insights and innovations are still ahead. We have yet to fully understand subpopulation efficacy of cancer therapies and the predictive indicators of relapse, for example, and we believe the big-data revolution will uncover many new learning opportunities in these areas.
        
        Although we are optimistic about big data’s potential to transform health care, some structural issues may pose obstacles. The move away from fee-for-service care—already well under way—must continue. Similarly, traditional medical-management techniques must change, since they pit payors and providers against each other, framing benefit plans with respect to what is and is not covered rather than what is and is not most effective. And all stakeholders must recognize the value of big data and be willing to act on its insights, a fundamental mind-set shift for many and one that may prove difficult to achieve. Patients will not benefit from research on exercise, for example, if they persist in their sedentary lifestyles. And physicians may not improve patient outcomes if they refuse to follow treatment protocols based on big data and instead rely solely on their own judgment.
        
        Privacy issues will continue to be a major concern. Although new computer programs can readily remove names and other personal information from records being transported into large databases, stakeholders across the industry must be vigilant and watch for potential problems as more information becomes public.
        
        Finally, health care will need to learn from other data-driven revolutions. All too often, players have taken advantage of data transparency by pursuing objectives that create value only for themselves, and this could also occur in the health-care sector. For instance, owners of MRI machines, looking to amortize fixed costs across more patients, might choose to use big data only to identify underserved patients and disease areas. If they convincingly market their services, patients may receive unnecessary MRIs—a situation that would increase costs without necessarily improving outcomes.
        
        Big-data initiatives have the potential to transform health care. Stakeholders that are committed to innovation, willing to build their capabilities, and open to a new view of value will likely be the first to reap the rewards of big data and help patients achieve better outcomes.
        
        Download the full report, The ‘big data’ revolution in healthcare: Accelerating value and innovation (PDF–1.4MB).After transforming customer-facing functions such as sales and marketing, big data is extending its reach to other parts of the enterprise. In research and development, for example, big data and analytics are being adopted across industries, including pharmaceuticals.
        
        The McKinsey Global Institute estimates that applying big-data strategies to better inform decision making could generate up to $100 billion in value annually across the US health-care system, by optimizing innovation, improving the efficiency of research and clinical trials, and building new tools for physicians, consumers, insurers, and regulators to meet the promise of more individualized approaches. (In a related video, McKinsey director Sam Marwaha outlines examples of how analytics is already changing the health-care industry.)
        
        The big-data opportunity is especially compelling in complex business environments experiencing an explosion in the types and volumes of available data. In the health-care and pharmaceutical industries, data growth is generated from several sources, including the R&D process itself, retailers, patients, and caregivers. Effectively utilizing these data will help pharmaceutical companies better identify new potential drug candidates and develop them into effective, approved and reimbursed medicines more quickly.
        
        Predictive modeling of biological processes and drugs becomes significantly more sophisticated and widespread. By leveraging the diversity of available molecular and clinical data, predictive modeling could help identify new potential-candidate molecules with a high probability of being successfully developed into drugs that act on biological targets safely and effectively.
        
        Patients are identified to enroll in clinical trials based on more sources—for example, social media—than doctors’ visits. Furthermore, the criteria for including patients in a trial could take significantly more factors (for instance, genetic information) into account to target specific populations, thereby enabling trials that are smaller, shorter, less expensive, and more powerful.
        
        Trials are monitored in real time to rapidly identify safety or operational signals requiring action to avoid significant and potentially costly issues such as adverse events and unnecessary delays.
        
        Instead of rigid data silos that are difficult to exploit, data are captured electronically and flow easily between functions, for example, discovery and clinical development, as well as to external partners, for instance, physicians and contract research organizations (CROs). This easy flow is essential for powering the real-time and predictive analytics that generate business value.
        
        That’s the vision. However, many pharmaceutical companies are wary about investing significantly in improving big-data analytical capabilities, partly because there are few examples of peers creating a lot of value from it. However, we believe investment and value creation will grow. The road ahead is indeed challenging, but the big-data opportunity in pharmaceutical R&D is real, and the rewards will be great for companies that succeed.
        
        Our research suggests that by implementing eight technology-enabled measures, pharmaceutical companies can expand the data they collect and improve their approach to managing and analyzing these data.
        
        Having data that are consistent, reliable, and well linked is one of the biggest challenges facing pharmaceutical R&D. The ability to manage and integrate data generated at all stages of the value chain, from discovery to real-world use after regulatory approval, is a fundamental requirement to allow companies to derive maximum benefit from the technology trends. Data are the foundation upon which the value-adding analytics are built. Effective end-to-end data integration establishes an authoritative source for all pieces of information and accurately links disparate data regardless of the source—be it internal or external, proprietary or publicly available. Data integration also enables comprehensive searches for subsets of data based on the linkages established rather than on the information itself. “Smart” algorithms linking laboratory and clinical data, for example, could create automatic reports that identify related applications or compounds and raise red flags concerning safety or efficacy.
        
        Implementing end-to-end data integration requires a number of capabilities, including trusted sources of data and documents, the ability to establish cross-linkages between elements, robust quality assurance, workflow management, and role-based access to ensure that specific data elements are visible only to those who are authorized to see it. Pharmaceutical companies generally avoid overhauling their entire data-integration system at once because of the logistical challenges and costs involved, although at least one global pharmaceutical enterprise has employed a “big bang” approach to remaking its clinical IT systems.
        
        Companies typically employ a two-step approach: first, they prioritize the specific data types to address (usually clinical data) and create additional data-warehousing capabilities as needed. The goal is to tackle the most important data first to obtain benefits as soon as possible. This step alone can take over a year and requires significant infrastructure and procedural changes. Second, the company develops an approach for the next levels of priority data, including scenario analysis, ownership, and expected costs and timelines.
        
        Pharmaceutical R&D has been a secretive activity conducted within the confines of the R&D department, with little internal and external collaboration. By breaking the silos that separate internal functions and enhancing collaboration with external partners, pharmaceutical companies can extend their knowledge and data networks.
        
        Whereas end-to-end integration aims to improve the linking of data elements, the goal of collaboration is to enhance the linkages among all stakeholders in drug research, development, commercialization, and delivery.
        
        Maximizing internal collaboration requires improved linkages among different functions, such as discovery, clinical development, and medical affairs. This can lead to insights across the portfolio, including clinical identification and research follow-up on potential opportunities in translational medicine or identification of personalized-medicine opportunities through the combination of biomarkers research and clinical outcomes; predictive sciences could also recommend options at the research stage based on clinical data or simulations.
        
        External collaborations are those between the company and stakeholders outside its four walls, including academic researchers, CROs, providers, and payors. Several examples show how effective external collaboration can broaden capabilities and insights:
        
        External partners, such as CROs, can quickly add or scale up internal capabilities and provide access to expertise in, for example, best-in-class management of clinical studies.
        
        Academic collaborators can share insights from the latest scientific breakthroughs and make a wealth of external innovation available. Examples include Eli Lilly’s Phenotypic Drug Discovery Initiative, which enables external researchers to submit their compounds for screening using Lilly’s proprietary tools and data to identify whether the compound is a potential drug candidate. Participation in the screening does not require the researcher to give up intellectual property, but it does offer Lilly a first look at new compounds, as well as an avenue to reach researchers who are not typical drug-discovery scientists.
        
        Collaborative “open space” initiatives can enable experts to address specific questions or share insights. Examples include the X PRIZE, which provides financial incentives for teams that successfully meet a big challenge (such as enabling low-cost manned space flight), and InnoCentive, which offers financial incentives for individuals or teams that address a specific problem (such as determining a compound’s synthesis pathway).
        
        Some pharmaceutical companies have made inroads in improving internal and external collaboration, which involves addressing a number of challenges. These include putting in place communications systems and governance to enable appropriate and effective information exchange. Another challenge is to promote a shift in mind-set, moving away from withholding all data and toward identifying which data can be shared and with whom. In addition, pharmaceutical enterprises must understand and mitigate the legal, regulatory, and intellectual-property risks associated with a more collaborative approach.
        
        Some pharmaceutical companies start to improve collaboration by identifying data elements to share with specific sets of trusted partners, such as CROs, and establishing privileged and near-real-time access to data produced by external partners. Such steps are only the beginning, however, as they are essentially just a way to expand the “circle of trust” to select partners.
        
        To ensure the appropriate allocation of scarce R&D funds, it is critical to enable expedited decision making for portfolio and pipeline progression. Pharmaceutical companies often find it challenging to make appropriate decisions about which assets to pursue or, sometimes more important, which assets to kill. The personnel or financial investments they have already made may influence decisions at the expense of merit, and they often lack appropriate decision-support tools to facilitate making tough calls.
        
        IT-enabled portfolio management allows data-driven decisions to be made quickly and seamlessly. Smart visual dashboards should be used whenever possible to allow rapid and effective decision making, including for the analysis of current projects, business-development opportunities, forecasting, and competitive information. These visual systems should provide high-level dashboards that permit users to deeply examine the data, including information to bolster managerial decision making as well as detailed tactical information, and that make asset performance and opportunities more transparent.
        
        In addition to the technical requirements, portfolio decision making should follow a defined process with known timing, deliverables, service levels, and stakeholders. The people involved in the process should be given clear roles and authority (for example, their ability to make decisions should be defined). Resource allocation should be based on a systematic approach that accommodates top-down budgetary requirements and bottom-up requests. And innovation boards at the corporate level and at the business-unit or therapeutic-area level should review the portfolio regularly. The boards should assess, manage, and prioritize the portfolio based on the corporate strategy and changes in the business landscape or industry context.
        
        Pharmaceutical R&D must continue to use cutting-edge tools. These include sophisticated modeling techniques such as systems biology and high-throughput data-production technologies—that is, technologies that produce a lot of data quickly, for example, next-generation sequencing, which, within 18 to 24 months, will make it possible to sequence an entire human genome at a cost of roughly $100.
        
        The wealth of new data and improved analytical techniques will enhance future innovation and feed the drug-development pipeline.
        
        Integrating vast amounts of new data will test a pharmaceutical company’s analytical capabilities. For example, a company will need to connect patient genotypes to clinical-trial results to identify opportunities for improving the identification of responsive patients. Such developments would make personalized medicine and diagnostics an integral part of the drug-development process rather than an afterthought and would lead to new discovery technologies and analytical techniques.
        
        Advances in instrumentation through miniaturized biosensors and the evolution in smartphones and their apps are resulting in increasingly sophisticated health-measurement devices. Pharmaceutical companies can deploy smart devices to gather large quantities of real-world data not previously available to scientists. Remote monitoring of patients through sensors and devices represents an immense opportunity. This kind of data could be used to facilitate R&D, analyze drug efficacy, enhance future drug sales, and create new economic models that combine the provision of drugs and services.
        
        Remote-monitoring devices can also add value by increasing patients’ adherence to their prescriptions. Examples of devices that are under development include smart pills that can release drugs and relay patient data, as well as smart bottles that help track usage. Technology and mobile providers are offering services such as data feeds, tracking, and analysis to complement medical devices. These devices and services, combined with in-home visits, have the potential to decrease health-care costs through shortened hospital stays and earlier identification of health issues.
        
        A combination of new, smarter devices and fluid data exchange will enable improvements in clinical-trial design and outcomes as well as greater efficiency. Clinical trials will become increasingly adaptable to react to drug-safety signals seen only in small but identifiable subpopulations of patients. Examples of potential clinical-trial efficiency gains include the following:
        
        Dynamic sample-size estimation (or reestimation) and other protocol changes could enable rapid responses to emerging insights from the clinical data. Efficiency gains are achieved by enabling smaller trials for equivalent power or shortening the time necessary to expand a trial.
        
        Adapting to differences in site patient-recruitment rates would allow a pharmaceutical company to address lagging sites, bring new sites online if necessary, and increase recruiting from more successful sites.
        
        Increased use of electronic data capture could help in recording patient information in the provider’s electronic medical records. Using electronic medical records as the primary source for clinical-trial data rather than having a separate system could accelerate trials and reduce the likelihood of data errors caused by manual or duplicate entry.
        
        Next-generation remote monitoring of sites, enabled by fluid, real-time data access, could improve management and responses to issues that arise in trials.
        
        Pharmaceutical companies can use safety as a competitive advantage in regulatory submissions and after regulatory approval, once the drug is on the market. Safety monitoring is moving beyond traditional approaches to sophisticated methods that identify possible safety signals arising from rare adverse events. Furthermore, signals could be detected from a range of sources, for example, patient inquiries on Web sites and search engines. Online physician communities, electronic medical records, and consumer-generated media are also potential sources of early signals regarding safety issues and can provide data on the reach and reputation of different medicines. Bayesian analytical methods, which can identify adverse events from incoming data, could highlight rare or ambiguous safety signals with greater accuracy and speed.
        
        An early response to physician and patient sentiments could prevent regulatory and public-relations backlashes. The FDA is investing in the evaluation of electronic health records through the Sentinel Initiative, a legally mandated electronic-surveillance system that links and analyzes health-care data from multiple sources. As part of this system, the FDA now has secured access to data concerning more than 120 million patients nationwide.
        
        Real-world outcomes are becoming more important to pharmaceutical companies as payors increasingly impose value-based pricing. These companies should respond to this cost-benefit pressure by pursuing drugs for which they can show differentiation through real-world outcomes, such as therapies targeted at specific patient populations. In addition, the FDA and other government organizations have created incentives for research on health economics and outcomes.
        
        To expand their data beyond clinical trials, some leading pharmaceutical companies are creating proprietary data networks to gather, analyze, share, and respond to real-world outcomes and claims data. Partnerships with payors, providers, and other institutions are critical to these efforts.
        
        For a big-data transformation in pharmaceutical R&D to succeed, executives must overcome several challenges.
        
        Organizational silos result in data silos. Functions typically have responsibility for their systems and the data they contain. Adopting a data-centric view, with a clear owner for each data type across functional silos and through the data life cycle, will greatly facilitate the ability to use and share data. The expertise gained by the data owner will be invaluable when developing ways to use existing information or to integrate internal and external data. Furthermore, having a single owner will enhance accountability for data quality. These organizational changes will be possible only if a company’s leadership understands the potential long-term value that can be unlocked through better use of internal and external data.
        
        Pharmaceutical companies are now saddled with legacy systems containing heterogeneous and disparate data. Increasing the ability to share data requires rationalizing and connecting these systems. There’s also a shortage of people equipped to develop the technology and analytics needed to extract maximum value from the existing data.
        
        Many pharmaceutical companies believe that unless they identify an ideal future state, there is little value to investing in improving big-data analytical capabilities. Indeed, they seem to fear being the first mover, since there are few examples of pharmaceutical companies creating a lot of value from the improved use of big data. Compounding their hesitation is concern about increasing interactions with regulators if they pursue a big-data change program. Pharmaceutical companies should learn from smaller, more entrepreneurial enterprises that see value in the incremental improvements that might emerge from small-scale pilots. The experience so obtained might yield long-term benefits and accelerate the path to the future state.
        
        Pharmaceutical companies desperately need to bolster R&D innovation and efficiency. By implementing these eight technology-enabled ways to benefit from big data, they could gradually turn the tide of declining success rates and stagnant pipelines.By now, most companies recognize that they have opportunities to use data and analytics to raise productivity, improve decision making, and gain competitive advantage. "Analytics will define the difference between the losers and winners going forward," says Tim McGuire, a McKinsey director.
        
        But actually mapping out an analytics plan is complicated. You have to set a strategy; draw a detailed road map for investing in assets such as technology, tools, and data sets; and tackle the intrinsic challenges of securing commitment, reinventing processes, and changing organizational behavior. Our collection of content, which synthesizes key insights drawn from many analytics projects, sets out the key issues, whether you are launching a pilot project or a large-scale transformation.
        
        Video Building a data-driven organization Matt Ariker, COO, McKinsey Consumer Marketing Analytics Center
        
        "Big data: What's your plan?" sets out the imperative task: to develop a plan that brings together data, analytics, frontline tools, and people to create business value. Only by spending the time to craft a plan can executives establish a common language to focus on goals and on ways of getting started.
        
        Above, in the first of three videos, Tim McGuire sets out the triple challenge that companies face: deciding which data to use (and where outside your organization to look), handling analytics (and securing the right capabilities to do so), and using the insights you've gained to transform your operations. Misconceptions around these tasks trip up many companies.
        
        In another video, Matt Ariker, of McKinsey's Consumer Marketing Analytics Center, focuses on the human element: the skills needed; how to organize and integrate new capabilities, people, and roles; and the mind-set and behavioral changes organizations must make to become data driven. Finally, McKinsey expert Matthias Roggendorf outlines the essentials of a business case for implementing a data transformation.Countries, sectors, and CEOs face an increasingly complex world characterized by the diffusion of new digital technologies, affecting productivity performance while causing disruption. In this world, the rewards for success—and the penalties for failure—are ever greater.
        
        In this new briefing note (PDF–179KB), the McKinsey Global Institute (MGI) draws on its extensive research on digitization that has looked at the importance of more inclusive infrastructure to broaden access to these technologies and at the emergence of new families of digital technologies including social media, enterprise 2.0, big data, and artificial intelligence (AI), and how these technologies are being diffused and affect economies, sectors, and firms.
        
        We illustrate these insights with up-to-date 2018 data from an annual series of global market research surveys conducted since 2015 that explore corporate practices in light of digitization. We have chosen to focus on the business and management aspects of digitization, but acknowledge, as we have made clear in much of our research, that digitization has broader effects notably on the future of work and on societal welfare.
        
        Large economic potential is linked to digitization—and much of it yet to be captured Digital superstars are rising far beyond the US big four and China’s big three Digital natives are calling the shots Digital changes everything—even industry boundaries Agile is the new way to compete Playing the platform economy is an “in the money” option Self-cannibalization and innovation are a necessity for digital reinvention Going after the right M&A is key Effective management of digital transformation is vital—but challenging Leveraging, and transitioning from, digital to new frontier technologies is an imperative
        
        Download the full briefing note, Twenty-five years of digitization: Ten insights into how to play it right (PDF–179KB) for additional details behind each insight.Is big data all hype? To the contrary: earlier research may have given only a partial view of the ultimate impact. A new report from the McKinsey Global Institute (MGI), The age of analytics: Competing in a data-driven world, suggests that the range of applications and opportunities has grown and will continue to expand. Given rapid technological advances, the question for companies now is how to integrate new capabilities into their operations and strategies—and position themselves in a world where analytics can upend entire industries.
        
        Video The age of analytics Big data continues to grow; if anything, earlier estimates understated its potential.
        
        A 2011 MGI report highlighted the transformational potential of big data. Five years later, we remain convinced that this potential has not been oversold. In fact, the convergence of several technology trends is accelerating progress. The volume of data continues to double every three years as information pours in from digital platforms, wireless sensors, virtual-reality applications, and billions of mobile phones. Data-storage capacity has increased, while its cost has plummeted. Data scientists now have unprecedented computing power at their disposal, and they are devising algorithms that are ever more sophisticated.
        
        Earlier, we estimated the potential for big data and analytics to create value in five specific domains. Revisiting them today shows uneven progress and a great deal of that value still on the table (exhibit). The greatest advances have occurred in location-based services and in US retail, both areas with competitors that are digital natives. In contrast, manufacturing, the EU public sector, and healthcare have captured less than 30 percent of the potential value we highlighted five years ago. And new opportunities have arisen since 2011, further widening the gap between the leaders and laggards.
        
        Exhibit We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Leading companies are using their capabilities not only to improve their core operations but also to launch entirely new business models. The network effects of digital platforms are creating a winner-take-most situation in some markets. The leading firms have remarkably deep analytical talent taking on various problems—and they are actively looking for ways to enter other industries. These companies can take advantage of their scale and data insights to add new business lines, and those expansions are increasingly blurring traditional sector boundaries.
        
        Where digital natives were built for analytics, legacy companies have to do the hard work of overhauling or changing existing systems. Adapting to an era of data-driven decision making is not always a simple proposition. Some companies have invested heavily in technology but have not yet changed their organizations so they can make the most of these investments. Many are struggling to develop the talent, business processes, and organizational muscle to capture real value from analytics.
        
        The first challenge is incorporating data and analytics into a core strategic vision. The next step is developing the right business processes and building capabilities, including both data infrastructure and talent. It is not enough simply to layer powerful technology systems on top of existing business operations. All these aspects of transformation need to come together to realize the full potential of data and analytics. The challenges incumbents face in pulling this off are precisely why much of the value we highlighted in 2011 is still unclaimed.
        
        The urgency for incumbents is growing, since leaders are staking out large advantages, and hesitating increases the risk of being disrupted. Disruption is already happening, and it takes multiple forms. Introducing new types of data sets (“orthogonal data”) can confer a competitive advantage, for instance, while massive integration capabilities can break through organizational silos, enabling new insights and models. Hyperscale digital platforms can match buyers and sellers in real time, transforming inefficient markets. Granular data can be used to personalize products and services—including, most intriguingly, healthcare. New analytical techniques can fuel discovery and innovation. Above all, businesses no longer have to go on gut instinct; they can use data and analytics to make faster decisions and more accurate forecasts supported by a mountain of evidence.
        
        The next generation of tools could unleash even bigger changes. New machine-learning and deep-learning capabilities have an enormous variety of applications that stretch into many sectors of the economy. Systems enabled by machine learning can provide customer service, manage logistics, analyze medical records, or even write news stories.
        
        These technologies could generate productivity gains and an improved quality of life, but they carry the risk of causing job losses and dislocations. Previous MGI research found that 45 percent of work activities could be automated using current technologies; some 80 percent of that is attributable to existing machine-learning capabilities. Breakthroughs in natural-language processing could expand that impact.
        
        Data and analytics are already shaking up multiple industries, and the effects will only become more pronounced as adoption reaches critical mass—and as machines gain unprecedented capabilities to solve problems and understand language. Organizations that can harness these capabilities effectively will be able to create significant value and differentiate themselves, while others will find themselves increasingly at a disadvantage.The amount of data in our world has been exploding, and analyzing large data sets—so-called big data—will become a key basis of competition, underpinning new waves of productivity growth, innovation, and consumer surplus, according to research by MGI and McKinsey's Business Technology Office. Leaders in every sector will have to grapple with the implications of big data, not just a few data-oriented managers. The increasing volume and detail of information captured by enterprises, the rise of multimedia, social media, and the Internet of Things will fuel exponential growth in data for the foreseeable future.
        
        Interactive Deep analytical talent: Where are they now? Research by MGI and McKinsey's Business Technology Office examines the state of digital data and documents the significant value that can potentially be unlocked. Open interactive popup
        
        MGI studied big data in five domains—healthcare in the United States, the public sector in Europe, retail in the United States, and manufacturing and personal-location data globally. Big data can generate value in each. For example, a retailer using big data to the full could increase its operating margin by more than 60 percent. Harnessing big data in the public sector has enormous potential, too. If US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year. Two-thirds of that would be in the form of reducing US healthcare expenditure by about 8 percent. In the developed economies of Europe, government administrators could save more than €100 billion ($149 billion) in operational efficiency improvements alone by using big data, not including using big data to reduce fraud and errors and boost the collection of tax revenues. And users of services enabled by personal-location data could capture $600 billion in consumer surplus. The research offers seven key insights.
        
        1. Data have swept into every industry and business function and are now an important factor of production, alongside labor and capital. We estimate that, by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data (twice the size of US retailer Wal-Mart's data warehouse in 1999) per company with more than 1,000 employees.
        
        2. There are five broad ways in which using big data can create value. First, big data can unlock significant value by making information transparent and usable at much higher frequency. Second, as organizations create and store more transactional data in digital form, they can collect more accurate and detailed performance information on everything from product inventories to sick days, and therefore expose variability and boost performance. Leading companies are using data collection and analysis to conduct controlled experiments to make better management decisions; others are using data for basic low-frequency forecasting to high-frequency nowcasting to adjust their business levers just in time. Third, big data allows ever-narrower segmentation of customers and therefore much more precisely tailored products or services. Fourth, sophisticated analytics can substantially improve decision-making. Finally, big data can be used to improve the development of the next generation of products and services. For instance, manufacturers are using data obtained from sensors embedded in products to create innovative after-sales service offerings such as proactive maintenance (preventive measures that take place before a failure occurs or is even noticed).
        
        00:00 Podcast Distilling value and driving productivity from mountains of data Michael Chui discusses how the scale and scope of companies' access to data is changing the way they do business.
        
        3. The use of big data will become a key basis of competition and growth for individual firms. From the standpoint of competitiveness and the potential capture of value, all companies need to take big data seriously. In most industries, established competitors and new entrants alike will leverage data-driven strategies to innovate, compete, and capture value from deep and up-to-real-time information. Indeed, we found early examples of such use of data in every sector we examined.
        
        4. The use of big data will underpin new waves of productivity growth and consumer surplus. For example, we estimate that a retailer using big data to the full has the potential to increase its operating margin by more than 60 percent. Big data offers considerable benefits to consumers as well as to companies and organizations. For instance, services enabled by personal-location data can allow consumers to capture $600 billion in economic surplus.
        
        5. While the use of big data will matter across sectors, some sectors are set for greater gains. We compared the historical productivity of sectors in the United States with the potential of these sectors to capture value from big data (using an index that combines several quantitative metrics), and found that the opportunities and challenges vary from sector to sector. The computer and electronic products and information sectors, as well as finance and insurance, and government are poised to gain substantially from the use of big data.
        
        6. There will be a shortage of talent necessary for organizations to take advantage of big data. By 2018, the United States alone could face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of big data to make effective decisions.
        
        7. Several issues will have to be addressed to capture the full potential of big data. Policies related to privacy, security, intellectual property, and even liability will need to be addressed in a big data world. Organizations need not only to put the right talent and technology in place but also structure workflows and incentives to optimize the use of big data. Access to data is critical—companies will increasingly need to integrate information from multiple data sources, often from third parties, and the incentives have to be in place to enable this.In a few short years, social technologies have given social interactions the speed and scale of the Internet. Whether discussing consumer products or organizing political movements, people around the world constantly use social-media platforms to seek and share information. Companies use them to reach consumers in new ways too; by tapping into these conversations, organizations can generate richer insights and create precisely targeted messages and offers.
        
        While 72 percent of companies use social technologies in some way, very few are anywhere near to achieving the full potential benefit. In fact, the most powerful applications of social technologies in the global economy are largely untapped. Companies will go on developing ways to reach consumers through social technologies and gathering insights for product development, marketing, and customer service. Yet the McKinsey Global Institute (MGI) finds that twice as much potential value lies in using social tools to enhance communications, knowledge sharing, and collaboration within and across enterprises. MGI’s estimates suggest that by fully implementing social technologies, companies have an opportunity to raise the productivity of interaction workers—high-skill knowledge workers, including managers and professionals—by 20 to 25 percent.
        
        Exhibit Improved communication and collaboration through social technologies could raise the productivity of interaction workers by 20 to 25 percent. We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        MGI’s report, The social economy: Unlocking value and productivity through social technologies, explores their potential economic impact by examining their current usage and evolving application in four commercial sectors: consumer packaged goods, retail financial services, advanced manufacturing, and professional services. These technologies, which create value by improving productivity across the value chain, could potentially contribute $900 billion to $1.3 trillion in annual value across the four sectors.
        
        00:00 Podcast Social media’s untapped productivity payoff MGI principal Michael Chui discusses the potential value in using social tools to enhance communications, knowledge sharing, and collaboration within and across enterprises.
        
        Two-thirds of this potential value lies in improving collaboration and communication within and across enterprises. The average interaction worker spends an estimated 28 percent of the workweek managing e-mail and nearly 20 percent looking for internal information or tracking down colleagues who can help with specific tasks. But when companies use social media internally, messages become content; a searchable record of knowledge can reduce, by as much as 35 percent, the time employees spend searching for company information. Additional value can be realized through faster, more efficient, more effective collaboration, both within and between enterprises.
        
        The amount of value individual companies can capture from social technologies varies widely by industry, as do the sources of value. Companies that have a high proportion of interaction workers can realize tremendous productivity improvements through faster internal communication and smoother collaboration. Companies that depend very heavily on influencing consumers can derive considerable value by interacting with them in social media and by monitoring the conversations to gain a richer perspective on product requirements or brand image—for much less than what traditional research methods would cost.
        
        To reap the full benefit of social technologies, organizations must transform their structures, processes, and cultures: they will need to become more open and nonhierarchical and to create a culture of trust. Ultimately, the power of social technologies hinges on the full and enthusiastic participation of employees who are not afraid to share their thoughts and trust that their contributions will be respected. Creating these conditions will be far more challenging than implementing the technologies themselves.In a paper prepared for the Foreign Commonwealth Office International Cyber Conference, MGI examines what more can be done to fully capture the benefits of the Internet.
        
        The Internet is changing the way we work, socialize, create and share information, and organize the flow of people, ideas, and things around the globe. Yet the magnitude of this transformation is still underappreciated. The Internet accounted for 21 percent of the GDP growth in mature economies over the past 5 years. In that time, we went from a few thousand students accessing Facebook to more than 800 million users around the world, including many leading firms, who regularly update their pages and share content. While large enterprises and national economies have reaped major benefits from this technological revolution, individual consumers and small, upstart entrepreneurs have been some of the greatest beneficiaries from the Internet’s empowering influence. If Internet were a sector, it would have a greater weight in GDP than agriculture or utilities.
        
        And yet we are still in the early stages of the transformations the Internet will unleash and the opportunities it will foster. Many more technological innovations and enabling capabilities such as payments platforms are likely to emerge, while the ability to connect many more people and things and engage them more deeply will continue to expand exponentially.
        
        As a result, governments, policy makers, and businesses must recognize and embrace the enormous opportunities the Internet can create, even as they work to address the risks to security and privacy the Internet brings. As the Internet’s evolution over the past two decades has demonstrated, such work must include helping to nurture the development of a healthy Internet ecosystem, one that boosts infrastructure and access, builds a competitive environment that benefits users and lets innovators and entrepreneurs thrive, and nurtures human capital. Together these elements can maximize the continued impact of the Internet on economic growth and prosperitySkepticism about the "new economy" following the collapse of the tech bubble called for a sober assessment of what the actual role of technology and IT was in productivity. MGI's report found that IT was only one of a number of factors that led to significant productivity growth.
        
        The labor productivity growth rate in the retail sector was more than twice that of the rest of the US economy in the 1990s. But that productivity was unevenly distributed across different categories of retail.
        
        Despite growth in IT spending in the banking sector from 1995–1999, labor productivity actually decreased over the same period of time. Retail banking did, however, see a high baseline of productivity growth when compared to the overall US economy.
        
        Labor productivity in the semiconductor industry saw labor productivity growth rates 35 times that of the US average in the 1990s. Changes in output quality were the main drivers of this growth.Every new technology has its skeptics. In the 1980s, many observers doubted that the broad use of information technologies such as enterprise resource planning (ERP) to remake processes would pay off in productivity improvements—indeed, the economist Robert Solow famously remarked, “You can see the computer age everywhere but in the productivity statistics.” Today, that sentiment has gravitated to Web 2.0 technologies. Management is trying to understand if they are a passing fad or an enduring trend that will underwrite a new era of better corporate performance.
        
        New McKinsey research shows that a payday could be arriving faster than expected. A new class of company is emerging—one that uses collaborative Web 2.0 technologies intensively to connect the internal efforts of employees and to extend the organization’s reach to customers, partners, and suppliers. We call this new kind of company the networked enterprise. Results from our analysis of proprietary survey data show that the Web 2.0 use of these companies is significantly improving their reported performance. In fact, our data show that fully networked enterprises are not only more likely to be market leaders or to be gaining market share but also use management practices that lead to margins higher than those of companies using the Web in more limited ways.
        
        Sidebar Managing the Web-based organization Respondents report that a variety of organizational structures and units manage Web 2.0. This year’s results show that the IT department is most likely to oversee internal Web initiatives (61 percent of respondents). For customer-facing initiatives, 74 percent of respondents say that oversight falls to the marketing department. For Web 2.0 initiatives involving external suppliers and partners, roughly equal numbers of respondents cite the IT, marketing, and business-development functions. Financing comes from a variety of places, including the IT function, central corporate sources, and discretionary funds at the business unit level. The social nature of most Web technologies, of course, opens companies to greater interaction with the outside world. To manage this change, a slim majority of respondents (51 percent) say their companies have adopted formal social-media policies; companies with higher levels of Web 2.0 adoption are likelier to have them. In most cases, only a few employees are authorized to speak on behalf of the company.
        
        Over the past four years, McKinsey has studied how enterprises use these social technologies, which first took hold in business-to-consumer models that gave rise to Web companies such as YouTube and Facebook. Recently, the technologies have been migrating into the enterprise, with the promise of creating new gains to augment those generated by the earlier wave of IT adoptions. The patterns of adoption and diffusion for the social Web’s enterprise applications appear to resemble those of earlier eras: a classic S curve, in which early adopters learn to use a new technology, and adoption then picks up rapidly as others begin to recognize its value. The implications are far reaching: in many industries, new competitive battle lines may form between companies that use the Web in sophisticated ways and companies that feel uncomfortable with new Web-inspired management styles or simply can’t execute at a sufficiently high level (see sidebar, “Managing the Web-based organization”).
        
        Our annual surveys of Web 2.0 use in the enterprise provided the basis for the findings in this article. The present survey, our fourth, garnered responses from 3,249 executives across a range of regions, industries, and functional areas. Two-thirds of the respondents reported using Web 2.0 in their organizations. As in past surveys, we asked respondents about their patterns of Web 2.0 use, the measurable business benefits they derived from it, and the organizational impact of Web technologies. We also inquired about the market position of the respondents’ companies, whether their market share had changed, and how their operating margins compared with those of competitors in the same industries.
        
        The share of companies where respondents report using Web 2.0 technologies continues to grow. Our research, for instance, shows significant increases in the percentage of companies using social networking (40 percent) and blogs (38 percent). Furthermore, our surveys show that the number of employees using the dozen Web 2.0 technologies continues to increase. Respondents at nearly half of the companies that use social networking say, for example, that at least 51 percent of their employees use it. And in 2010, nearly two-thirds of respondents at companies using Web 2.0 say they will increase future investments in these technologies, compared with just over half in 2009. The healthy spending plans during both of these difficult years underscore the value companies expect to gain.
        
        Among respondents at companies using Web 2.0, a large majority continue to report that they are receiving measurable business benefits—with nearly nine out of ten reporting at least one. These benefits ranged from more effective marketing to faster access to knowledge (Exhibit 1).
        
        Exhibit 1 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        We analyzed the shared characteristics of groups of organizations in our survey and clustered them according to the magnitude of the business benefits respondents reported from the use of Web 2.0 tools and technologies. Our analysis revealed striking differences.
        
        Among respondents who say their companies are using Web 2.0, most (79 percent) achieved a mean improvement of 5 percent or less across a range of business benefit metrics (Exhibit 2). Respondents at the companies in this group report the lowest percentages of usage among their employees, customers, and business partners; say that Web 2.0 is less integrated into their employees’ day-to-day work than respondents at other companies do; and are least likely to report high levels of collaboration or information sharing across the organization. We call these companies, still learning the ropes of Web 2.0, the “developing” group.
        
        Exhibit 2 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Three types of organizations, however, seem to have learned how to realize a much higher level of business benefits from their use of Web 2.0.
        
        Internally networked organizations. Some companies are achieving benefits from using Web 2.0 primarily within their own corporate walls. The survey results indicate that companies in this group—13 percent of those using Web 2.0—derive substantial benefits from deploying these technologies in employee interactions. Respondents at such organizations report a higher percentage of employees using Web 2.0 than respondents at developing organizations do. Respondents at half of the internally networked organizations reported that Web 2.0 is integrated tightly into their work flows, for example, compared with only 21 percent of respondents at developing organizations. Web 2.0 also seems to promote significantly more flexible processes at internally networked organizations: respondents say that information is shared more readily and less hierarchically, collaboration across organizational silos is more common, and tasks are more often tackled in a project-based fashion.
        
        Externally networked organizations. Other companies (5 percent of those deploying Web 2.0) achieved substantial benefits from interactions that spread beyond corporate borders by using Web 2.0 technologies to interact with customers and business partners, according to survey results. Executives at these organizations reported larger percentages of their employees, customers, and partners using Web 2.0 than respondents at internally networked organizations did. But the responses suggest that the internal organizational processes of externally networked organizations are less fluid than those of internally networked ones.
        
        Fully networked enterprises. Finally, some companies use Web 2.0 in revolutionary ways. This elite group of organizations—3 percent of those in our survey—derives very high levels of benefits from Web 2.0’s widespread use, involving employees, customers, and business partners, according to the survey. Respondents at these organizations reported higher levels of employee benefits than internally networked organizations did and higher levels of customer and partner benefits than did externally networked organizations. In applying Web 2.0 technologies, fully networked enterprises seem to have moved much further along the learning curve than other organizations have. The integration of Web 2.0 into day-to-day activities is high, executives say, and they report that these technologies are promoting higher levels of collaboration by helping to break down organizational barriers that impede information flows.
        
        Executives at the more highly networked companies in our survey reported that they captured a broad set of benefits from their Web investments. A key question remained, however: do these benefits translate into fundamental performance improvements, measured by self-reported market share gains and higher profits?
        
        We performed a series of statistical analyses to better understand the relationship between our categories of networked organizations and three core self-reported performance metrics: market share gains, operating profits, and market leadership. Exhibit 3 shows the results.
        
        Market share gains reported by respondents were significantly correlated with fully networked and externally networked organizations. This, we believe, is statistically significant evidence that technology-enabled collaboration with external stakeholders helps organizations gain market share from the competition. They do this, in our experience, by forging closer marketing relationships with customers and by involving them in customer support and product-development efforts. Respondents at companies that used Web 2.0 to collaborate across organizational silos and to share information more broadly also reported improved market shares.
        
        Exhibit 3 We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        The attainment of higher operating margins (again, self-reported) than competitors correlated with a different set of factors: the ability to make decisions lower in the corporate hierarchy and a willingness to allow the formation of working teams comprising both in-house employees and individuals outside the organization. These findings suggest that Web technologies can underwrite a more agile organization where frontline staff members make local decisions and companies are better at leveraging outside resources to raise productivity and to create more valuable products and services. The result, the survey suggests, is higher profits.
        
        Market leadership, which we ascribed to those organizations where respondents reported a top ranking in industry market share, correlated positively with internally networked organizations that have high levels of organizational collaboration. Self-reported market leadership also, however, correlated negatively with externally networked organizations. We believe it is unlikely that better interactions with external stakeholders lead to a decline in market position. A more likely explanation for the data is that market leaders use Web 2.0 to strengthen internal collaboration, seeking to enhance the organizational resiliency required to maintain their leadership positions. Market challengers, by contrast, may be more focused on external uses of Web 2.0 to win customers from industry leaders.
        
        Overall, we found that respondents at 27 percent of the companies in our survey reported having both market share gains against their competitors and higher profit margins. That kind of performance clearly makes these companies profit consolidators in their industries, with earnings growing faster than the rest. Highly networked enterprises were 50 percent more likely to fall in this high-performance group than other organizations were. This finding suggests that the fully networked enterprise could become the benchmark for more vigorous competition in many industries.
        
        Moreover, the benefits from the use of collaborative technologies at fully networked organizations appear to be multiplicative in nature: these enterprises seem to be “learning organizations” in which lessons from interacting with one set of stakeholders in turn improve the ability to realize value in interactions with others. If this hypothesis is correct, competitive advantage at these companies will accelerate as network effects kick in, network connections become richer, and learning cycles speed up.
        
        The imperative for business leaders is clear: falling behind in creating internal and external networks could be a critical mistake. Executives need to push their organizations toward becoming fully networked enterprises. Our research suggests some specific steps:
        
        Integrate the use of Web 2.0 into employees’ day-to-day work activities. This practice is the key success factor in all of our analyses, as well as other research we have done. What’s in the work flow is what gets used by employees and what leads to benefits.
        
        Continue to drive adoption and usage. Benefits appear to be limited without a base level of adoption and usage. Respondents who reported the lowest levels of both also reported the lowest levels of benefits.
        
        Break down the barriers to organizational change. Fully networked organizations appear to have more fluid information flows, deploy talent more flexibly to deal with problems, and allow employees lower in the corporate hierarchy to make decisions. Organizational collaboration is correlated with self-reported market share gains; distributed decision making and work, with increased self-reported profitability.
        
        Apply Web 2.0 technologies to interactions with customers, business partners, and employees. External interactions are correlated with self-reported market share gains. So are internal organizational collaboration and flexibility, and the benefits appear to be multiplicative. Fully networked organizations can achieve the highest levels of self-reported benefits in all types of interactions.
        
        Jim Smith is executive vice president and managing head of Internet Services Group at Wells Fargo, which began its online strategy 16 years ago. Today, the bank is moving forward using many elements of the networked enterprise—launching banking via mobile devices and deploying social technologies such as Twitter and blogs to forge broader relationships with its customers. Better data on consumers’ needs are helping the bank create higher-value services, and over time, Smith says, the goal is to involve a greater numbers of bankers in Web interactions. In this podcast, Smith discusses Wells Fargo’s Web strategy and the cultural changes it has wrought.
        
        Robert Stephens is chief technology officer of the electronics retailer Best Buy and founder of Geek Squad, the company’s technical-services business. Stephens envisions social and mobile technologies as key tools to helping Best Buy become a vast knowledge network where customers would have access to employees worldwide, both to inform themselves about products and to find answers to problems. Creating a networked enterprise, Stephens believes, means that all employees—from the executive suite down to the store level—need to be committed users of emerging technologies. Above all, those who interact directly with customers need to be curious about what consumers really want. In this podcast, Stephens discusses Best Buy’s initiatives and how the networked enterprise creates competitive advantage.Over the past three years, we have tracked the rising adoption of Web 2.0 technologies, as well as the ways organizations are using them. This year, we sought to get a clear idea of whether companies are deriving measurable business benefits from their investments in the Web. Our findings indicate that they are.
        
        Nearly 1,700 executives from around the world, across a range of industries and functional areas, responded to this year’s survey. We asked them about the value they have realized from their Web 2.0 deployments in three main areas: within their organizations; externally, in their relations with customers; and in their dealings with suppliers, partners, and outside experts.
        
        Their responses suggest why Web 2.0 remains of high interest: 69 percent of respondents report that their companies have gained measurable business benefits, including more innovative products and services, more effective marketing, better access to knowledge, lower cost of doing business, and higher revenues. Companies that made greater use of the technologies, the results show, report even greater benefits. We also looked closely at the factors driving these improvements—for example, the types of technologies companies are using, management practices that produce benefits, and any organizational and cultural characteristics that may contribute to the gains. We found that successful companies not only tightly integrate Web 2.0 technologies with the work flows of their employees but also create a “networked company,” linking themselves with customers and suppliers through the use of Web 2.0 tools. Despite the current recession, respondents overwhelmingly say that they will continue to invest in Web 2.0.
        
        Sidebar Web 2.0’s Power Curves Web 2.0 technologies improve interactions with employees, customers, and suppliers at some companies more than at others. An outside study titled “Power Law of Enterprise 2.0” analyzed data from earlier McKinsey Web 2.0 surveys to gain a better understanding of the factors that contribute most significantly to the successful use of these technologies. The findings demonstrate that success follows a “power curve distribution”—in other words, a small group of users accounts for the largest portion of the gains. According to our research, the 20 percent of users reporting the greatest satisfaction received 80 percent of the benefits. Drilling a bit deeper, we found that this 20 percent included 68 percent of the companies reporting the highest adoption rates for a range of Web 2.0 tools, 58 percent of the companies where use by employees was most widespread, and 82 percent of the respondents who claimed the highest levels of satisfaction from Web 2.0 use at their companies. To improve our understanding of some underlying factors leading to these companies’ success, we first created an index of Web 2.0 performance, combining the previously mentioned variables: adoption, breadth of employee use, and satisfaction. A score of 100 percent represents the highest performance level possible across the three components. We then analyzed how these scores correlated with three company characteristics: the competitive environment (using industry type as a proxy), company features (the size and location of operations), and the extent to which the company actively managed Web 2.0. These three factors explained two-thirds of the companies’ scores. Furthermore, while all of the factors are slightly correlated with one another—for example, there are more high-tech companies in the United States than in South America—each factor by itself explains much of why companies achieved their performance scores. Management capabilities ranked highest at 54 percent, meaning that good management is more than half of the battle in ensuring satisfaction with Web 2.0, a high rate of adoption, and widespread use of the tools. The competitive environment explained 28 percent, size and location 17 percent. Parsing these results even further, we found that three aspects of management were particularly critical to superior performance: a lack of internal barriers to Web 2.0, a culture favoring open collaboration (a factor confirmed in the 2009 survey), and early adoption of Web 2.0 technologies. The high-tech and telecom industries had higher scores than manufacturing, while companies with sales of less than $1 billion or those located in the United States were more likely to have relatively high performance scores than larger companies located elsewhere. While the evidence suggests that focused management improves Web 2.0 performance, there’s still a way to go before users become as satisfied with these technologies as they are with others. The top 20 percent of companies reached a performance score of only 35 percent (the score increased to 44 percent in the 2009 survey). When the same score methodology is applied to technologies that corporations had previously adopted, Web 2.0’s score is below the 57 percent for traditional corporate IT services, such as e-mail, and the 80 percent for mobile-communications services. About the Author(s) Jacques Bughin is a director in McKinsey’s Brussels office.
        
        This year, for the first time, we have consolidated the data from our Web 2.0 research into an interactive graphic (see Business and Web 2.0: An interactive feature). With just a few clicks, users can compare technologies, usage, satisfaction, and much more across all three survey years.
        
        Web 2.0 technologies can be a powerful lure for an organization; their interactivity promises to bring more employees into daily contact at lower cost. When used effectively, they also may encourage participation in projects and idea sharing, thus deepening a company’s pool of knowledge. They may bring greater scope and scale to organizations as well, strengthening bonds with customers and improving communications with suppliers and outside partners.
        
        This year’s survey turned up strong evidence that these advantages are translating into measurable business gains (Exhibit 1). When we asked respondents about the business benefits their companies have gained as a result of using Web 2.0 technologies, they most often report greater ability to share ideas; improved access to knowledge experts; and reduced costs of communications, travel, and operations. Many respondents also say Web 2.0 tools have decreased the time to market for products and have had the effect of improving employee satisfaction.
        
        Looking beyond company borders, significant benefits have stemmed from better interactions with organizations and customers. The ability to forge closer ties has increased customers’ awareness and consideration of companies’ products and has improved customer satisfaction. Respondents also say they have been able to burnish their innovation skills, perhaps because their companies and customers jointly shape and cocreate products using Web 2.0 connections. Some respondents report that these customer interactions have resulted in measurable increases in revenues.
        
        Respondents cite similar gains resulting from better ties to suppliers and partners. Highest on that list of benefits is the ability to gain access to expertise outside company walls more quickly. These respondents also cite lower costs of communication with business partners and lower travel costs.
        
        We also asked respondents to specify the percentage improvement they experienced for each reported benefit across all three benefit classes. The median level of gains derived from internal Web 2.0 use ranged from a 10 percent improvement in operational costs to a 30 percent increase in the speed at which employees are able to tap outside experts.
        
        Exhibit 1 Greater knowledge and better marketing We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Web 2.0 delivers benefits by multiplying the opportunities for collaboration and by allowing knowledge to spread more effectively. These benefits can accrue through companies’ use of automatic information feeds such as RSS or microblogs, of which Twitter is the most popular manifestation. Although many companies use a mix of tools, the survey shows that among all respondents deriving benefits, the more heavily used technologies are blogs, wikis, and podcasts—the same tools that are popular among consumers—(Exhibit 2).
        
        Among respondents who report seeing benefits within their companies, many cite blogs, RSS, and social networks as important means of exchanging knowledge. These networks often help companies coalesce affinity groups internally. Finally, respondents report using Web videos more frequently since the previous survey; technology improvements have made videos easier to produce and disseminate within organizations.
        
        Respondents who report that Web technologies have strengthened their companies’ links to customers also cite blogs and social networks as important. Both allow companies to distribute product information more readily and, perhaps more critically, they invite customer feedback and even participation in the creation of products.
        
        Similarly, among those capturing benefits in their dealings with suppliers and partners, the tools of choice again are blogs, social networks, and video sharing. While respondents tell us that tapping expert knowledge from outside is their top priority, few report deploying prediction markets to harvest collective insights from these external networks.
        
        Exhibit 2 A mix of technologies We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Regardless of industry, executives at companies that use more Web 2.0 technologies also report greater benefits. Comparing respondents’ industries, those at high-technology companies are most likely to report measurable benefits from Web 2.0 across the board, followed by those at companies offering business, legal, and professional services (Exhibit 3). Companies with revenues exceeding $1 billion—along with business-to-business organizations—are more likely to report benefits than are smaller companies or consumer companies. Among functions, respondents in information technology, business development, and sales and marketing are more likely to report seeing benefits at various levels than are those in finance or purchasing. IT executives, in general, are more focused on using Web tools to achieve internal improvements, while business development and sales functions often rely on the technologies to deliver better insights into markets or to interact with consumers.
        
        Regionally, respondents in North America and India are most likely to claim that they are reaping benefits from their companies’ use of Web 2.0. These respondents also report higher levels of technology usage in general. Respondents in North America and China report the highest customer benefits. Those from India and China, meanwhile, are more likely to report benefits flowing from their interactions with customers and partners.
        
        Exhibit 3 Where the benefits are We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        These survey results indicate that a different type of company may be emerging—one that makes intensive use of interactive technologies. This networked organization is characterized both by the internal integration of Web tools among employees, as well as use of the technologies to strengthen company ties with external stakeholders—customers and business partners.
        
        As such, companies reporting business benefits also report high levels of Web 2.0 integration into employee workflows. They most often deploy three or more Web tools, and usage is high throughout these organizations (Exhibit 4).
        
        Half of respondents report that Web 2.0 technologies have fostered in-company interactions across geographic borders; 45 percent cite interactions across functions, and 39 percent across business units.
        
        This integrated internal use of Web 2.0 is also the model for interactivity outside the company. The survey results suggest that networked organizations have created processes and Web platforms that serve to manage significant portions of these external ties. Respondents reporting measurable benefits say their companies, on average, have Web 2.0 interactions with 35 percent of their customers. These companies forged similar Web ties to 48 percent of their suppliers, partners, and outside experts. An organizational structure that’s more porous and networked may make companies more resilient and adaptive, sharpening their ability to access knowledge and thus innovate more effectively.
        
        Exhibit 4 Web 2.0 in the work stream We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.com
        
        Many companies experiment with Web 2.0 technologies, but creating an environment with a critical mass of committed users is more difficult. The survey results confirm that successful adoption requires that the use of these tools be integrated into the flow of users’ work (Exhibit 5). Furthermore, encouraging continuing use requires approaches other than the traditional financial or performance incentives deployed as motivational tools. In the Web community, status is often built on a reputation for making meaningful contributions. Respondents say informal incentives incorporating the Web ethos, such as ratings by peers and online recognition of status, have been most effective in encouraging Web 2.0 adoption. They also say role modeling—active Web use by executives—has been important for encouraging adoption internally.
        
        Exhibit 5 Integrating for success We strive to provide individuals with disabilities equal access to our website. If you would like information about this content we will be happy to work with you. Please email us at: McKinsey_Website_Accessibility@mckinsey.comFor the past seven years, thousands of executives from around the world—across a range of industries and functional areas—have responded to a McKinsey survey on how organizations are using social (or Web 2.0) technologies. In 2009 we created an interactive tool that links the data from these survey results and charts it to the emerging trends in Web 2.0 adoption.
        
        This interactive focuses on several of the survey’s core questions—from what technologies and tools companies view as most important to what kind of investments, if any, organizations plan to make in Web 2.0 in the future. Our most recent survey examines the business use of 13 social technologies and tools: blogs, collaborative document editing, mash-ups (a Web application that combines multiple sources of data into a single tool), microblogging, online videoconferencing, podcasts, prediction markets, rating, RSS (Really Simple Syndication), social networking, tagging, video sharing, and wikis.
        
        Using the interactive, you can track the performance of each technology through the years or customize the view to compare particular technologies side by side. The interactive also contains an audio guide from Michael Chui—a McKinsey principal and one of the drivers of the Web 2.0 research initiative—who takes you further inside the results and trends.
        
        This interactive archive will evolve from year to year as the survey progresses and as businesses continue to evaluate their use of and satisfaction with Web 2.0.It’s hardly news that the Internet has evolved into the primary vehicle for communication, information, and commerce. But in a surprising twist, today’s online customers—as both producers and consumers of their own content and services—ferociously guard their online experiences. This trend, which goes far beyond Web buzz, is catching some executives by surprise and making others more than a bit worried.
        
        What does this development mean for your company? In effect, that its marketers are being replaced. As markets morph into Web 2.0 “conversations” and consumers gain much greater freedom to pursue their own interests, customers are doing things that online marketing managers don’t necessarily want—or expect—them to do. For example, they can easily connect with one another, often using multimedia sites such as YouTube and Flickr, so they themselves can satisfy their need for information about products. What’s more, consumers may trust information obtained in this way much more than they do information from your company. What will happen when these consumer experiences are much more interesting than anything your marketers have put up on the Web?
        
        Executives can use a model we at the Sloan Center for Internet Retailing have developed called LEAD (listen, experiment, apply, develop) to create a road map that will help companies thrive in the online world’s environment of constant change.
        
        Listen. Your organization should have a formal process to monitor and analyze what its customers are saying about it online and then use this information as an early-warning system. Even casual observation of these online conversations is better than nothing. Indeed, customers are probably already talking about you on sites such as Facebook or Kaboodle, whether or not you’ve set up pages there. They are also using microblogging platforms such as Twitter to broadcast their latest feelings about using your products and services. Some companies (such as Nielsen Online, with its BuzzMetrics services) specialize in analyzing this online chatter, though in-house efforts also can be very effective.
        
        But simply entering the game is only a start. Companies should always assume that the digital environment will change rapidly—so they must adapt accordingly. Rather than pushing messages at consumers, marketers should listen to them and think constantly about ways to engage with them actively. A social Web presence that is tone deaf to a customer’s needs augurs rough times ahead: after one consumer products company, late last year, aired a Web video that some customers perceived as insensitive, many were so outraged that they pressed for a boycott of the brand. Worse still, its besieged managers didn’t bother to listen to the reaction after releasing the video—behavior that further incensed consumers. Social-media experts unanimously turned thumbs down on the company’s lackadaisical response, which led to widely publicized compilations of the tweets, blog posts, and YouTube video reactions; to mainstream media coverage of the debacle; and, finally, to what some considered an inadequate apology from the company. What’s particularly relevant is that most of these events unfolded over the course of a single weekend day.
        
        Experiment. Don’t just monitor social media—engage your newly empowered customers by using the novel tools of Web 2.0 and beyond. Start with simple pilots: for example, create a company profile on social-networking sites, such as Ning, or sponsor a promotion on the innovative social-shopping site Polyvore. Make friends with bloggers and tweet your customers on Twitter. Kimberly-Clark, for instance, used its Huggies “Baby Countdown” widget to engage its customers on their computer desktops. Smirnoff used the viral-video marketing vehicle Tea Partay to promote its Raw Tea product. The campaign was a hit on YouTube.
        
        While return-on-investment metrics for social media are still in the early stages, these experiments clearly pay off big time in greater customer awareness and brand engagement. Unless you have Web 2.0 experts on your team, stick with small experiments, since big ones can fail badly. For example, create a company-controlled community, perhaps through a blog, that gives your customers a place to offer feedback about your products and services—a basic move many companies still ignore. Also, take the first steps toward cocreation: engage your customers through collaborative efforts that conceive new offerings and ad campaigns, as Frito Lay did with its innovative customer-created ads campaigns “Crash the Super Bowl,” “Fight for the Flavor,” and “The Quest.” Remember, though, that there really aren’t any best practices or established business models yet. For now, companies just need to get some experience—and quickly.
        
        Apply. Take the experiments and apply them. To make it easier to reach out to customers, optimize your Web site so that it connects fluidly with online communities and social-media sites. Make it simple for consumers to link to you and tag your content, and find ways to make your site more relevant in social-networking searches. If you have nothing worth linking to or tagging, or if your content isn’t relevant to consumers at all, you’re in trouble. Measuring impact is paramount, so you’ll need to use the Web’s predictive tools and quantitative analysis to track the results of your experiments. As you gain experience, you can apply what you learn on a larger scale.
        
        Develop. The Internet is a social medium and should therefore be a crucial part of any company’s marketing mix. But it is critical to develop integrated marketing programs that use the Web as more than just another advertising channel. Companies must therefore rapidly flee from the mass-media broadcast mentality: for example, rather than simply buying ads on MySpace, they should make interactive Web 2.0 elements part of their marketing programs.
        
        Consider the way GlaxoSmithKline handled the growing consumer confusion and concern surrounding the side effects of its mass-market over-the-counter weight-loss drug Alli. The company confronted the problem directly by setting up the My Alli community site, which includes active forums, videos, FAQs, a membership plan to aid weight loss, interactive diet tools, information on diet and support groups, cobranding, links to partners (eDiets for home meals, links to Alli retailers), and a sweepstakes tie-in to TV and print advertising campaigns. This approach allows the company to address consumers in a direct, nonthreatening way and to use basic Web 2.0 features that wrap these messages in a warm and supportive experience. When consumers search the Web for information about Alli, they see GlaxoSmithKline’s myalli.com within the first three organic search results, the better to counteract paid ads that are either scary (“diet-pill warnings”) or questionable (supposedly unbiased diet pill reviews on Britneys-blog.com), as well as links that emphasize the “icky” side effects. When consumers click through to myalli.com, they find a comprehensive site with straightforward, detailed product information, which helps the company mitigate the effects of information it can’t control. This site is a terrific example of how to integrate social media into a marketing campaign in an effective way.
        
        Bottom line: by focusing on the fundamental aspects of the consumers’ online behavior— not just current best practices—companies will be better prepared when Web 2.0+ morphs into Web 3.0 and beyond.As the downturn continues, millions of corporate managers—gripped by the job jitters—are rushing to join online social networks in a scramble to build their social capital. The popularity of sites such as LinkedIn is soaring: less than a year ago the site had little brand profile and was seen mostly as a venue for corporate suits trolling for professional contacts while plotting their next career move. Facebook, by contrast, has largely attracted individuals seeking a compelling site for fun social networking.
        
        Today LinkedIn’s year-on-year growth is up nearly 200 percent in the United States and it now has more than 35 million members—many of whom were formerly employed within the hard-hit financial sector. And it’s just one of the many sites to which recession-struck managers are flocking: Xing (based in Germany), with its 7 million members and special Lehman Brothers alumni section, and Meet the Boss (based in the United Kingdom), which restricts membership to C-level financial types, are also experiencing burgeoning membership levels.
        
        This surging popularity of online social networking is transforming the nature of business networking, with profound implications for the way business people manage their careers. But it also augurs profound change for social networking itself.
        
        With so many people stampeding into Web-based social networks, the line between social and business networking is becoming increasingly blurred. An important question is whether the values and codes of conduct specific to the virtual world will come into conflict with real-world values and norms. Facebook, where the idea of a “friend” is directly embedded in the interface, is increasingly cluttered with self-promoters, career artists, and marketing entrepreneurs. What happens as this trend intensifies and those using Facebook exclusively for career networking invade?
        
        There are, of course, powerful economic reasons behind the trend. As sociologist Nan Lin puts it in his book, Social Capital, “Individuals engage in interactions and networking in order to produce profits.” These profits are based upon information, influence, social credentials, and recognition. The accumulated social capital, meanwhile, helps individuals to gain competitive advantages in the labor market as a result of privileged access to “resources” located on the social networks.
        
        Still, for many there’s nothing more irritating than when a new “friend” contacts you almost immediately with an inappropriate request for a favor. Generally, it’s more advisable to approach social networking as a giver, not a taker, and gradually build relationships according to reciprocated favors. Overall, online social networking, with its support groups and trusted access, is governed by a culture of sharing, not selling.
        
        And can the throngs of interlopers really be considered friends? Anthropologists tell us that it’s impossible to maintain stable social relationships with more than 150 people. Maintaining a professional network of more than 150 looser connections on LinkedIn might be plausible, but it would strain the richer social relations that make up the fabric of sites such as Facebook. Among Facebook’s 175 million members, the instances of “defriending” are already growing.
        
        It’s a safe bet that if the economic downturn grinds on, we will witness further conflict between the nonrational instinct to connect socially and the rational calculation to build social capital for professional reasons. If so, it may put further strain on the notion of an online friend. We may find ourselves asking more frequently that age-old question, “What are friends for?”In recent years, using technology to change the way people work has often meant painful disruption, as CIOs rolled enterprise software programs through the ranks of reluctant staffers. Today, employees are more likely to bring in new technologies on their own—and to do so enthusiastically—through their Web browser, whether it’s starting a blog, setting up a wiki to share knowledge, or collaborating on documents hosted online. Andrew McAfee, principal research scientist at the Center for Digital Business at the MIT Sloan School of Management, has been watching this shift closely.
        
        00:00 Audio MIT's Andrew McAfee on how Web 2.0 is changing the way we work Do Web 2.0 technologies help organizations achieve their goals?
        
        His new book, Enterprise 2.0: New Collaborative Tools for your Organization’s Toughest Challenges, explores the ways that leading organizations are bringing Web 2.0 tools inside. McAfee calls these tools “emergent social software platforms”—highly visible environments with tools that evolve as people use them—and he is optimistic about their potential to improve the way we work.
        
        McAfee spoke with McKinsey’s Roger Roberts, a principal in the Silicon Valley office, in Palo Alto, California, in October 2009.More than ten years into the widespread business adoption of the Web, some managers still fail to grasp the economic implications of cheap and ubiquitous information on and about their business. Hal Varian, professor of information sciences, business, and economics at the University of California at Berkeley, says it’s imperative for managers to gain a keener understanding of the potential for technology to reconfigure their industries. Varian, currently serving as Google's chief economist, compares the current period to previous times of industrialization when new technologies combined to create ever more complex and valuable systems—and thus reshaped the economy.
        
        Varian spoke with McKinsey’s James Manyika, a director in the San Francisco office, in Napa, California, in October 2008. Watch the video or read the transcript of his comments below.
        
        Interactive Hal Varian on how the Web challenges managers Google’s chief economist on how technology empowers innovation. Open interactive popup
        
        We’re in the middle of a period that I refer to as a period of “combinatorial innovation.” So if you look historically, you’ll find periods in history where there would be the availability of a different component parts that innovators could combine or recombine to create new inventions. In the 1800s, it was interchangeable parts. In 1920, it was electronics. In the 1970s, it was integrated circuits.
        
        Now what we see is a period where you have Internet components, where you have software, protocols, languages, and capabilities to combine these component parts in ways that create totally new innovations. The great thing about the current period is that component parts are all bits. That means you never run out of them. You can reproduce them, you can duplicate them, you can spread them around the world, and you can have thousands and tens of thousands of innovators combining or recombining the same component parts to create new innovation. So there’s no shortage. There are no inventory delays. It’s a situation where the components are available for everyone, and so we get this tremendous burst of innovation that we’re seeing.
        
        The question is, “What are other periods where we saw technology influence the way organizations work?” One nice example comes from the works of Alfred Chandler, where he describes how the telegraph and the railroad had a big impact on the development of the modern corporation. And this was a synergistic operation: one, you had to have a large organization to manage these technologies, and two, you had to have the communications and transportation infrastructure to enable the management at a distance.
        
        So I think now, with what we’re seeing with mobility, we’re going to have a totally different concept of what it means to go to work. The work goes to you, and you’re able to deal with your work at any time and any place, using the infrastructure that’s now become available.
        
        At the base, there’s the innovation infrastructure making better, faster, cheaper networks. There’s the improvement in the human–computer interface because the big challenge in mobile communication has always been dealing with this—quite limited—interface. But then, the kinds of innovations I think will arise on top of that will be innovations in how work is done. And that’s going to be one of the most exciting aspects, in my opinion.
        
        If you look at the beginning of the 20th century, we saw the rise of mass production. Henry Ford and the entire team were down on the factory floor raising this, lowering that, speeding up the assembly line, changing the way things were built, and were able to extract far more efficiencies than were available before. I think the same thing is happening now with digital technology. When we’re all networked, we all have access to the same documents, to the same capabilities, to this common infrastructure, and we can improve the way work—intellectual work, knowledge work—flows through the organization. And again, in my opinion, that will lead to a substantial advantage in terms of productivity.
        
        Back in the early days of the Web, every document had at the bottom, “Copyright 1997. Do not redistribute.” Now every document has at the bottom, “Copyright 2008. Click here to send to your friends.” So there’s already been a big revolution in how we view intellectual property. So it’s not so much the question of what’s owned or what’s not owned. It’s a question of how can you leverage the assets you have to realize the most value.
        
        I think that the availability of these very inexpensive platforms you’re creating, in disseminating content, means that it’s become intensely competitive. The content is as valuable as it ever was, it’s just the competition that’s pushed the prices down to something that approximates zero. So it’s not something that the content producers necessarily embrace, but it’s something they’re forced into by the nature of the technological change.
        
        In these models, there is typically a revenue-generating component somewhere in the value chain. And most commonly today we’re seeing it on the advertising side. To look at this from a historical perspective, it’s really not so new. If you look at the 1920s, the technological question in the ’20s was, “How can we build a business model around broadcast radio?” And nobody really had a good idea. And back in the mid-1990s we asked, “How can we build a business model around the Internet?” And the preferred model at the time was a micropayments system. That never happened, for some reasons, but what did happen instead is we moved into the advertising model, and the advertising’s model been phenomenally successful.
        
        We have to look at today’s economy and say, “What is it that’s really scarce in the Internet economy?” And the answer is attention. [Psychologist] Herb Simon recognized this many years ago. He said, “A wealth of information creates a poverty of attention.” So being able to capture someone’s attention at the right time is a very valuable asset. And Google really has built an entire business around this, because we’re capturing your attention when you’re doing a search for something you’re interested in. That’s the ideal time to show you an advertisement for a product that may be related or complimentary to what your search is all about.
        
        I keep saying the sexy job in the next ten years will be statisticians. People think I’m joking, but who would’ve guessed that computer engineers would’ve been the sexy job of the 1990s? The ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary scarce factor is the ability to understand that data and extract value from it.
        
        I think statisticians are part of it, but it’s just a part. You also want to be able to visualize the data, communicate the data, and utilize it effectively. But I do think those skills—of being able to access, understand, and communicate the insights you get from data analysis—are going to be extremely important. Managers need to be able to access and understand the data themselves.
        
        You always have this problem of being surrounded by “yes men” and people who want to predigest everything for you. In the old organization, you had to have this whole army of people digesting information to be able to feed it to the decision maker at the top. But that’s not the way it works anymore: the information can be available across the ranks, to everyone in the organization. And what you need to ensure is that people have access to the data they need to make their day-to-day decisions. And this can be done much more easily than it could be done in the past. And it really empowers the knowledge workers to work more effectively.
        
        One of the really interesting phenomena that’s been going on in the last 20 years is what I call “computer-mediated transactions.” So now, in the middle of almost every transaction from person to person or organization to organization, there’s a computer. And the computer can monitor that transaction, record the information, collect the data, and assure that the transaction is carried out the way it was intended to be carried out. So one of the subtle implications of this is you can now write contracts and make contracts enforceable that simply weren’t enforceable before.
        
        Let me give you an example. Suppose you go rent a car and they say, “Hey, we’ll give you $10 off if you don’t go over the speed limit.” Well, that might sound like a good deal, but what’s to keep you from going over the speed limit? Well, the answer is now they’ve got a transponder in the trunk and it will monitor your behavior and charge you accordingly. And the same thing happens with semitrucks: virtually every semi on the road today has a computer in it. And that computer improves the logistics. It monitors the performance of the driver and it helps things get to the consumer more quickly. So there are a lot of capabilities of that sort that allow you to contract on terms that were just not available to you before.
        
        [At the same time,] you get a new technology in and people are excited about the positive sides of it. Then you see there are also some negative aspects. And you’ll have a regulatory infrastructure that arises to deal with those. I think everybody is very excited about the intended aspects of this technology—the fact that you can personalize, the fact that you can monitor, the fact that you can provide products that are more closely suited to a consumer’s interests and needs. What people are worried about are the unintended consequences, the downsides, the negative sides, the security, the identity theft, the possibility of extortion or embarrassment. These are the problems: not what people want to do but what could happen if these technologies weren’t appropriately managed.
        
        We’re obviously going to see enormous change in the traditional marketing industry. You look at TV, you look at print, you look at radio and other media of that sort. On the Internet, we’ve learned to measure advertising effectiveness, and the challenge now is to move those same effectiveness measures over to the offline media.
        
        That can be done. I think we’re going to see vast improvements in how those industries function in the future. And in general, if we look at service industries—well, everybody I think is in agreement that we’re going to see lots of efficiency improvements in services, because we do have this network capability. We have the technological infrastructure. We can improve communication flows. The second beneficiaries of that will be with service industries who’ve already seen a lot of advances in manufacturing productivity. And the tough nut is the one we’re working on cracking now.
        
        What I actually work on to a large extent is a current feeding of the auction model that we have at Google. As you know, all of our ads are sold by auction. That’s a relatively novel pricing mechanism in the ad world. And there’re a lot of intricacies that involve how you manage that. We’d like to extend that model to the offline world: to radio, TV, print, and other media. It’s a model that was enabled by the Internet. It’s not something you could’ve done without that information technology there. And it’s a great model for all sorts of resource allocation issues.
        
        I think the people who originally designed the model way back in 2001 had a very, very useful insight. They recognized that the content provider has impressions to sell. So you’ve got some space in your TV show. You’ve got some space on your page. You’ve got some space that’s available to put an ad. But what the advertiser wants to pay for is clicks or conversions or visits. So they don’t really care how many impressions they show. Normally, what they care about is getting people into their store and, ultimately, getting people to purchase. So you have to build a system that allows the publisher to sell impressions but the advertiser to buy clicks. And I think we’ve managed to accomplish that in a nice, elegant way.More than ten years into the widespread business adoption of the Web, some managers still fail to grasp the economic implications of cheap and ubiquitous information on and about their business. Hal Varian, professor of information sciences, business, and economics at the University of California at Berkeley, says it’s imperative for managers to gain a keener understanding of the potential for technology to reconfigure their industries. Varian, currently serving as Google's chief economist, compares the current period to previous times of industrialization when new technologies combined to create ever more complex and valuable systems—and thus reshaped the economy.
        
        Varian spoke with McKinsey’s James Manyika, a director in the San Francisco office, in Napa, California, in October 2008. Watch the video or read the transcript of his comments below.
        
        Interactive Hal Varian on how the Web challenges managers Google’s chief economist on how technology empowers innovation. Open interactive popup
        
        We’re in the middle of a period that I refer to as a period of “combinatorial innovation.” So if you look historically, you’ll find periods in history where there would be the availability of a different component parts that innovators could combine or recombine to create new inventions. In the 1800s, it was interchangeable parts. In 1920, it was electronics. In the 1970s, it was integrated circuits.
        
        Now what we see is a period where you have Internet components, where you have software, protocols, languages, and capabilities to combine these component parts in ways that create totally new innovations. The great thing about the current period is that component parts are all bits. That means you never run out of them. You can reproduce them, you can duplicate them, you can spread them around the world, and you can have thousands and tens of thousands of innovators combining or recombining the same component parts to create new innovation. So there’s no shortage. There are no inventory delays. It’s a situation where the components are available for everyone, and so we get this tremendous burst of innovation that we’re seeing.
        
        The question is, “What are other periods where we saw technology influence the way organizations work?” One nice example comes from the works of Alfred Chandler, where he describes how the telegraph and the railroad had a big impact on the development of the modern corporation. And this was a synergistic operation: one, you had to have a large organization to manage these technologies, and two, you had to have the communications and transportation infrastructure to enable the management at a distance.
        
        So I think now, with what we’re seeing with mobility, we’re going to have a totally different concept of what it means to go to work. The work goes to you, and you’re able to deal with your work at any time and any place, using the infrastructure that’s now become available.
        
        At the base, there’s the innovation infrastructure making better, faster, cheaper networks. There’s the improvement in the human–computer interface because the big challenge in mobile communication has always been dealing with this—quite limited—interface. But then, the kinds of innovations I think will arise on top of that will be innovations in how work is done. And that’s going to be one of the most exciting aspects, in my opinion.
        
        If you look at the beginning of the 20th century, we saw the rise of mass production. Henry Ford and the entire team were down on the factory floor raising this, lowering that, speeding up the assembly line, changing the way things were built, and were able to extract far more efficiencies than were available before. I think the same thing is happening now with digital technology. When we’re all networked, we all have access to the same documents, to the same capabilities, to this common infrastructure, and we can improve the way work—intellectual work, knowledge work—flows through the organization. And again, in my opinion, that will lead to a substantial advantage in terms of productivity.
        
        Back in the early days of the Web, every document had at the bottom, “Copyright 1997. Do not redistribute.” Now every document has at the bottom, “Copyright 2008. Click here to send to your friends.” So there’s already been a big revolution in how we view intellectual property. So it’s not so much the question of what’s owned or what’s not owned. It’s a question of how can you leverage the assets you have to realize the most value.
        
        I think that the availability of these very inexpensive platforms you’re creating, in disseminating content, means that it’s become intensely competitive. The content is as valuable as it ever was, it’s just the competition that’s pushed the prices down to something that approximates zero. So it’s not something that the content producers necessarily embrace, but it’s something they’re forced into by the nature of the technological change.
        
        In these models, there is typically a revenue-generating component somewhere in the value chain. And most commonly today we’re seeing it on the advertising side. To look at this from a historical perspective, it’s really not so new. If you look at the 1920s, the technological question in the ’20s was, “How can we build a business model around broadcast radio?” And nobody really had a good idea. And back in the mid-1990s we asked, “How can we build a business model around the Internet?” And the preferred model at the time was a micropayments system. That never happened, for some reasons, but what did happen instead is we moved into the advertising model, and the advertising’s model been phenomenally successful.
        
        We have to look at today’s economy and say, “What is it that’s really scarce in the Internet economy?” And the answer is attention. [Psychologist] Herb Simon recognized this many years ago. He said, “A wealth of information creates a poverty of attention.” So being able to capture someone’s attention at the right time is a very valuable asset. And Google really has built an entire business around this, because we’re capturing your attention when you’re doing a search for something you’re interested in. That’s the ideal time to show you an advertisement for a product that may be related or complimentary to what your search is all about.
        
        I keep saying the sexy job in the next ten years will be statisticians. People think I’m joking, but who would’ve guessed that computer engineers would’ve been the sexy job of the 1990s? The ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary scarce factor is the ability to understand that data and extract value from it.
        
        I think statisticians are part of it, but it’s just a part. You also want to be able to visualize the data, communicate the data, and utilize it effectively. But I do think those skills—of being able to access, understand, and communicate the insights you get from data analysis—are going to be extremely important. Managers need to be able to access and understand the data themselves.
        
        You always have this problem of being surrounded by “yes men” and people who want to predigest everything for you. In the old organization, you had to have this whole army of people digesting information to be able to feed it to the decision maker at the top. But that’s not the way it works anymore: the information can be available across the ranks, to everyone in the organization. And what you need to ensure is that people have access to the data they need to make their day-to-day decisions. And this can be done much more easily than it could be done in the past. And it really empowers the knowledge workers to work more effectively.
        
        One of the really interesting phenomena that’s been going on in the last 20 years is what I call “computer-mediated transactions.” So now, in the middle of almost every transaction from person to person or organization to organization, there’s a computer. And the computer can monitor that transaction, record the information, collect the data, and assure that the transaction is carried out the way it was intended to be carried out. So one of the subtle implications of this is you can now write contracts and make contracts enforceable that simply weren’t enforceable before.
        
        Let me give you an example. Suppose you go rent a car and they say, “Hey, we’ll give you $10 off if you don’t go over the speed limit.” Well, that might sound like a good deal, but what’s to keep you from going over the speed limit? Well, the answer is now they’ve got a transponder in the trunk and it will monitor your behavior and charge you accordingly. And the same thing happens with semitrucks: virtually every semi on the road today has a computer in it. And that computer improves the logistics. It monitors the performance of the driver and it helps things get to the consumer more quickly. So there are a lot of capabilities of that sort that allow you to contract on terms that were just not available to you before.
        
        [At the same time,] you get a new technology in and people are excited about the positive sides of it. Then you see there are also some negative aspects. And you’ll have a regulatory infrastructure that arises to deal with those. I think everybody is very excited about the intended aspects of this technology—the fact that you can personalize, the fact that you can monitor, the fact that you can provide products that are more closely suited to a consumer’s interests and needs. What people are worried about are the unintended consequences, the downsides, the negative sides, the security, the identity theft, the possibility of extortion or embarrassment. These are the problems: not what people want to do but what could happen if these technologies weren’t appropriately managed.
        
        We’re obviously going to see enormous change in the traditional marketing industry. You look at TV, you look at print, you look at radio and other media of that sort. On the Internet, we’ve learned to measure advertising effectiveness, and the challenge now is to move those same effectiveness measures over to the offline media.
        
        That can be done. I think we’re going to see vast improvements in how those industries function in the future. And in general, if we look at service industries—well, everybody I think is in agreement that we’re going to see lots of efficiency improvements in services, because we do have this network capability. We have the technological infrastructure. We can improve communication flows. The second beneficiaries of that will be with service industries who’ve already seen a lot of advances in manufacturing productivity. And the tough nut is the one we’re working on cracking now.
        
        What I actually work on to a large extent is a current feeding of the auction model that we have at Google. As you know, all of our ads are sold by auction. That’s a relatively novel pricing mechanism in the ad world. And there’re a lot of intricacies that involve how you manage that. We’d like to extend that model to the offline world: to radio, TV, print, and other media. It’s a model that was enabled by the Internet. It’s not something you could’ve done without that information technology there. And it’s a great model for all sorts of resource allocation issues.
        
        I think the people who originally designed the model way back in 2001 had a very, very useful insight. They recognized that the content provider has impressions to sell. So you’ve got some space in your TV show. You’ve got some space on your page. You’ve got some space that’s available to put an ad. But what the advertiser wants to pay for is clicks or conversions or visits. So they don’t really care how many impressions they show. Normally, what they care about is getting people into their store and, ultimately, getting people to purchase. So you have to build a system that allows the publisher to sell impressions but the advertiser to buy clicks. And I think we’ve managed to accomplish that in a nice, elegant way.